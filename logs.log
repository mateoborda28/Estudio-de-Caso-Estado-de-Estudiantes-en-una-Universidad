2024-09-06 22:53:09,752:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-06 22:53:09,752:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-06 22:53:09,752:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-06 22:53:09,752:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-06 22:57:06,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-06 22:57:06,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-06 22:57:06,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-06 22:57:06,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-06 23:02:22,958:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-06 23:02:22,958:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-06 23:02:22,958:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-06 23:02:22,958:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 09:32:03,302:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 09:32:03,302:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 09:32:03,302:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 09:32:03,302:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 09:43:46,533:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 09:43:46,533:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 09:43:46,533:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 09:43:46,533:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:26:23,106:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:26:23,106:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:26:23,106:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:26:23,106:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:36:59,688:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:36:59,688:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:36:59,690:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:36:59,690:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:38:09,632:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:38:09,645:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:38:09,646:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:38:09,647:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:40:46,485:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:40:46,490:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:40:46,490:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:40:46,490:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:41:11,226:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:41:11,227:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:41:11,228:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:41:11,228:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-07 23:49:25,452:INFO:PyCaret ClassificationExperiment
2024-09-07 23:49:25,452:INFO:Logging name: clf-default-name
2024-09-07 23:49:25,456:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-07 23:49:25,458:INFO:version 3.3.2
2024-09-07 23:49:25,458:INFO:Initializing setup()
2024-09-07 23:49:25,458:INFO:self.USI: b39d
2024-09-07 23:49:25,458:INFO:self._variable_keys: {'fold_shuffle_param', 'pipeline', 'is_multiclass', 'X_train', 'memory', 'target_param', 'seed', 'USI', 'y', 'gpu_n_jobs_param', 'exp_name_log', 'fix_imbalance', 'html_param', '_ml_usecase', 'fold_generator', 'X_test', 'log_plots_param', 'n_jobs_param', 'idx', 'gpu_param', 'X', 'data', 'fold_groups_param', 'y_test', 'y_train', '_available_plots', 'logging_param', 'exp_id'}
2024-09-07 23:49:25,458:INFO:Checking environment
2024-09-07 23:49:25,458:INFO:python_version: 3.10.11
2024-09-07 23:49:25,458:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-07 23:49:25,458:INFO:machine: AMD64
2024-09-07 23:49:25,481:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-07 23:49:25,481:INFO:Memory: svmem(total=16407719936, available=1986330624, percent=87.9, used=14421389312, free=1986330624)
2024-09-07 23:49:25,481:INFO:Physical Core: 4
2024-09-07 23:49:25,481:INFO:Logical Core: 8
2024-09-07 23:49:25,481:INFO:Checking libraries
2024-09-07 23:49:25,489:INFO:System:
2024-09-07 23:49:25,489:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-07 23:49:25,489:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-07 23:49:25,489:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-07 23:49:25,489:INFO:PyCaret required dependencies:
2024-09-07 23:49:25,661:INFO:                 pip: 24.2
2024-09-07 23:49:25,661:INFO:          setuptools: 72.1.0
2024-09-07 23:49:25,661:INFO:             pycaret: 3.3.2
2024-09-07 23:49:25,661:INFO:             IPython: 8.25.0
2024-09-07 23:49:25,669:INFO:          ipywidgets: 8.1.5
2024-09-07 23:49:25,670:INFO:                tqdm: 4.66.5
2024-09-07 23:49:25,670:INFO:               numpy: 1.26.4
2024-09-07 23:49:25,670:INFO:              pandas: 2.1.4
2024-09-07 23:49:25,670:INFO:              jinja2: 3.1.4
2024-09-07 23:49:25,670:INFO:               scipy: 1.11.4
2024-09-07 23:49:25,670:INFO:              joblib: 1.3.2
2024-09-07 23:49:25,670:INFO:             sklearn: 1.4.2
2024-09-07 23:49:25,670:INFO:                pyod: 2.0.1
2024-09-07 23:49:25,670:INFO:            imblearn: 0.12.3
2024-09-07 23:49:25,670:INFO:   category_encoders: 2.6.3
2024-09-07 23:49:25,670:INFO:            lightgbm: 4.5.0
2024-09-07 23:49:25,670:INFO:               numba: 0.60.0
2024-09-07 23:49:25,670:INFO:            requests: 2.32.3
2024-09-07 23:49:25,670:INFO:          matplotlib: 3.7.5
2024-09-07 23:49:25,670:INFO:          scikitplot: 0.3.7
2024-09-07 23:49:25,670:INFO:         yellowbrick: 1.5
2024-09-07 23:49:25,670:INFO:              plotly: 5.24.0
2024-09-07 23:49:25,670:INFO:    plotly-resampler: Not installed
2024-09-07 23:49:25,670:INFO:             kaleido: 0.2.1
2024-09-07 23:49:25,670:INFO:           schemdraw: 0.15
2024-09-07 23:49:25,670:INFO:         statsmodels: 0.14.2
2024-09-07 23:49:25,670:INFO:              sktime: 0.26.0
2024-09-07 23:49:25,670:INFO:               tbats: 1.1.3
2024-09-07 23:49:25,670:INFO:            pmdarima: 2.0.4
2024-09-07 23:49:25,670:INFO:              psutil: 5.9.0
2024-09-07 23:49:25,670:INFO:          markupsafe: 2.1.3
2024-09-07 23:49:25,670:INFO:             pickle5: Not installed
2024-09-07 23:49:25,670:INFO:         cloudpickle: 3.0.0
2024-09-07 23:49:25,670:INFO:         deprecation: 2.1.0
2024-09-07 23:49:25,670:INFO:              xxhash: 3.5.0
2024-09-07 23:49:25,670:INFO:           wurlitzer: Not installed
2024-09-07 23:49:25,670:INFO:PyCaret optional dependencies:
2024-09-07 23:49:25,694:INFO:                shap: Not installed
2024-09-07 23:49:25,694:INFO:           interpret: Not installed
2024-09-07 23:49:25,694:INFO:                umap: Not installed
2024-09-07 23:49:25,694:INFO:     ydata_profiling: Not installed
2024-09-07 23:49:25,694:INFO:  explainerdashboard: Not installed
2024-09-07 23:49:25,694:INFO:             autoviz: Not installed
2024-09-07 23:49:25,694:INFO:           fairlearn: Not installed
2024-09-07 23:49:25,694:INFO:          deepchecks: Not installed
2024-09-07 23:49:25,694:INFO:             xgboost: 2.1.1
2024-09-07 23:49:25,694:INFO:            catboost: Not installed
2024-09-07 23:49:25,694:INFO:              kmodes: Not installed
2024-09-07 23:49:25,694:INFO:             mlxtend: Not installed
2024-09-07 23:49:25,694:INFO:       statsforecast: Not installed
2024-09-07 23:49:25,694:INFO:        tune_sklearn: Not installed
2024-09-07 23:49:25,694:INFO:                 ray: Not installed
2024-09-07 23:49:25,694:INFO:            hyperopt: 0.2.7
2024-09-07 23:49:25,694:INFO:              optuna: 4.0.0
2024-09-07 23:49:25,694:INFO:               skopt: 0.10.2
2024-09-07 23:49:25,694:INFO:              mlflow: Not installed
2024-09-07 23:49:25,694:INFO:              gradio: Not installed
2024-09-07 23:49:25,694:INFO:             fastapi: Not installed
2024-09-07 23:49:25,694:INFO:             uvicorn: Not installed
2024-09-07 23:49:25,694:INFO:              m2cgen: Not installed
2024-09-07 23:49:25,694:INFO:           evidently: Not installed
2024-09-07 23:49:25,702:INFO:               fugue: Not installed
2024-09-07 23:49:25,703:INFO:           streamlit: 1.38.0
2024-09-07 23:49:25,703:INFO:             prophet: Not installed
2024-09-07 23:49:25,703:INFO:None
2024-09-07 23:49:25,704:INFO:Set up data.
2024-09-07 23:49:25,997:INFO:Set up folding strategy.
2024-09-07 23:49:25,999:INFO:Set up train/test split.
2024-09-07 23:49:26,104:INFO:Set up index.
2024-09-07 23:49:26,104:INFO:Assigning column types.
2024-09-07 23:49:26,154:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-07 23:49:26,261:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-07 23:49:26,269:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-07 23:49:26,354:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-07 23:49:26,362:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-07 23:49:26,469:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-07 23:49:26,469:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-07 23:49:26,542:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-07 23:49:26,551:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-07 23:49:26,551:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-07 23:49:26,665:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-07 23:49:26,732:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-07 23:49:26,739:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-07 23:49:26,846:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-07 23:49:26,920:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-07 23:49:26,920:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-07 23:49:26,920:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-07 23:49:27,108:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-07 23:49:27,116:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-07 23:49:27,288:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-07 23:49:27,298:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-07 23:49:27,313:INFO:Preparing preprocessing pipeline...
2024-09-07 23:49:27,322:INFO:Set up simple imputation.
2024-09-07 23:49:27,379:INFO:Set up encoding of ordinal features.
2024-09-07 23:49:27,518:INFO:Set up encoding of categorical features.
2024-09-07 23:49:27,527:INFO:Set up column name cleaning.
2024-09-07 23:49:28,486:INFO:Finished creating preprocessing pipeline.
2024-09-07 23:49:28,863:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                             'Fathers occupation'],
                                    transformer=TargetEncoder(cols=[],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-07 23:49:28,863:INFO:Creating final display dataframe.
2024-09-07 23:49:30,187:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape       (76518, 38)
4        Transformed data shape      (76518, 122)
5   Transformed train set shape      (53562, 122)
6    Transformed test set shape      (22956, 122)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              b39d
2024-09-07 23:49:30,398:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-07 23:49:30,406:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-07 23:49:30,589:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-07 23:49:30,596:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-07 23:49:30,601:INFO:setup() successfully completed in 5.19s...............
2024-09-07 23:49:30,602:INFO:Initializing compare_models()
2024-09-07 23:49:30,603:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-07 23:49:30,603:INFO:Checking exceptions
2024-09-07 23:49:30,666:INFO:Preparing display monitor
2024-09-07 23:49:30,674:INFO:Initializing Logistic Regression
2024-09-07 23:49:30,675:INFO:Total runtime is 1.6736984252929688e-05 minutes
2024-09-07 23:49:30,676:INFO:SubProcess create_model() called ==================================
2024-09-07 23:49:30,677:INFO:Initializing create_model()
2024-09-07 23:49:30,678:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-07 23:49:30,678:INFO:Checking exceptions
2024-09-07 23:49:30,678:INFO:Importing libraries
2024-09-07 23:49:30,678:INFO:Copying training dataset
2024-09-07 23:49:30,787:INFO:Defining folds
2024-09-07 23:49:30,787:INFO:Declaring metric variables
2024-09-07 23:49:30,788:INFO:Importing untrained model
2024-09-07 23:49:30,788:INFO:Logistic Regression Imported successfully
2024-09-07 23:49:30,789:INFO:Starting cross validation
2024-09-07 23:49:30,801:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-07 23:51:11,425:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-07 23:51:12,113:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:51:12,300:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-07 23:51:12,488:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-07 23:51:13,036:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:51:13,082:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:51:16,599:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-07 23:51:17,210:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:51:19,993:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-07 23:51:20,493:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:51:20,775:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-07 23:51:21,228:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:51:21,400:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-07 23:51:21,509:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-07 23:51:21,884:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:51:21,962:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:52:10,523:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-07 23:52:10,933:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:52:11,700:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-07 23:52:12,071:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:52:12,128:INFO:Calculating mean and std
2024-09-07 23:52:12,131:INFO:Creating metrics dataframe
2024-09-07 23:52:12,135:INFO:Uploading results into container
2024-09-07 23:52:12,135:INFO:Uploading model into container now
2024-09-07 23:52:12,135:INFO:_master_model_container: 1
2024-09-07 23:52:12,135:INFO:_display_container: 2
2024-09-07 23:52:12,135:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-07 23:52:12,141:INFO:create_model() successfully completed......................................
2024-09-07 23:52:12,322:INFO:SubProcess create_model() end ==================================
2024-09-07 23:52:12,322:INFO:Creating metrics dataframe
2024-09-07 23:52:12,331:INFO:Initializing K Neighbors Classifier
2024-09-07 23:52:12,331:INFO:Total runtime is 2.694281442960103 minutes
2024-09-07 23:52:12,331:INFO:SubProcess create_model() called ==================================
2024-09-07 23:52:12,331:INFO:Initializing create_model()
2024-09-07 23:52:12,334:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-07 23:52:12,334:INFO:Checking exceptions
2024-09-07 23:52:12,334:INFO:Importing libraries
2024-09-07 23:52:12,334:INFO:Copying training dataset
2024-09-07 23:52:12,446:INFO:Defining folds
2024-09-07 23:52:12,446:INFO:Declaring metric variables
2024-09-07 23:52:12,446:INFO:Importing untrained model
2024-09-07 23:52:12,446:INFO:K Neighbors Classifier Imported successfully
2024-09-07 23:52:12,446:INFO:Starting cross validation
2024-09-07 23:52:12,453:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-07 23:53:02,586:INFO:Calculating mean and std
2024-09-07 23:53:02,586:INFO:Creating metrics dataframe
2024-09-07 23:53:02,586:INFO:Uploading results into container
2024-09-07 23:53:02,586:INFO:Uploading model into container now
2024-09-07 23:53:02,586:INFO:_master_model_container: 2
2024-09-07 23:53:02,586:INFO:_display_container: 2
2024-09-07 23:53:02,586:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-07 23:53:02,586:INFO:create_model() successfully completed......................................
2024-09-07 23:53:02,736:INFO:SubProcess create_model() end ==================================
2024-09-07 23:53:02,736:INFO:Creating metrics dataframe
2024-09-07 23:53:02,751:INFO:Initializing Naive Bayes
2024-09-07 23:53:02,751:INFO:Total runtime is 3.5346181392669673 minutes
2024-09-07 23:53:02,751:INFO:SubProcess create_model() called ==================================
2024-09-07 23:53:02,751:INFO:Initializing create_model()
2024-09-07 23:53:02,751:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-07 23:53:02,751:INFO:Checking exceptions
2024-09-07 23:53:02,751:INFO:Importing libraries
2024-09-07 23:53:02,751:INFO:Copying training dataset
2024-09-07 23:53:02,830:INFO:Defining folds
2024-09-07 23:53:02,830:INFO:Declaring metric variables
2024-09-07 23:53:02,830:INFO:Importing untrained model
2024-09-07 23:53:02,830:INFO:Naive Bayes Imported successfully
2024-09-07 23:53:02,830:INFO:Starting cross validation
2024-09-07 23:53:02,846:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-07 23:53:13,117:INFO:Calculating mean and std
2024-09-07 23:53:13,117:INFO:Creating metrics dataframe
2024-09-07 23:53:13,117:INFO:Uploading results into container
2024-09-07 23:53:13,117:INFO:Uploading model into container now
2024-09-07 23:53:13,117:INFO:_master_model_container: 3
2024-09-07 23:53:13,117:INFO:_display_container: 2
2024-09-07 23:53:13,117:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-07 23:53:13,117:INFO:create_model() successfully completed......................................
2024-09-07 23:53:13,257:INFO:SubProcess create_model() end ==================================
2024-09-07 23:53:13,257:INFO:Creating metrics dataframe
2024-09-07 23:53:13,257:INFO:Initializing Decision Tree Classifier
2024-09-07 23:53:13,257:INFO:Total runtime is 3.709722606341044 minutes
2024-09-07 23:53:13,257:INFO:SubProcess create_model() called ==================================
2024-09-07 23:53:13,273:INFO:Initializing create_model()
2024-09-07 23:53:13,273:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-07 23:53:13,273:INFO:Checking exceptions
2024-09-07 23:53:13,273:INFO:Importing libraries
2024-09-07 23:53:13,273:INFO:Copying training dataset
2024-09-07 23:53:13,363:INFO:Defining folds
2024-09-07 23:53:13,363:INFO:Declaring metric variables
2024-09-07 23:53:13,363:INFO:Importing untrained model
2024-09-07 23:53:13,367:INFO:Decision Tree Classifier Imported successfully
2024-09-07 23:53:13,367:INFO:Starting cross validation
2024-09-07 23:53:13,367:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-07 23:53:28,069:INFO:Calculating mean and std
2024-09-07 23:53:28,069:INFO:Creating metrics dataframe
2024-09-07 23:53:28,069:INFO:Uploading results into container
2024-09-07 23:53:28,069:INFO:Uploading model into container now
2024-09-07 23:53:28,069:INFO:_master_model_container: 4
2024-09-07 23:53:28,069:INFO:_display_container: 2
2024-09-07 23:53:28,069:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-07 23:53:28,069:INFO:create_model() successfully completed......................................
2024-09-07 23:53:28,226:INFO:SubProcess create_model() end ==================================
2024-09-07 23:53:28,226:INFO:Creating metrics dataframe
2024-09-07 23:53:28,226:INFO:Initializing SVM - Linear Kernel
2024-09-07 23:53:28,226:INFO:Total runtime is 3.9591910401980077 minutes
2024-09-07 23:53:28,226:INFO:SubProcess create_model() called ==================================
2024-09-07 23:53:28,226:INFO:Initializing create_model()
2024-09-07 23:53:28,226:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-07 23:53:28,226:INFO:Checking exceptions
2024-09-07 23:53:28,226:INFO:Importing libraries
2024-09-07 23:53:28,226:INFO:Copying training dataset
2024-09-07 23:53:28,319:INFO:Defining folds
2024-09-07 23:53:28,319:INFO:Declaring metric variables
2024-09-07 23:53:28,319:INFO:Importing untrained model
2024-09-07 23:53:28,319:INFO:SVM - Linear Kernel Imported successfully
2024-09-07 23:53:28,319:INFO:Starting cross validation
2024-09-07 23:53:28,319:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-07 23:54:30,385:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:33,450:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:34,022:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:34,294:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:36,079:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:37,344:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:38,611:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:38,830:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:50,845:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:51,158:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:51,221:INFO:Calculating mean and std
2024-09-07 23:54:51,221:INFO:Creating metrics dataframe
2024-09-07 23:54:51,221:INFO:Uploading results into container
2024-09-07 23:54:51,221:INFO:Uploading model into container now
2024-09-07 23:54:51,221:INFO:_master_model_container: 5
2024-09-07 23:54:51,221:INFO:_display_container: 2
2024-09-07 23:54:51,221:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-07 23:54:51,221:INFO:create_model() successfully completed......................................
2024-09-07 23:54:51,362:INFO:SubProcess create_model() end ==================================
2024-09-07 23:54:51,362:INFO:Creating metrics dataframe
2024-09-07 23:54:51,377:INFO:Initializing Ridge Classifier
2024-09-07 23:54:51,377:INFO:Total runtime is 5.34505230585734 minutes
2024-09-07 23:54:51,377:INFO:SubProcess create_model() called ==================================
2024-09-07 23:54:51,377:INFO:Initializing create_model()
2024-09-07 23:54:51,377:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-07 23:54:51,377:INFO:Checking exceptions
2024-09-07 23:54:51,377:INFO:Importing libraries
2024-09-07 23:54:51,377:INFO:Copying training dataset
2024-09-07 23:54:51,455:INFO:Defining folds
2024-09-07 23:54:51,455:INFO:Declaring metric variables
2024-09-07 23:54:51,455:INFO:Importing untrained model
2024-09-07 23:54:51,455:INFO:Ridge Classifier Imported successfully
2024-09-07 23:54:51,455:INFO:Starting cross validation
2024-09-07 23:54:51,468:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-07 23:54:57,973:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:58,071:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:58,177:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:58,255:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:58,800:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:58,825:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:58,913:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:54:59,084:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:55:02,199:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:55:02,256:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:55:02,302:INFO:Calculating mean and std
2024-09-07 23:55:02,305:INFO:Creating metrics dataframe
2024-09-07 23:55:02,309:INFO:Uploading results into container
2024-09-07 23:55:02,309:INFO:Uploading model into container now
2024-09-07 23:55:02,309:INFO:_master_model_container: 6
2024-09-07 23:55:02,309:INFO:_display_container: 2
2024-09-07 23:55:02,309:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-07 23:55:02,309:INFO:create_model() successfully completed......................................
2024-09-07 23:55:02,524:INFO:SubProcess create_model() end ==================================
2024-09-07 23:55:02,524:INFO:Creating metrics dataframe
2024-09-07 23:55:02,541:INFO:Initializing Random Forest Classifier
2024-09-07 23:55:02,541:INFO:Total runtime is 5.531111617883046 minutes
2024-09-07 23:55:02,541:INFO:SubProcess create_model() called ==================================
2024-09-07 23:55:02,541:INFO:Initializing create_model()
2024-09-07 23:55:02,541:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-07 23:55:02,541:INFO:Checking exceptions
2024-09-07 23:55:02,541:INFO:Importing libraries
2024-09-07 23:55:02,541:INFO:Copying training dataset
2024-09-07 23:55:02,624:INFO:Defining folds
2024-09-07 23:55:02,624:INFO:Declaring metric variables
2024-09-07 23:55:02,624:INFO:Importing untrained model
2024-09-07 23:55:02,624:INFO:Random Forest Classifier Imported successfully
2024-09-07 23:55:02,633:INFO:Starting cross validation
2024-09-07 23:55:02,642:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-07 23:55:59,916:INFO:Calculating mean and std
2024-09-07 23:55:59,920:INFO:Creating metrics dataframe
2024-09-07 23:55:59,925:INFO:Uploading results into container
2024-09-07 23:55:59,925:INFO:Uploading model into container now
2024-09-07 23:55:59,925:INFO:_master_model_container: 7
2024-09-07 23:55:59,925:INFO:_display_container: 2
2024-09-07 23:55:59,930:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-07 23:55:59,930:INFO:create_model() successfully completed......................................
2024-09-07 23:56:00,211:INFO:SubProcess create_model() end ==================================
2024-09-07 23:56:00,211:INFO:Creating metrics dataframe
2024-09-07 23:56:00,220:INFO:Initializing Quadratic Discriminant Analysis
2024-09-07 23:56:00,220:INFO:Total runtime is 6.492439103126525 minutes
2024-09-07 23:56:00,220:INFO:SubProcess create_model() called ==================================
2024-09-07 23:56:00,220:INFO:Initializing create_model()
2024-09-07 23:56:00,220:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-07 23:56:00,220:INFO:Checking exceptions
2024-09-07 23:56:00,220:INFO:Importing libraries
2024-09-07 23:56:00,220:INFO:Copying training dataset
2024-09-07 23:56:00,345:INFO:Defining folds
2024-09-07 23:56:00,345:INFO:Declaring metric variables
2024-09-07 23:56:00,345:INFO:Importing untrained model
2024-09-07 23:56:00,345:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-07 23:56:00,345:INFO:Starting cross validation
2024-09-07 23:56:00,356:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-07 23:56:08,184:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-07 23:56:08,193:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-07 23:56:08,823:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-07 23:56:09,785:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-07 23:56:11,871:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-07 23:56:11,978:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-07 23:56:12,287:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:12,474:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:13,466:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:13,966:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-07 23:56:14,542:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:15,725:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-07 23:56:16,533:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:16,607:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:16,863:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:17,777:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:19,057:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-07 23:56:19,270:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-07 23:56:20,757:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:20,871:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:20,935:INFO:Calculating mean and std
2024-09-07 23:56:20,937:INFO:Creating metrics dataframe
2024-09-07 23:56:20,937:INFO:Uploading results into container
2024-09-07 23:56:20,937:INFO:Uploading model into container now
2024-09-07 23:56:20,943:INFO:_master_model_container: 8
2024-09-07 23:56:20,943:INFO:_display_container: 2
2024-09-07 23:56:20,943:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-07 23:56:20,943:INFO:create_model() successfully completed......................................
2024-09-07 23:56:21,124:INFO:SubProcess create_model() end ==================================
2024-09-07 23:56:21,124:INFO:Creating metrics dataframe
2024-09-07 23:56:21,132:INFO:Initializing Ada Boost Classifier
2024-09-07 23:56:21,132:INFO:Total runtime is 6.8409697055816645 minutes
2024-09-07 23:56:21,137:INFO:SubProcess create_model() called ==================================
2024-09-07 23:56:21,137:INFO:Initializing create_model()
2024-09-07 23:56:21,137:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-07 23:56:21,137:INFO:Checking exceptions
2024-09-07 23:56:21,137:INFO:Importing libraries
2024-09-07 23:56:21,137:INFO:Copying training dataset
2024-09-07 23:56:21,249:INFO:Defining folds
2024-09-07 23:56:21,249:INFO:Declaring metric variables
2024-09-07 23:56:21,249:INFO:Importing untrained model
2024-09-07 23:56:21,249:INFO:Ada Boost Classifier Imported successfully
2024-09-07 23:56:21,249:INFO:Starting cross validation
2024-09-07 23:56:21,257:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-07 23:56:25,897:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-07 23:56:26,012:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-07 23:56:26,017:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-07 23:56:26,120:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-07 23:56:28,640:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-07 23:56:28,805:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-07 23:56:30,521:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-07 23:56:30,991:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-07 23:56:44,569:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:44,879:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:46,999:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:49,253:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-07 23:56:49,904:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:50,829:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-07 23:56:53,710:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:55,399:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:56:55,642:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:57:03,635:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:57:04,733:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-07 23:57:04,791:INFO:Calculating mean and std
2024-09-07 23:57:04,791:INFO:Creating metrics dataframe
2024-09-07 23:57:04,791:INFO:Uploading results into container
2024-09-07 23:57:04,791:INFO:Uploading model into container now
2024-09-07 23:57:04,791:INFO:_master_model_container: 9
2024-09-07 23:57:04,791:INFO:_display_container: 2
2024-09-07 23:57:04,799:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-07 23:57:04,799:INFO:create_model() successfully completed......................................
2024-09-07 23:57:04,960:INFO:SubProcess create_model() end ==================================
2024-09-07 23:57:04,960:INFO:Creating metrics dataframe
2024-09-07 23:57:04,963:INFO:Initializing Gradient Boosting Classifier
2024-09-07 23:57:04,963:INFO:Total runtime is 7.571484537919362 minutes
2024-09-07 23:57:04,963:INFO:SubProcess create_model() called ==================================
2024-09-07 23:57:04,963:INFO:Initializing create_model()
2024-09-07 23:57:04,963:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-07 23:57:04,963:INFO:Checking exceptions
2024-09-07 23:57:04,963:INFO:Importing libraries
2024-09-07 23:57:04,963:INFO:Copying training dataset
2024-09-07 23:57:05,050:INFO:Defining folds
2024-09-07 23:57:05,050:INFO:Declaring metric variables
2024-09-07 23:57:05,050:INFO:Importing untrained model
2024-09-07 23:57:05,055:INFO:Gradient Boosting Classifier Imported successfully
2024-09-07 23:57:05,055:INFO:Starting cross validation
2024-09-07 23:57:05,064:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-07 23:59:51,672:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:00:05,987:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:00:13,084:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:00:28,119:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:00:29,271:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:00:29,271:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:00:33,986:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:00:38,119:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:03,945:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:07,672:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:07,727:INFO:Calculating mean and std
2024-09-08 00:02:07,727:INFO:Creating metrics dataframe
2024-09-08 00:02:07,735:INFO:Uploading results into container
2024-09-08 00:02:07,735:INFO:Uploading model into container now
2024-09-08 00:02:07,735:INFO:_master_model_container: 10
2024-09-08 00:02:07,735:INFO:_display_container: 2
2024-09-08 00:02:07,735:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 00:02:07,735:INFO:create_model() successfully completed......................................
2024-09-08 00:02:07,907:INFO:SubProcess create_model() end ==================================
2024-09-08 00:02:07,907:INFO:Creating metrics dataframe
2024-09-08 00:02:07,912:INFO:Initializing Linear Discriminant Analysis
2024-09-08 00:02:07,912:INFO:Total runtime is 12.620639300346374 minutes
2024-09-08 00:02:07,912:INFO:SubProcess create_model() called ==================================
2024-09-08 00:02:07,912:INFO:Initializing create_model()
2024-09-08 00:02:07,916:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:02:07,916:INFO:Checking exceptions
2024-09-08 00:02:07,916:INFO:Importing libraries
2024-09-08 00:02:07,916:INFO:Copying training dataset
2024-09-08 00:02:08,016:INFO:Defining folds
2024-09-08 00:02:08,016:INFO:Declaring metric variables
2024-09-08 00:02:08,016:INFO:Importing untrained model
2024-09-08 00:02:08,016:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 00:02:08,016:INFO:Starting cross validation
2024-09-08 00:02:08,024:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:02:23,084:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:23,194:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:23,257:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:23,284:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:23,294:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:23,404:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:23,614:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:23,789:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:28,771:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:28,964:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:02:29,003:INFO:Calculating mean and std
2024-09-08 00:02:29,004:INFO:Creating metrics dataframe
2024-09-08 00:02:29,004:INFO:Uploading results into container
2024-09-08 00:02:29,009:INFO:Uploading model into container now
2024-09-08 00:02:29,009:INFO:_master_model_container: 11
2024-09-08 00:02:29,009:INFO:_display_container: 2
2024-09-08 00:02:29,011:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 00:02:29,011:INFO:create_model() successfully completed......................................
2024-09-08 00:02:29,161:INFO:SubProcess create_model() end ==================================
2024-09-08 00:02:29,161:INFO:Creating metrics dataframe
2024-09-08 00:02:29,164:INFO:Initializing Extra Trees Classifier
2024-09-08 00:02:29,164:INFO:Total runtime is 12.974833178520202 minutes
2024-09-08 00:02:29,164:INFO:SubProcess create_model() called ==================================
2024-09-08 00:02:29,164:INFO:Initializing create_model()
2024-09-08 00:02:29,164:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:02:29,164:INFO:Checking exceptions
2024-09-08 00:02:29,164:INFO:Importing libraries
2024-09-08 00:02:29,164:INFO:Copying training dataset
2024-09-08 00:02:29,251:INFO:Defining folds
2024-09-08 00:02:29,251:INFO:Declaring metric variables
2024-09-08 00:02:29,254:INFO:Importing untrained model
2024-09-08 00:02:29,254:INFO:Extra Trees Classifier Imported successfully
2024-09-08 00:02:29,254:INFO:Starting cross validation
2024-09-08 00:02:29,264:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:03:35,387:INFO:Calculating mean and std
2024-09-08 00:03:35,389:INFO:Creating metrics dataframe
2024-09-08 00:03:35,394:INFO:Uploading results into container
2024-09-08 00:03:35,396:INFO:Uploading model into container now
2024-09-08 00:03:35,396:INFO:_master_model_container: 12
2024-09-08 00:03:35,396:INFO:_display_container: 2
2024-09-08 00:03:35,396:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 00:03:35,396:INFO:create_model() successfully completed......................................
2024-09-08 00:03:35,562:INFO:SubProcess create_model() end ==================================
2024-09-08 00:03:35,562:INFO:Creating metrics dataframe
2024-09-08 00:03:35,571:INFO:Initializing Extreme Gradient Boosting
2024-09-08 00:03:35,571:INFO:Total runtime is 14.081610945860543 minutes
2024-09-08 00:03:35,571:INFO:SubProcess create_model() called ==================================
2024-09-08 00:03:35,571:INFO:Initializing create_model()
2024-09-08 00:03:35,571:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:03:35,571:INFO:Checking exceptions
2024-09-08 00:03:35,571:INFO:Importing libraries
2024-09-08 00:03:35,571:INFO:Copying training dataset
2024-09-08 00:03:35,671:INFO:Defining folds
2024-09-08 00:03:35,671:INFO:Declaring metric variables
2024-09-08 00:03:35,672:INFO:Importing untrained model
2024-09-08 00:03:35,672:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 00:03:35,672:INFO:Starting cross validation
2024-09-08 00:03:35,688:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:04:15,517:INFO:Calculating mean and std
2024-09-08 00:04:15,519:INFO:Creating metrics dataframe
2024-09-08 00:04:15,522:INFO:Uploading results into container
2024-09-08 00:04:15,525:INFO:Uploading model into container now
2024-09-08 00:04:15,525:INFO:_master_model_container: 13
2024-09-08 00:04:15,525:INFO:_display_container: 2
2024-09-08 00:04:15,527:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 00:04:15,527:INFO:create_model() successfully completed......................................
2024-09-08 00:04:15,677:INFO:SubProcess create_model() end ==================================
2024-09-08 00:04:15,677:INFO:Creating metrics dataframe
2024-09-08 00:04:15,687:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 00:04:15,687:INFO:Total runtime is 14.750214469432828 minutes
2024-09-08 00:04:15,687:INFO:SubProcess create_model() called ==================================
2024-09-08 00:04:15,687:INFO:Initializing create_model()
2024-09-08 00:04:15,687:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:04:15,687:INFO:Checking exceptions
2024-09-08 00:04:15,687:INFO:Importing libraries
2024-09-08 00:04:15,687:INFO:Copying training dataset
2024-09-08 00:04:15,793:INFO:Defining folds
2024-09-08 00:04:15,794:INFO:Declaring metric variables
2024-09-08 00:04:15,794:INFO:Importing untrained model
2024-09-08 00:04:15,794:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 00:04:15,794:INFO:Starting cross validation
2024-09-08 00:04:15,802:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:04:43,936:INFO:Calculating mean and std
2024-09-08 00:04:43,936:INFO:Creating metrics dataframe
2024-09-08 00:04:43,936:INFO:Uploading results into container
2024-09-08 00:04:43,936:INFO:Uploading model into container now
2024-09-08 00:04:43,944:INFO:_master_model_container: 14
2024-09-08 00:04:43,944:INFO:_display_container: 2
2024-09-08 00:04:43,944:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 00:04:43,944:INFO:create_model() successfully completed......................................
2024-09-08 00:04:44,104:INFO:SubProcess create_model() end ==================================
2024-09-08 00:04:44,104:INFO:Creating metrics dataframe
2024-09-08 00:04:44,122:INFO:Initializing Dummy Classifier
2024-09-08 00:04:44,122:INFO:Total runtime is 15.224134206771849 minutes
2024-09-08 00:04:44,122:INFO:SubProcess create_model() called ==================================
2024-09-08 00:04:44,122:INFO:Initializing create_model()
2024-09-08 00:04:44,124:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002AB10DA7AF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:04:44,124:INFO:Checking exceptions
2024-09-08 00:04:44,124:INFO:Importing libraries
2024-09-08 00:04:44,124:INFO:Copying training dataset
2024-09-08 00:04:44,209:INFO:Defining folds
2024-09-08 00:04:44,209:INFO:Declaring metric variables
2024-09-08 00:04:44,209:INFO:Importing untrained model
2024-09-08 00:04:44,209:INFO:Dummy Classifier Imported successfully
2024-09-08 00:04:44,209:INFO:Starting cross validation
2024-09-08 00:04:44,224:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:04:49,249:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 00:04:49,324:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 00:04:49,429:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 00:04:49,831:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 00:04:50,318:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 00:04:50,474:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 00:04:50,526:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 00:04:50,582:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 00:04:52,793:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 00:04:52,809:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 00:04:52,833:INFO:Calculating mean and std
2024-09-08 00:04:52,833:INFO:Creating metrics dataframe
2024-09-08 00:04:52,833:INFO:Uploading results into container
2024-09-08 00:04:52,840:INFO:Uploading model into container now
2024-09-08 00:04:52,840:INFO:_master_model_container: 15
2024-09-08 00:04:52,841:INFO:_display_container: 2
2024-09-08 00:04:52,841:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 00:04:52,841:INFO:create_model() successfully completed......................................
2024-09-08 00:04:53,000:INFO:SubProcess create_model() end ==================================
2024-09-08 00:04:53,000:INFO:Creating metrics dataframe
2024-09-08 00:04:53,010:INFO:Initializing create_model()
2024-09-08 00:04:53,010:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:04:53,010:INFO:Checking exceptions
2024-09-08 00:04:53,010:INFO:Importing libraries
2024-09-08 00:04:53,010:INFO:Copying training dataset
2024-09-08 00:04:53,090:INFO:Defining folds
2024-09-08 00:04:53,090:INFO:Declaring metric variables
2024-09-08 00:04:53,090:INFO:Importing untrained model
2024-09-08 00:04:53,090:INFO:Declaring custom model
2024-09-08 00:04:53,090:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 00:04:53,105:INFO:Cross validation set to False
2024-09-08 00:04:53,105:INFO:Fitting Model
2024-09-08 00:04:55,295:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 00:04:55,720:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 00:04:55,752:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007536 seconds.
2024-09-08 00:04:55,752:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 00:04:55,752:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 00:04:55,752:INFO:[LightGBM] [Info] Total Bins 1658
2024-09-08 00:04:55,760:INFO:[LightGBM] [Info] Number of data points in the train set: 53562, number of used features: 91
2024-09-08 00:04:55,762:INFO:[LightGBM] [Info] Start training from score -0.746209
2024-09-08 00:04:55,762:INFO:[LightGBM] [Info] Start training from score -1.106880
2024-09-08 00:04:55,762:INFO:[LightGBM] [Info] Start training from score -1.633473
2024-09-08 00:04:57,265:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 00:04:57,265:INFO:create_model() successfully completed......................................
2024-09-08 00:04:57,424:INFO:Initializing create_model()
2024-09-08 00:04:57,424:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:04:57,430:INFO:Checking exceptions
2024-09-08 00:04:57,430:INFO:Importing libraries
2024-09-08 00:04:57,432:INFO:Copying training dataset
2024-09-08 00:04:57,510:INFO:Defining folds
2024-09-08 00:04:57,510:INFO:Declaring metric variables
2024-09-08 00:04:57,510:INFO:Importing untrained model
2024-09-08 00:04:57,510:INFO:Declaring custom model
2024-09-08 00:04:57,513:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 00:04:57,521:INFO:Cross validation set to False
2024-09-08 00:04:57,521:INFO:Fitting Model
2024-09-08 00:04:59,690:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 00:05:04,330:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 00:05:04,330:INFO:create_model() successfully completed......................................
2024-09-08 00:05:04,527:INFO:Initializing create_model()
2024-09-08 00:05:04,527:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:05:04,527:INFO:Checking exceptions
2024-09-08 00:05:04,527:INFO:Importing libraries
2024-09-08 00:05:04,527:INFO:Copying training dataset
2024-09-08 00:05:04,642:INFO:Defining folds
2024-09-08 00:05:04,642:INFO:Declaring metric variables
2024-09-08 00:05:04,642:INFO:Importing untrained model
2024-09-08 00:05:04,642:INFO:Declaring custom model
2024-09-08 00:05:04,642:INFO:Random Forest Classifier Imported successfully
2024-09-08 00:05:04,650:INFO:Cross validation set to False
2024-09-08 00:05:04,650:INFO:Fitting Model
2024-09-08 00:05:06,921:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 00:05:11,511:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 00:05:11,511:INFO:create_model() successfully completed......................................
2024-09-08 00:05:11,790:INFO:_master_model_container: 15
2024-09-08 00:05:11,790:INFO:_display_container: 2
2024-09-08 00:05:11,794:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)]
2024-09-08 00:05:11,794:INFO:compare_models() successfully completed......................................
2024-09-08 00:05:11,796:INFO:Initializing create_model()
2024-09-08 00:05:11,796:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:05:11,796:INFO:Checking exceptions
2024-09-08 00:05:11,799:INFO:Importing libraries
2024-09-08 00:05:11,801:INFO:Copying training dataset
2024-09-08 00:05:11,881:INFO:Defining folds
2024-09-08 00:05:11,881:INFO:Declaring metric variables
2024-09-08 00:05:11,881:INFO:Importing untrained model
2024-09-08 00:05:11,881:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 00:05:11,886:INFO:Starting cross validation
2024-09-08 00:05:11,891:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:05:42,445:INFO:Calculating mean and std
2024-09-08 00:05:42,445:INFO:Creating metrics dataframe
2024-09-08 00:05:42,454:INFO:Finalizing model
2024-09-08 00:05:44,811:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 00:05:45,258:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 00:05:45,298:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012219 seconds.
2024-09-08 00:05:45,298:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 00:05:45,298:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 00:05:45,298:INFO:[LightGBM] [Info] Total Bins 1658
2024-09-08 00:05:45,298:INFO:[LightGBM] [Info] Number of data points in the train set: 53562, number of used features: 91
2024-09-08 00:05:45,302:INFO:[LightGBM] [Info] Start training from score -0.746209
2024-09-08 00:05:45,302:INFO:[LightGBM] [Info] Start training from score -1.106880
2024-09-08 00:05:45,302:INFO:[LightGBM] [Info] Start training from score -1.633473
2024-09-08 00:05:47,348:INFO:Uploading results into container
2024-09-08 00:05:47,348:INFO:Uploading model into container now
2024-09-08 00:05:47,381:INFO:_master_model_container: 16
2024-09-08 00:05:47,381:INFO:_display_container: 3
2024-09-08 00:05:47,381:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 00:05:47,381:INFO:create_model() successfully completed......................................
2024-09-08 00:05:47,684:INFO:Initializing create_model()
2024-09-08 00:05:47,689:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:05:47,689:INFO:Checking exceptions
2024-09-08 00:05:47,692:INFO:Importing libraries
2024-09-08 00:05:47,692:INFO:Copying training dataset
2024-09-08 00:05:47,818:INFO:Defining folds
2024-09-08 00:05:47,818:INFO:Declaring metric variables
2024-09-08 00:05:47,818:INFO:Importing untrained model
2024-09-08 00:05:47,818:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 00:05:47,824:INFO:Starting cross validation
2024-09-08 00:05:47,833:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:06:26,933:INFO:Calculating mean and std
2024-09-08 00:06:26,933:INFO:Creating metrics dataframe
2024-09-08 00:06:26,936:INFO:Finalizing model
2024-09-08 00:06:29,147:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 00:06:33,511:INFO:Uploading results into container
2024-09-08 00:06:33,511:INFO:Uploading model into container now
2024-09-08 00:06:33,537:INFO:_master_model_container: 17
2024-09-08 00:06:33,537:INFO:_display_container: 4
2024-09-08 00:06:33,544:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 00:06:33,544:INFO:create_model() successfully completed......................................
2024-09-08 00:06:33,717:INFO:Initializing create_model()
2024-09-08 00:06:33,717:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A9808EEAA0>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:06:33,718:INFO:Checking exceptions
2024-09-08 00:06:33,719:INFO:Importing libraries
2024-09-08 00:06:33,719:INFO:Copying training dataset
2024-09-08 00:06:33,809:INFO:Defining folds
2024-09-08 00:06:33,809:INFO:Declaring metric variables
2024-09-08 00:06:33,809:INFO:Importing untrained model
2024-09-08 00:06:33,809:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 00:06:33,818:INFO:Starting cross validation
2024-09-08 00:06:33,826:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:09:43,455:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:09:54,724:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:10:02,988:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:10:05,578:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:10:16,072:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:10:16,607:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:10:17,565:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:10:18,094:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:11:45,370:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:11:52,142:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:11:52,191:INFO:Calculating mean and std
2024-09-08 00:11:52,194:INFO:Creating metrics dataframe
2024-09-08 00:11:52,194:INFO:Finalizing model
2024-09-08 00:11:54,451:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 00:13:46,523:INFO:Uploading results into container
2024-09-08 00:13:46,523:INFO:Uploading model into container now
2024-09-08 00:13:46,560:INFO:_master_model_container: 18
2024-09-08 00:13:46,560:INFO:_display_container: 5
2024-09-08 00:13:46,560:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 00:13:46,560:INFO:create_model() successfully completed......................................
2024-09-08 00:39:18,258:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 00:39:18,258:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 00:39:18,258:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 00:39:18,263:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 00:39:45,869:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 00:39:45,869:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 00:39:45,879:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 00:39:45,879:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 00:41:37,561:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 00:41:37,561:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 00:41:37,561:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 00:41:37,566:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 00:48:34,761:INFO:PyCaret ClassificationExperiment
2024-09-08 00:48:34,761:INFO:Logging name: clf-default-name
2024-09-08 00:48:34,761:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 00:48:34,761:INFO:version 3.3.2
2024-09-08 00:48:34,761:INFO:Initializing setup()
2024-09-08 00:48:34,761:INFO:self.USI: d84f
2024-09-08 00:48:34,761:INFO:self._variable_keys: {'html_param', 'exp_id', 'USI', 'logging_param', 'is_multiclass', 'fold_generator', 'X_test', 'exp_name_log', 'gpu_n_jobs_param', 'fix_imbalance', '_ml_usecase', 'pipeline', 'y_test', '_available_plots', 'fold_groups_param', 'log_plots_param', 'X', 'target_param', 'data', 'y', 'seed', 'gpu_param', 'X_train', 'memory', 'fold_shuffle_param', 'idx', 'n_jobs_param', 'y_train'}
2024-09-08 00:48:34,761:INFO:Checking environment
2024-09-08 00:48:34,761:INFO:python_version: 3.10.11
2024-09-08 00:48:34,761:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 00:48:34,761:INFO:machine: AMD64
2024-09-08 00:48:34,795:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 00:48:34,795:INFO:Memory: svmem(total=16407719936, available=1743781888, percent=89.4, used=14663938048, free=1743781888)
2024-09-08 00:48:34,795:INFO:Physical Core: 4
2024-09-08 00:48:34,795:INFO:Logical Core: 8
2024-09-08 00:48:34,795:INFO:Checking libraries
2024-09-08 00:48:34,795:INFO:System:
2024-09-08 00:48:34,795:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 00:48:34,795:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 00:48:34,795:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 00:48:34,795:INFO:PyCaret required dependencies:
2024-09-08 00:48:34,977:INFO:                 pip: 24.2
2024-09-08 00:48:34,977:INFO:          setuptools: 72.1.0
2024-09-08 00:48:34,977:INFO:             pycaret: 3.3.2
2024-09-08 00:48:34,977:INFO:             IPython: 8.25.0
2024-09-08 00:48:34,977:INFO:          ipywidgets: 8.1.5
2024-09-08 00:48:34,977:INFO:                tqdm: 4.66.5
2024-09-08 00:48:34,977:INFO:               numpy: 1.26.4
2024-09-08 00:48:34,977:INFO:              pandas: 2.1.4
2024-09-08 00:48:34,977:INFO:              jinja2: 3.1.4
2024-09-08 00:48:34,977:INFO:               scipy: 1.11.4
2024-09-08 00:48:34,977:INFO:              joblib: 1.3.2
2024-09-08 00:48:34,977:INFO:             sklearn: 1.4.2
2024-09-08 00:48:34,977:INFO:                pyod: 2.0.1
2024-09-08 00:48:34,977:INFO:            imblearn: 0.12.3
2024-09-08 00:48:34,977:INFO:   category_encoders: 2.6.3
2024-09-08 00:48:34,977:INFO:            lightgbm: 4.5.0
2024-09-08 00:48:34,977:INFO:               numba: 0.60.0
2024-09-08 00:48:34,977:INFO:            requests: 2.32.3
2024-09-08 00:48:34,977:INFO:          matplotlib: 3.7.5
2024-09-08 00:48:34,977:INFO:          scikitplot: 0.3.7
2024-09-08 00:48:34,977:INFO:         yellowbrick: 1.5
2024-09-08 00:48:34,977:INFO:              plotly: 5.24.0
2024-09-08 00:48:34,977:INFO:    plotly-resampler: Not installed
2024-09-08 00:48:34,980:INFO:             kaleido: 0.2.1
2024-09-08 00:48:34,980:INFO:           schemdraw: 0.15
2024-09-08 00:48:34,980:INFO:         statsmodels: 0.14.2
2024-09-08 00:48:34,980:INFO:              sktime: 0.26.0
2024-09-08 00:48:34,980:INFO:               tbats: 1.1.3
2024-09-08 00:48:34,980:INFO:            pmdarima: 2.0.4
2024-09-08 00:48:34,980:INFO:              psutil: 5.9.0
2024-09-08 00:48:34,980:INFO:          markupsafe: 2.1.3
2024-09-08 00:48:34,980:INFO:             pickle5: Not installed
2024-09-08 00:48:34,980:INFO:         cloudpickle: 3.0.0
2024-09-08 00:48:34,980:INFO:         deprecation: 2.1.0
2024-09-08 00:48:34,980:INFO:              xxhash: 3.5.0
2024-09-08 00:48:34,980:INFO:           wurlitzer: Not installed
2024-09-08 00:48:34,980:INFO:PyCaret optional dependencies:
2024-09-08 00:48:35,020:INFO:                shap: Not installed
2024-09-08 00:48:35,020:INFO:           interpret: Not installed
2024-09-08 00:48:35,020:INFO:                umap: Not installed
2024-09-08 00:48:35,020:INFO:     ydata_profiling: Not installed
2024-09-08 00:48:35,020:INFO:  explainerdashboard: Not installed
2024-09-08 00:48:35,020:INFO:             autoviz: Not installed
2024-09-08 00:48:35,020:INFO:           fairlearn: Not installed
2024-09-08 00:48:35,020:INFO:          deepchecks: Not installed
2024-09-08 00:48:35,020:INFO:             xgboost: 2.1.1
2024-09-08 00:48:35,020:INFO:            catboost: Not installed
2024-09-08 00:48:35,020:INFO:              kmodes: Not installed
2024-09-08 00:48:35,020:INFO:             mlxtend: Not installed
2024-09-08 00:48:35,020:INFO:       statsforecast: Not installed
2024-09-08 00:48:35,020:INFO:        tune_sklearn: Not installed
2024-09-08 00:48:35,020:INFO:                 ray: Not installed
2024-09-08 00:48:35,020:INFO:            hyperopt: 0.2.7
2024-09-08 00:48:35,020:INFO:              optuna: 4.0.0
2024-09-08 00:48:35,020:INFO:               skopt: 0.10.2
2024-09-08 00:48:35,020:INFO:              mlflow: Not installed
2024-09-08 00:48:35,020:INFO:              gradio: Not installed
2024-09-08 00:48:35,020:INFO:             fastapi: Not installed
2024-09-08 00:48:35,020:INFO:             uvicorn: Not installed
2024-09-08 00:48:35,020:INFO:              m2cgen: Not installed
2024-09-08 00:48:35,020:INFO:           evidently: Not installed
2024-09-08 00:48:35,020:INFO:               fugue: Not installed
2024-09-08 00:48:35,020:INFO:           streamlit: 1.38.0
2024-09-08 00:48:35,020:INFO:             prophet: Not installed
2024-09-08 00:48:35,020:INFO:None
2024-09-08 00:48:35,020:INFO:Set up data.
2024-09-08 00:48:35,330:INFO:Set up folding strategy.
2024-09-08 00:48:35,330:INFO:Set up train/test split.
2024-09-08 00:48:35,460:INFO:Set up index.
2024-09-08 00:48:35,461:INFO:Assigning column types.
2024-09-08 00:48:35,511:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 00:48:35,626:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 00:48:35,643:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 00:48:35,739:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 00:48:35,750:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 00:48:35,875:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 00:48:35,880:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 00:48:35,960:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 00:48:35,961:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 00:48:35,961:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 00:48:36,090:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 00:48:36,165:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 00:48:36,175:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 00:48:36,294:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 00:48:36,376:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 00:48:36,384:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 00:48:36,384:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 00:48:36,600:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 00:48:36,603:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 00:48:36,790:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 00:48:36,794:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 00:48:36,810:INFO:Preparing preprocessing pipeline...
2024-09-08 00:48:36,825:INFO:Set up simple imputation.
2024-09-08 00:48:36,890:INFO:Set up encoding of ordinal features.
2024-09-08 00:48:37,060:INFO:Set up encoding of categorical features.
2024-09-08 00:48:37,073:INFO:Set up column name cleaning.
2024-09-08 00:48:38,310:INFO:Finished creating preprocessing pipeline.
2024-09-08 00:48:38,981:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                             'Fathers occupation'],
                                    transformer=TargetEncoder(cols=[],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 00:48:38,981:INFO:Creating final display dataframe.
2024-09-08 00:48:40,330:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape       (76518, 38)
4        Transformed data shape      (76518, 122)
5   Transformed train set shape      (53562, 122)
6    Transformed test set shape      (22956, 122)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              d84f
2024-09-08 00:48:40,570:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 00:48:40,577:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 00:48:40,760:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 00:48:40,770:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 00:48:40,770:INFO:setup() successfully completed in 6.06s...............
2024-09-08 00:48:40,770:INFO:Initializing compare_models()
2024-09-08 00:48:40,770:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 00:48:40,775:INFO:Checking exceptions
2024-09-08 00:48:40,840:INFO:Preparing display monitor
2024-09-08 00:48:40,850:INFO:Initializing Logistic Regression
2024-09-08 00:48:40,850:INFO:Total runtime is 0.0 minutes
2024-09-08 00:48:40,850:INFO:SubProcess create_model() called ==================================
2024-09-08 00:48:40,852:INFO:Initializing create_model()
2024-09-08 00:48:40,852:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:48:40,852:INFO:Checking exceptions
2024-09-08 00:48:40,852:INFO:Importing libraries
2024-09-08 00:48:40,852:INFO:Copying training dataset
2024-09-08 00:48:40,955:INFO:Defining folds
2024-09-08 00:48:40,955:INFO:Declaring metric variables
2024-09-08 00:48:40,955:INFO:Importing untrained model
2024-09-08 00:48:40,955:INFO:Logistic Regression Imported successfully
2024-09-08 00:48:40,955:INFO:Starting cross validation
2024-09-08 00:48:40,963:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:50:19,485:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 00:50:20,549:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:50:32,027:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 00:50:32,763:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:50:43,289:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 00:50:43,754:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:50:49,134:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 00:50:49,595:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:50:50,228:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 00:50:50,960:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:50:53,355:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 00:50:53,795:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:50:54,450:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 00:50:54,515:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 00:50:55,145:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:50:55,173:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:51:42,302:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 00:51:42,810:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:51:54,753:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 00:51:55,181:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:51:55,231:INFO:Calculating mean and std
2024-09-08 00:51:55,231:INFO:Creating metrics dataframe
2024-09-08 00:51:55,247:INFO:Uploading results into container
2024-09-08 00:51:55,247:INFO:Uploading model into container now
2024-09-08 00:51:55,247:INFO:_master_model_container: 1
2024-09-08 00:51:55,249:INFO:_display_container: 2
2024-09-08 00:51:55,249:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 00:51:55,249:INFO:create_model() successfully completed......................................
2024-09-08 00:51:55,434:INFO:SubProcess create_model() end ==================================
2024-09-08 00:51:55,436:INFO:Creating metrics dataframe
2024-09-08 00:51:55,440:INFO:Initializing K Neighbors Classifier
2024-09-08 00:51:55,440:INFO:Total runtime is 3.243161896864573 minutes
2024-09-08 00:51:55,440:INFO:SubProcess create_model() called ==================================
2024-09-08 00:51:55,440:INFO:Initializing create_model()
2024-09-08 00:51:55,440:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:51:55,440:INFO:Checking exceptions
2024-09-08 00:51:55,440:INFO:Importing libraries
2024-09-08 00:51:55,440:INFO:Copying training dataset
2024-09-08 00:51:55,525:INFO:Defining folds
2024-09-08 00:51:55,525:INFO:Declaring metric variables
2024-09-08 00:51:55,525:INFO:Importing untrained model
2024-09-08 00:51:55,525:INFO:K Neighbors Classifier Imported successfully
2024-09-08 00:51:55,525:INFO:Starting cross validation
2024-09-08 00:51:55,530:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:53:01,324:INFO:Calculating mean and std
2024-09-08 00:53:01,324:INFO:Creating metrics dataframe
2024-09-08 00:53:01,334:INFO:Uploading results into container
2024-09-08 00:53:01,334:INFO:Uploading model into container now
2024-09-08 00:53:01,334:INFO:_master_model_container: 2
2024-09-08 00:53:01,334:INFO:_display_container: 2
2024-09-08 00:53:01,334:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 00:53:01,334:INFO:create_model() successfully completed......................................
2024-09-08 00:53:01,574:INFO:SubProcess create_model() end ==================================
2024-09-08 00:53:01,574:INFO:Creating metrics dataframe
2024-09-08 00:53:01,584:INFO:Initializing Naive Bayes
2024-09-08 00:53:01,584:INFO:Total runtime is 4.345567015806834 minutes
2024-09-08 00:53:01,584:INFO:SubProcess create_model() called ==================================
2024-09-08 00:53:01,584:INFO:Initializing create_model()
2024-09-08 00:53:01,584:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:53:01,584:INFO:Checking exceptions
2024-09-08 00:53:01,584:INFO:Importing libraries
2024-09-08 00:53:01,584:INFO:Copying training dataset
2024-09-08 00:53:01,779:INFO:Defining folds
2024-09-08 00:53:01,779:INFO:Declaring metric variables
2024-09-08 00:53:01,782:INFO:Importing untrained model
2024-09-08 00:53:01,782:INFO:Naive Bayes Imported successfully
2024-09-08 00:53:01,782:INFO:Starting cross validation
2024-09-08 00:53:01,794:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:53:15,472:INFO:Calculating mean and std
2024-09-08 00:53:15,472:INFO:Creating metrics dataframe
2024-09-08 00:53:15,482:INFO:Uploading results into container
2024-09-08 00:53:15,482:INFO:Uploading model into container now
2024-09-08 00:53:15,482:INFO:_master_model_container: 3
2024-09-08 00:53:15,487:INFO:_display_container: 2
2024-09-08 00:53:15,487:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 00:53:15,487:INFO:create_model() successfully completed......................................
2024-09-08 00:53:15,742:INFO:SubProcess create_model() end ==================================
2024-09-08 00:53:15,742:INFO:Creating metrics dataframe
2024-09-08 00:53:15,759:INFO:Initializing Decision Tree Classifier
2024-09-08 00:53:15,759:INFO:Total runtime is 4.581814646720886 minutes
2024-09-08 00:53:15,759:INFO:SubProcess create_model() called ==================================
2024-09-08 00:53:15,761:INFO:Initializing create_model()
2024-09-08 00:53:15,761:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:53:15,761:INFO:Checking exceptions
2024-09-08 00:53:15,761:INFO:Importing libraries
2024-09-08 00:53:15,761:INFO:Copying training dataset
2024-09-08 00:53:15,920:INFO:Defining folds
2024-09-08 00:53:15,920:INFO:Declaring metric variables
2024-09-08 00:53:15,920:INFO:Importing untrained model
2024-09-08 00:53:15,924:INFO:Decision Tree Classifier Imported successfully
2024-09-08 00:53:15,924:INFO:Starting cross validation
2024-09-08 00:53:15,941:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:53:35,930:INFO:Calculating mean and std
2024-09-08 00:53:35,932:INFO:Creating metrics dataframe
2024-09-08 00:53:35,938:INFO:Uploading results into container
2024-09-08 00:53:35,942:INFO:Uploading model into container now
2024-09-08 00:53:35,946:INFO:_master_model_container: 4
2024-09-08 00:53:35,946:INFO:_display_container: 2
2024-09-08 00:53:35,946:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 00:53:35,946:INFO:create_model() successfully completed......................................
2024-09-08 00:53:36,222:INFO:SubProcess create_model() end ==================================
2024-09-08 00:53:36,222:INFO:Creating metrics dataframe
2024-09-08 00:53:36,237:INFO:Initializing SVM - Linear Kernel
2024-09-08 00:53:36,237:INFO:Total runtime is 4.923109682401021 minutes
2024-09-08 00:53:36,237:INFO:SubProcess create_model() called ==================================
2024-09-08 00:53:36,237:INFO:Initializing create_model()
2024-09-08 00:53:36,237:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:53:36,237:INFO:Checking exceptions
2024-09-08 00:53:36,237:INFO:Importing libraries
2024-09-08 00:53:36,237:INFO:Copying training dataset
2024-09-08 00:53:36,455:INFO:Defining folds
2024-09-08 00:53:36,455:INFO:Declaring metric variables
2024-09-08 00:53:36,455:INFO:Importing untrained model
2024-09-08 00:53:36,455:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 00:53:36,455:INFO:Starting cross validation
2024-09-08 00:53:36,497:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:54:31,589:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:54:31,848:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:54:40,773:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:54:41,022:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:54:46,380:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:54:46,753:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:54:47,387:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:54:47,677:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:54:54,938:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:54:56,473:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:54:56,556:INFO:Calculating mean and std
2024-09-08 00:54:56,563:INFO:Creating metrics dataframe
2024-09-08 00:54:56,564:INFO:Uploading results into container
2024-09-08 00:54:56,564:INFO:Uploading model into container now
2024-09-08 00:54:56,564:INFO:_master_model_container: 5
2024-09-08 00:54:56,573:INFO:_display_container: 2
2024-09-08 00:54:56,573:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 00:54:56,573:INFO:create_model() successfully completed......................................
2024-09-08 00:54:56,788:INFO:SubProcess create_model() end ==================================
2024-09-08 00:54:56,788:INFO:Creating metrics dataframe
2024-09-08 00:54:56,793:INFO:Initializing Ridge Classifier
2024-09-08 00:54:56,793:INFO:Total runtime is 6.265712475776672 minutes
2024-09-08 00:54:56,793:INFO:SubProcess create_model() called ==================================
2024-09-08 00:54:56,798:INFO:Initializing create_model()
2024-09-08 00:54:56,798:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:54:56,798:INFO:Checking exceptions
2024-09-08 00:54:56,798:INFO:Importing libraries
2024-09-08 00:54:56,798:INFO:Copying training dataset
2024-09-08 00:54:56,955:INFO:Defining folds
2024-09-08 00:54:56,955:INFO:Declaring metric variables
2024-09-08 00:54:56,955:INFO:Importing untrained model
2024-09-08 00:54:56,955:INFO:Ridge Classifier Imported successfully
2024-09-08 00:54:56,955:INFO:Starting cross validation
2024-09-08 00:54:56,971:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:55:02,274:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:55:03,244:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:55:04,576:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:55:05,164:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:55:05,734:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:55:05,866:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:55:05,940:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:55:05,982:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:55:07,918:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:55:08,640:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:55:08,690:INFO:Calculating mean and std
2024-09-08 00:55:08,692:INFO:Creating metrics dataframe
2024-09-08 00:55:08,699:INFO:Uploading results into container
2024-09-08 00:55:08,699:INFO:Uploading model into container now
2024-09-08 00:55:08,699:INFO:_master_model_container: 6
2024-09-08 00:55:08,699:INFO:_display_container: 2
2024-09-08 00:55:08,704:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 00:55:08,704:INFO:create_model() successfully completed......................................
2024-09-08 00:55:08,939:INFO:SubProcess create_model() end ==================================
2024-09-08 00:55:08,939:INFO:Creating metrics dataframe
2024-09-08 00:55:08,949:INFO:Initializing Random Forest Classifier
2024-09-08 00:55:08,949:INFO:Total runtime is 6.468312454223632 minutes
2024-09-08 00:55:08,949:INFO:SubProcess create_model() called ==================================
2024-09-08 00:55:08,949:INFO:Initializing create_model()
2024-09-08 00:55:08,949:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:55:08,949:INFO:Checking exceptions
2024-09-08 00:55:08,949:INFO:Importing libraries
2024-09-08 00:55:08,949:INFO:Copying training dataset
2024-09-08 00:55:09,093:INFO:Defining folds
2024-09-08 00:55:09,093:INFO:Declaring metric variables
2024-09-08 00:55:09,093:INFO:Importing untrained model
2024-09-08 00:55:09,093:INFO:Random Forest Classifier Imported successfully
2024-09-08 00:55:09,093:INFO:Starting cross validation
2024-09-08 00:55:09,109:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:55:59,055:INFO:Calculating mean and std
2024-09-08 00:55:59,055:INFO:Creating metrics dataframe
2024-09-08 00:55:59,063:INFO:Uploading results into container
2024-09-08 00:55:59,067:INFO:Uploading model into container now
2024-09-08 00:55:59,067:INFO:_master_model_container: 7
2024-09-08 00:55:59,067:INFO:_display_container: 2
2024-09-08 00:55:59,067:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 00:55:59,067:INFO:create_model() successfully completed......................................
2024-09-08 00:55:59,287:INFO:SubProcess create_model() end ==================================
2024-09-08 00:55:59,287:INFO:Creating metrics dataframe
2024-09-08 00:55:59,303:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 00:55:59,303:INFO:Total runtime is 7.3075382987658175 minutes
2024-09-08 00:55:59,303:INFO:SubProcess create_model() called ==================================
2024-09-08 00:55:59,303:INFO:Initializing create_model()
2024-09-08 00:55:59,303:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:55:59,303:INFO:Checking exceptions
2024-09-08 00:55:59,303:INFO:Importing libraries
2024-09-08 00:55:59,303:INFO:Copying training dataset
2024-09-08 00:55:59,454:INFO:Defining folds
2024-09-08 00:55:59,454:INFO:Declaring metric variables
2024-09-08 00:55:59,460:INFO:Importing untrained model
2024-09-08 00:55:59,460:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 00:55:59,463:INFO:Starting cross validation
2024-09-08 00:55:59,473:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:56:05,752:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 00:56:07,448:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 00:56:09,999:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:10,042:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 00:56:10,089:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 00:56:12,200:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:12,742:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 00:56:12,792:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 00:56:12,802:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:13,629:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 00:56:13,799:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:15,189:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:15,254:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:15,449:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:16,529:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 00:56:16,932:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:18,026:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:19,572:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 00:56:20,913:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:20,963:INFO:Calculating mean and std
2024-09-08 00:56:20,969:INFO:Creating metrics dataframe
2024-09-08 00:56:20,971:INFO:Uploading results into container
2024-09-08 00:56:20,980:INFO:Uploading model into container now
2024-09-08 00:56:20,980:INFO:_master_model_container: 8
2024-09-08 00:56:20,980:INFO:_display_container: 2
2024-09-08 00:56:20,980:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 00:56:20,980:INFO:create_model() successfully completed......................................
2024-09-08 00:56:21,239:INFO:SubProcess create_model() end ==================================
2024-09-08 00:56:21,239:INFO:Creating metrics dataframe
2024-09-08 00:56:21,253:INFO:Initializing Ada Boost Classifier
2024-09-08 00:56:21,253:INFO:Total runtime is 7.6733827153841645 minutes
2024-09-08 00:56:21,254:INFO:SubProcess create_model() called ==================================
2024-09-08 00:56:21,254:INFO:Initializing create_model()
2024-09-08 00:56:21,255:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:56:21,255:INFO:Checking exceptions
2024-09-08 00:56:21,255:INFO:Importing libraries
2024-09-08 00:56:21,255:INFO:Copying training dataset
2024-09-08 00:56:21,409:INFO:Defining folds
2024-09-08 00:56:21,409:INFO:Declaring metric variables
2024-09-08 00:56:21,409:INFO:Importing untrained model
2024-09-08 00:56:21,414:INFO:Ada Boost Classifier Imported successfully
2024-09-08 00:56:21,416:INFO:Starting cross validation
2024-09-08 00:56:21,429:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 00:56:24,900:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 00:56:27,410:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 00:56:27,810:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 00:56:28,161:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 00:56:29,275:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 00:56:29,291:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 00:56:29,316:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 00:56:29,363:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 00:56:44,626:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:45,811:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:48,727:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 00:56:50,161:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 00:56:52,256:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:55,963:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:58,301:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:59,167:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:59,218:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:56:59,271:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:57:03,718:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:57:04,942:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 00:57:04,984:INFO:Calculating mean and std
2024-09-08 00:57:04,987:INFO:Creating metrics dataframe
2024-09-08 00:57:04,987:INFO:Uploading results into container
2024-09-08 00:57:04,993:INFO:Uploading model into container now
2024-09-08 00:57:04,993:INFO:_master_model_container: 9
2024-09-08 00:57:04,993:INFO:_display_container: 2
2024-09-08 00:57:04,993:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 00:57:04,993:INFO:create_model() successfully completed......................................
2024-09-08 00:57:05,208:INFO:SubProcess create_model() end ==================================
2024-09-08 00:57:05,208:INFO:Creating metrics dataframe
2024-09-08 00:57:05,224:INFO:Initializing Gradient Boosting Classifier
2024-09-08 00:57:05,224:INFO:Total runtime is 8.406236604849497 minutes
2024-09-08 00:57:05,224:INFO:SubProcess create_model() called ==================================
2024-09-08 00:57:05,224:INFO:Initializing create_model()
2024-09-08 00:57:05,228:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 00:57:05,228:INFO:Checking exceptions
2024-09-08 00:57:05,228:INFO:Importing libraries
2024-09-08 00:57:05,228:INFO:Copying training dataset
2024-09-08 00:57:05,360:INFO:Defining folds
2024-09-08 00:57:05,360:INFO:Declaring metric variables
2024-09-08 00:57:05,360:INFO:Importing untrained model
2024-09-08 00:57:05,369:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 00:57:05,369:INFO:Starting cross validation
2024-09-08 00:57:05,380:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:00:10,748:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:00:13,776:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:00:15,817:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:00:18,433:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:00:19,233:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:00:19,903:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:00:21,644:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:00:28,562:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:09,170:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:10,570:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:10,619:INFO:Calculating mean and std
2024-09-08 01:02:10,620:INFO:Creating metrics dataframe
2024-09-08 01:02:10,627:INFO:Uploading results into container
2024-09-08 01:02:10,627:INFO:Uploading model into container now
2024-09-08 01:02:10,630:INFO:_master_model_container: 10
2024-09-08 01:02:10,630:INFO:_display_container: 2
2024-09-08 01:02:10,630:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 01:02:10,630:INFO:create_model() successfully completed......................................
2024-09-08 01:02:10,794:INFO:SubProcess create_model() end ==================================
2024-09-08 01:02:10,794:INFO:Creating metrics dataframe
2024-09-08 01:02:10,814:INFO:Initializing Linear Discriminant Analysis
2024-09-08 01:02:10,814:INFO:Total runtime is 13.49939239025116 minutes
2024-09-08 01:02:10,814:INFO:SubProcess create_model() called ==================================
2024-09-08 01:02:10,814:INFO:Initializing create_model()
2024-09-08 01:02:10,814:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:02:10,814:INFO:Checking exceptions
2024-09-08 01:02:10,814:INFO:Importing libraries
2024-09-08 01:02:10,814:INFO:Copying training dataset
2024-09-08 01:02:10,901:INFO:Defining folds
2024-09-08 01:02:10,901:INFO:Declaring metric variables
2024-09-08 01:02:10,901:INFO:Importing untrained model
2024-09-08 01:02:10,901:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 01:02:10,901:INFO:Starting cross validation
2024-09-08 01:02:10,910:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:02:26,302:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:26,752:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:26,875:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:27,291:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:27,303:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:27,569:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:27,577:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:27,621:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:33,766:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:33,924:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:02:33,972:INFO:Calculating mean and std
2024-09-08 01:02:33,973:INFO:Creating metrics dataframe
2024-09-08 01:02:33,973:INFO:Uploading results into container
2024-09-08 01:02:33,982:INFO:Uploading model into container now
2024-09-08 01:02:33,982:INFO:_master_model_container: 11
2024-09-08 01:02:33,982:INFO:_display_container: 2
2024-09-08 01:02:33,982:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 01:02:33,982:INFO:create_model() successfully completed......................................
2024-09-08 01:02:34,172:INFO:SubProcess create_model() end ==================================
2024-09-08 01:02:34,172:INFO:Creating metrics dataframe
2024-09-08 01:02:34,188:INFO:Initializing Extra Trees Classifier
2024-09-08 01:02:34,188:INFO:Total runtime is 13.888962817192077 minutes
2024-09-08 01:02:34,192:INFO:SubProcess create_model() called ==================================
2024-09-08 01:02:34,192:INFO:Initializing create_model()
2024-09-08 01:02:34,192:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:02:34,192:INFO:Checking exceptions
2024-09-08 01:02:34,192:INFO:Importing libraries
2024-09-08 01:02:34,194:INFO:Copying training dataset
2024-09-08 01:02:34,343:INFO:Defining folds
2024-09-08 01:02:34,343:INFO:Declaring metric variables
2024-09-08 01:02:34,343:INFO:Importing untrained model
2024-09-08 01:02:34,343:INFO:Extra Trees Classifier Imported successfully
2024-09-08 01:02:34,343:INFO:Starting cross validation
2024-09-08 01:02:34,359:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:03:51,705:INFO:Calculating mean and std
2024-09-08 01:03:51,705:INFO:Creating metrics dataframe
2024-09-08 01:03:51,713:INFO:Uploading results into container
2024-09-08 01:03:51,713:INFO:Uploading model into container now
2024-09-08 01:03:51,717:INFO:_master_model_container: 12
2024-09-08 01:03:51,717:INFO:_display_container: 2
2024-09-08 01:03:51,717:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 01:03:51,723:INFO:create_model() successfully completed......................................
2024-09-08 01:03:51,997:INFO:SubProcess create_model() end ==================================
2024-09-08 01:03:51,999:INFO:Creating metrics dataframe
2024-09-08 01:03:52,008:INFO:Initializing Extreme Gradient Boosting
2024-09-08 01:03:52,008:INFO:Total runtime is 15.185960598786671 minutes
2024-09-08 01:03:52,008:INFO:SubProcess create_model() called ==================================
2024-09-08 01:03:52,008:INFO:Initializing create_model()
2024-09-08 01:03:52,008:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:03:52,008:INFO:Checking exceptions
2024-09-08 01:03:52,008:INFO:Importing libraries
2024-09-08 01:03:52,008:INFO:Copying training dataset
2024-09-08 01:03:52,139:INFO:Defining folds
2024-09-08 01:03:52,139:INFO:Declaring metric variables
2024-09-08 01:03:52,139:INFO:Importing untrained model
2024-09-08 01:03:52,143:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 01:03:52,144:INFO:Starting cross validation
2024-09-08 01:03:52,155:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:04:32,675:INFO:Calculating mean and std
2024-09-08 01:04:32,675:INFO:Creating metrics dataframe
2024-09-08 01:04:32,681:INFO:Uploading results into container
2024-09-08 01:04:32,682:INFO:Uploading model into container now
2024-09-08 01:04:32,682:INFO:_master_model_container: 13
2024-09-08 01:04:32,682:INFO:_display_container: 2
2024-09-08 01:04:32,686:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 01:04:32,686:INFO:create_model() successfully completed......................................
2024-09-08 01:04:32,836:INFO:SubProcess create_model() end ==================================
2024-09-08 01:04:32,840:INFO:Creating metrics dataframe
2024-09-08 01:04:32,856:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 01:04:32,856:INFO:Total runtime is 15.866759665807088 minutes
2024-09-08 01:04:32,856:INFO:SubProcess create_model() called ==================================
2024-09-08 01:04:32,856:INFO:Initializing create_model()
2024-09-08 01:04:32,856:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:04:32,856:INFO:Checking exceptions
2024-09-08 01:04:32,860:INFO:Importing libraries
2024-09-08 01:04:32,860:INFO:Copying training dataset
2024-09-08 01:04:32,998:INFO:Defining folds
2024-09-08 01:04:32,998:INFO:Declaring metric variables
2024-09-08 01:04:32,998:INFO:Importing untrained model
2024-09-08 01:04:33,006:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 01:04:33,008:INFO:Starting cross validation
2024-09-08 01:04:33,016:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:05:02,523:INFO:Calculating mean and std
2024-09-08 01:05:02,523:INFO:Creating metrics dataframe
2024-09-08 01:05:02,531:INFO:Uploading results into container
2024-09-08 01:05:02,533:INFO:Uploading model into container now
2024-09-08 01:05:02,533:INFO:_master_model_container: 14
2024-09-08 01:05:02,533:INFO:_display_container: 2
2024-09-08 01:05:02,533:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 01:05:02,533:INFO:create_model() successfully completed......................................
2024-09-08 01:05:02,723:INFO:SubProcess create_model() end ==================================
2024-09-08 01:05:02,723:INFO:Creating metrics dataframe
2024-09-08 01:05:02,733:INFO:Initializing Dummy Classifier
2024-09-08 01:05:02,737:INFO:Total runtime is 16.364784117539724 minutes
2024-09-08 01:05:02,737:INFO:SubProcess create_model() called ==================================
2024-09-08 01:05:02,737:INFO:Initializing create_model()
2024-09-08 01:05:02,737:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB2B4AB30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:05:02,737:INFO:Checking exceptions
2024-09-08 01:05:02,737:INFO:Importing libraries
2024-09-08 01:05:02,737:INFO:Copying training dataset
2024-09-08 01:05:02,833:INFO:Defining folds
2024-09-08 01:05:02,833:INFO:Declaring metric variables
2024-09-08 01:05:02,837:INFO:Importing untrained model
2024-09-08 01:05:02,837:INFO:Dummy Classifier Imported successfully
2024-09-08 01:05:02,837:INFO:Starting cross validation
2024-09-08 01:05:02,844:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:05:08,472:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:05:08,645:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:05:08,705:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:05:08,914:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:05:09,854:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:05:09,921:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:05:09,977:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:05:10,008:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:05:12,336:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:05:12,351:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:05:12,379:INFO:Calculating mean and std
2024-09-08 01:05:12,381:INFO:Creating metrics dataframe
2024-09-08 01:05:12,387:INFO:Uploading results into container
2024-09-08 01:05:12,387:INFO:Uploading model into container now
2024-09-08 01:05:12,390:INFO:_master_model_container: 15
2024-09-08 01:05:12,390:INFO:_display_container: 2
2024-09-08 01:05:12,390:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 01:05:12,390:INFO:create_model() successfully completed......................................
2024-09-08 01:05:12,549:INFO:SubProcess create_model() end ==================================
2024-09-08 01:05:12,549:INFO:Creating metrics dataframe
2024-09-08 01:05:12,570:INFO:Initializing create_model()
2024-09-08 01:05:12,570:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:05:12,573:INFO:Checking exceptions
2024-09-08 01:05:12,573:INFO:Importing libraries
2024-09-08 01:05:12,573:INFO:Copying training dataset
2024-09-08 01:05:12,682:INFO:Defining folds
2024-09-08 01:05:12,682:INFO:Declaring metric variables
2024-09-08 01:05:12,682:INFO:Importing untrained model
2024-09-08 01:05:12,682:INFO:Declaring custom model
2024-09-08 01:05:12,682:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 01:05:12,690:INFO:Cross validation set to False
2024-09-08 01:05:12,690:INFO:Fitting Model
2024-09-08 01:05:14,951:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:05:15,440:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 01:05:15,476:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010684 seconds.
2024-09-08 01:05:15,476:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 01:05:15,476:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 01:05:15,476:INFO:[LightGBM] [Info] Total Bins 1658
2024-09-08 01:05:15,479:INFO:[LightGBM] [Info] Number of data points in the train set: 53562, number of used features: 91
2024-09-08 01:05:15,480:INFO:[LightGBM] [Info] Start training from score -0.746209
2024-09-08 01:05:15,480:INFO:[LightGBM] [Info] Start training from score -1.106880
2024-09-08 01:05:15,480:INFO:[LightGBM] [Info] Start training from score -1.633473
2024-09-08 01:05:17,634:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 01:05:17,634:INFO:create_model() successfully completed......................................
2024-09-08 01:05:17,827:INFO:Initializing create_model()
2024-09-08 01:05:17,827:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:05:17,827:INFO:Checking exceptions
2024-09-08 01:05:17,827:INFO:Importing libraries
2024-09-08 01:05:17,827:INFO:Copying training dataset
2024-09-08 01:05:17,914:INFO:Defining folds
2024-09-08 01:05:17,914:INFO:Declaring metric variables
2024-09-08 01:05:17,914:INFO:Importing untrained model
2024-09-08 01:05:17,914:INFO:Declaring custom model
2024-09-08 01:05:17,921:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 01:05:17,931:INFO:Cross validation set to False
2024-09-08 01:05:17,931:INFO:Fitting Model
2024-09-08 01:05:20,421:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:05:24,870:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 01:05:24,870:INFO:create_model() successfully completed......................................
2024-09-08 01:05:25,080:INFO:Initializing create_model()
2024-09-08 01:05:25,080:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:05:25,080:INFO:Checking exceptions
2024-09-08 01:05:25,085:INFO:Importing libraries
2024-09-08 01:05:25,085:INFO:Copying training dataset
2024-09-08 01:05:25,225:INFO:Defining folds
2024-09-08 01:05:25,225:INFO:Declaring metric variables
2024-09-08 01:05:25,231:INFO:Importing untrained model
2024-09-08 01:05:25,231:INFO:Declaring custom model
2024-09-08 01:05:25,231:INFO:Random Forest Classifier Imported successfully
2024-09-08 01:05:25,240:INFO:Cross validation set to False
2024-09-08 01:05:25,240:INFO:Fitting Model
2024-09-08 01:05:27,606:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:05:33,746:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 01:05:33,746:INFO:create_model() successfully completed......................................
2024-09-08 01:05:34,048:INFO:_master_model_container: 15
2024-09-08 01:05:34,048:INFO:_display_container: 2
2024-09-08 01:05:34,048:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)]
2024-09-08 01:05:34,056:INFO:compare_models() successfully completed......................................
2024-09-08 01:05:34,056:INFO:Initializing create_model()
2024-09-08 01:05:34,056:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:05:34,056:INFO:Checking exceptions
2024-09-08 01:05:34,056:INFO:Importing libraries
2024-09-08 01:05:34,056:INFO:Copying training dataset
2024-09-08 01:05:34,148:INFO:Defining folds
2024-09-08 01:05:34,148:INFO:Declaring metric variables
2024-09-08 01:05:34,148:INFO:Importing untrained model
2024-09-08 01:05:34,157:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 01:05:34,157:INFO:Starting cross validation
2024-09-08 01:05:34,166:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:06:03,161:INFO:Calculating mean and std
2024-09-08 01:06:03,161:INFO:Creating metrics dataframe
2024-09-08 01:06:03,168:INFO:Finalizing model
2024-09-08 01:06:05,464:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:06:05,918:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 01:06:05,958:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011334 seconds.
2024-09-08 01:06:05,958:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 01:06:05,958:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 01:06:05,963:INFO:[LightGBM] [Info] Total Bins 1658
2024-09-08 01:06:05,964:INFO:[LightGBM] [Info] Number of data points in the train set: 53562, number of used features: 91
2024-09-08 01:06:05,966:INFO:[LightGBM] [Info] Start training from score -0.746209
2024-09-08 01:06:05,966:INFO:[LightGBM] [Info] Start training from score -1.106880
2024-09-08 01:06:05,966:INFO:[LightGBM] [Info] Start training from score -1.633473
2024-09-08 01:06:07,768:INFO:Uploading results into container
2024-09-08 01:06:07,768:INFO:Uploading model into container now
2024-09-08 01:06:07,808:INFO:_master_model_container: 16
2024-09-08 01:06:07,811:INFO:_display_container: 3
2024-09-08 01:06:07,811:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 01:06:07,811:INFO:create_model() successfully completed......................................
2024-09-08 01:06:07,972:INFO:Initializing create_model()
2024-09-08 01:06:07,972:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:06:07,972:INFO:Checking exceptions
2024-09-08 01:06:07,975:INFO:Importing libraries
2024-09-08 01:06:07,975:INFO:Copying training dataset
2024-09-08 01:06:08,056:INFO:Defining folds
2024-09-08 01:06:08,056:INFO:Declaring metric variables
2024-09-08 01:06:08,056:INFO:Importing untrained model
2024-09-08 01:06:08,059:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 01:06:08,059:INFO:Starting cross validation
2024-09-08 01:06:08,063:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:06:43,417:INFO:Calculating mean and std
2024-09-08 01:06:43,418:INFO:Creating metrics dataframe
2024-09-08 01:06:43,418:INFO:Finalizing model
2024-09-08 01:06:45,807:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:06:50,433:INFO:Uploading results into container
2024-09-08 01:06:50,437:INFO:Uploading model into container now
2024-09-08 01:06:50,467:INFO:_master_model_container: 17
2024-09-08 01:06:50,467:INFO:_display_container: 4
2024-09-08 01:06:50,467:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 01:06:50,472:INFO:create_model() successfully completed......................................
2024-09-08 01:06:50,650:INFO:Initializing create_model()
2024-09-08 01:06:50,650:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:06:50,650:INFO:Checking exceptions
2024-09-08 01:06:50,657:INFO:Importing libraries
2024-09-08 01:06:50,657:INFO:Copying training dataset
2024-09-08 01:06:50,782:INFO:Defining folds
2024-09-08 01:06:50,782:INFO:Declaring metric variables
2024-09-08 01:06:50,782:INFO:Importing untrained model
2024-09-08 01:06:50,782:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 01:06:50,782:INFO:Starting cross validation
2024-09-08 01:06:50,790:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:09:51,422:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:09:55,016:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:09:55,405:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:09:56,165:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:09:56,529:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:09:56,646:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:09:58,729:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:09:58,826:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:11:46,395:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:11:49,001:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:11:49,043:INFO:Calculating mean and std
2024-09-08 01:11:49,044:INFO:Creating metrics dataframe
2024-09-08 01:11:49,044:INFO:Finalizing model
2024-09-08 01:11:51,474:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:13:47,193:INFO:Uploading results into container
2024-09-08 01:13:47,196:INFO:Uploading model into container now
2024-09-08 01:13:47,229:INFO:_master_model_container: 18
2024-09-08 01:13:47,229:INFO:_display_container: 5
2024-09-08 01:13:47,233:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 01:13:47,236:INFO:create_model() successfully completed......................................
2024-09-08 01:13:47,418:INFO:Initializing tune_model()
2024-09-08 01:13:47,418:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>)
2024-09-08 01:13:47,418:INFO:Checking exceptions
2024-09-08 01:13:47,418:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 01:13:47,623:INFO:Copying training dataset
2024-09-08 01:13:47,693:INFO:Checking base model
2024-09-08 01:13:47,693:INFO:Base model : Extreme Gradient Boosting
2024-09-08 01:13:47,693:INFO:Declaring metric variables
2024-09-08 01:13:47,693:INFO:Defining Hyperparameters
2024-09-08 01:13:47,873:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 01:13:47,873:INFO:Tuning with n_jobs=-1
2024-09-08 01:13:47,893:INFO:Initializing skopt.BayesSearchCV
2024-09-08 01:16:22,916:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 1.0), ('actual_estimator__gamma', 0.3), ('actual_estimator__learning_rate', 0.2), ('actual_estimator__max_depth', 3), ('actual_estimator__min_child_weight', 5), ('actual_estimator__n_estimators', 50), ('actual_estimator__subsample', 0.6)])
2024-09-08 01:16:22,916:INFO:Hyperparameter search completed
2024-09-08 01:16:22,924:INFO:SubProcess create_model() called ==================================
2024-09-08 01:16:22,924:INFO:Initializing create_model()
2024-09-08 01:16:22,924:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002ABB34C8580>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 1.0, 'gamma': 0.3, 'learning_rate': 0.2, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 50, 'subsample': 0.6})
2024-09-08 01:16:22,924:INFO:Checking exceptions
2024-09-08 01:16:22,924:INFO:Importing libraries
2024-09-08 01:16:22,924:INFO:Copying training dataset
2024-09-08 01:16:23,034:INFO:Defining folds
2024-09-08 01:16:23,034:INFO:Declaring metric variables
2024-09-08 01:16:23,034:INFO:Importing untrained model
2024-09-08 01:16:23,034:INFO:Declaring custom model
2024-09-08 01:16:23,038:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 01:16:23,038:INFO:Starting cross validation
2024-09-08 01:16:23,047:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:16:33,605:INFO:Calculating mean and std
2024-09-08 01:16:33,605:INFO:Creating metrics dataframe
2024-09-08 01:16:33,613:INFO:Finalizing model
2024-09-08 01:16:35,917:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:16:38,075:INFO:Uploading results into container
2024-09-08 01:16:38,079:INFO:Uploading model into container now
2024-09-08 01:16:38,079:INFO:_master_model_container: 19
2024-09-08 01:16:38,079:INFO:_display_container: 6
2024-09-08 01:16:38,082:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=1.0, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0.3, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.2, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=3, max_leaves=None,
              min_child_weight=5, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=50, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 01:16:38,082:INFO:create_model() successfully completed......................................
2024-09-08 01:16:38,275:INFO:SubProcess create_model() end ==================================
2024-09-08 01:16:38,275:INFO:choose_better activated
2024-09-08 01:16:38,275:INFO:SubProcess create_model() called ==================================
2024-09-08 01:16:38,275:INFO:Initializing create_model()
2024-09-08 01:16:38,275:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:16:38,279:INFO:Checking exceptions
2024-09-08 01:16:38,280:INFO:Importing libraries
2024-09-08 01:16:38,280:INFO:Copying training dataset
2024-09-08 01:16:38,375:INFO:Defining folds
2024-09-08 01:16:38,375:INFO:Declaring metric variables
2024-09-08 01:16:38,378:INFO:Importing untrained model
2024-09-08 01:16:38,378:INFO:Declaring custom model
2024-09-08 01:16:38,380:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 01:16:38,380:INFO:Starting cross validation
2024-09-08 01:16:38,385:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:16:55,360:INFO:Calculating mean and std
2024-09-08 01:16:55,360:INFO:Creating metrics dataframe
2024-09-08 01:16:55,360:INFO:Finalizing model
2024-09-08 01:16:57,753:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:17:02,419:INFO:Uploading results into container
2024-09-08 01:17:02,419:INFO:Uploading model into container now
2024-09-08 01:17:02,419:INFO:_master_model_container: 20
2024-09-08 01:17:02,419:INFO:_display_container: 7
2024-09-08 01:17:02,419:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 01:17:02,419:INFO:create_model() successfully completed......................................
2024-09-08 01:17:02,617:INFO:SubProcess create_model() end ==================================
2024-09-08 01:17:02,619:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8279
2024-09-08 01:17:02,619:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=1.0, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0.3, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.2, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=3, max_leaves=None,
              min_child_weight=5, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=50, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.826
2024-09-08 01:17:02,619:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 01:17:02,619:INFO:choose_better completed
2024-09-08 01:17:02,628:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-09-08 01:17:02,660:INFO:_master_model_container: 20
2024-09-08 01:17:02,660:INFO:_display_container: 6
2024-09-08 01:17:02,667:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 01:17:02,667:INFO:tune_model() successfully completed......................................
2024-09-08 01:17:02,871:INFO:Initializing predict_model()
2024-09-08 01:17:02,871:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002AB8D8813F0>)
2024-09-08 01:17:02,871:INFO:Checking exceptions
2024-09-08 01:17:02,871:INFO:Preloading libraries
2024-09-08 01:17:04,262:INFO:Initializing get_config()
2024-09-08 01:17:04,262:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, variable=X_train)
2024-09-08 01:17:04,262:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 01:17:04,407:INFO:Variable:  returned as           id Marital status Application mode  ... Unemployment rate Inflation rate   GDP
63473  63473              1               39  ...              13.9           -0.3  0.79
58279  58279              1               39  ...              15.5            2.8 -4.06
68323  68323              1               18  ...              13.9           -0.3  0.79
27374  27374              1                1  ...              13.9           -0.3  0.79
46832  46832              1               17  ...              13.9           -0.3  0.79
...      ...            ...              ...  ...               ...            ...   ...
71758  71758              1                1  ...              12.7            3.7 -1.70
38253  38253              1               17  ...               9.4           -0.8 -3.12
71420  71420              1                1  ...              15.5            2.8 -4.06
74640  74640              1               17  ...               9.4           -0.8 -3.12
13710  13710              1               17  ...               8.9            1.4  3.51

[53562 rows x 37 columns]
2024-09-08 01:17:04,409:INFO:get_config() successfully completed......................................
2024-09-08 01:17:04,409:INFO:Initializing predict_model()
2024-09-08 01:17:04,409:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002AD149F8430>)
2024-09-08 01:17:04,409:INFO:Checking exceptions
2024-09-08 01:17:04,409:INFO:Preloading libraries
2024-09-08 01:17:04,409:INFO:Set up data.
2024-09-08 01:17:04,447:INFO:Set up index.
2024-09-08 01:17:05,988:INFO:Initializing get_config()
2024-09-08 01:17:05,988:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, variable=y_train)
2024-09-08 01:17:05,988:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 01:17:06,012:INFO:Variable:  returned as 63473    1
58279    1
68323    0
27374    0
46832    0
        ..
71758    1
38253    1
71420    0
74640    2
13710    0
Name: Target, Length: 53562, dtype: int8
2024-09-08 01:17:06,012:INFO:get_config() successfully completed......................................
2024-09-08 01:17:06,012:INFO:Initializing get_config()
2024-09-08 01:17:06,012:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, variable=y_test)
2024-09-08 01:17:06,012:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 01:17:06,028:INFO:Variable:  returned as 36387    0
29621    1
22002    1
4939     0
66101    0
        ..
65620    0
60576    0
10486    2
62584    2
39904    2
Name: Target, Length: 22956, dtype: int8
2024-09-08 01:17:06,034:INFO:get_config() successfully completed......................................
2024-09-08 01:17:06,034:INFO:Initializing finalize_model()
2024-09-08 01:17:06,034:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 01:17:06,034:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 01:17:06,068:INFO:Initializing create_model()
2024-09-08 01:17:06,068:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:17:06,068:INFO:Checking exceptions
2024-09-08 01:17:06,076:INFO:Importing libraries
2024-09-08 01:17:06,076:INFO:Copying training dataset
2024-09-08 01:17:06,078:INFO:Defining folds
2024-09-08 01:17:06,078:INFO:Declaring metric variables
2024-09-08 01:17:06,078:INFO:Importing untrained model
2024-09-08 01:17:06,078:INFO:Declaring custom model
2024-09-08 01:17:06,084:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 01:17:06,092:INFO:Cross validation set to False
2024-09-08 01:17:06,092:INFO:Fitting Model
2024-09-08 01:17:09,084:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:17:14,478:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=None, max_leaves=None,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=None, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 01:17:14,478:INFO:create_model() successfully completed......................................
2024-09-08 01:17:14,635:INFO:_master_model_container: 20
2024-09-08 01:17:14,635:INFO:_display_container: 7
2024-09-08 01:17:15,017:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=None, max_leaves=None,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=None, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 01:17:15,017:INFO:finalize_model() successfully completed......................................
2024-09-08 01:17:16,918:INFO:Initializing predict_model()
2024-09-08 01:17:16,918:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002ABB34DAAA0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=None, max_leaves=None,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=None, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002AB8D8815A0>)
2024-09-08 01:17:16,918:INFO:Checking exceptions
2024-09-08 01:17:16,918:INFO:Preloading libraries
2024-09-08 01:17:16,918:INFO:Set up data.
2024-09-08 01:17:17,132:INFO:Set up index.
2024-09-08 01:24:30,675:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 01:24:30,675:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 01:24:30,675:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 01:24:30,675:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 01:32:42,107:INFO:PyCaret ClassificationExperiment
2024-09-08 01:32:42,107:INFO:Logging name: clf-default-name
2024-09-08 01:32:42,107:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 01:32:42,111:INFO:version 3.3.2
2024-09-08 01:32:42,111:INFO:Initializing setup()
2024-09-08 01:32:42,111:INFO:self.USI: 9cf6
2024-09-08 01:32:42,111:INFO:self._variable_keys: {'html_param', '_available_plots', 'X_train', 'y_train', 'target_param', 'fold_groups_param', 'data', 'seed', 'y_test', 'X_test', 'gpu_n_jobs_param', 'fix_imbalance', '_ml_usecase', 'logging_param', 'log_plots_param', 'fold_shuffle_param', 'USI', 'n_jobs_param', 'fold_generator', 'X', 'exp_name_log', 'y', 'pipeline', 'idx', 'exp_id', 'gpu_param', 'memory', 'is_multiclass'}
2024-09-08 01:32:42,111:INFO:Checking environment
2024-09-08 01:32:42,111:INFO:python_version: 3.10.11
2024-09-08 01:32:42,116:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 01:32:42,116:INFO:machine: AMD64
2024-09-08 01:32:42,133:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 01:32:42,133:INFO:Memory: svmem(total=16407719936, available=2918109184, percent=82.2, used=13489610752, free=2918109184)
2024-09-08 01:32:42,133:INFO:Physical Core: 4
2024-09-08 01:32:42,133:INFO:Logical Core: 8
2024-09-08 01:32:42,133:INFO:Checking libraries
2024-09-08 01:32:42,133:INFO:System:
2024-09-08 01:32:42,133:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 01:32:42,133:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 01:32:42,133:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 01:32:42,133:INFO:PyCaret required dependencies:
2024-09-08 01:32:42,312:INFO:                 pip: 24.2
2024-09-08 01:32:42,312:INFO:          setuptools: 72.1.0
2024-09-08 01:32:42,312:INFO:             pycaret: 3.3.2
2024-09-08 01:32:42,312:INFO:             IPython: 8.25.0
2024-09-08 01:32:42,312:INFO:          ipywidgets: 8.1.5
2024-09-08 01:32:42,312:INFO:                tqdm: 4.66.5
2024-09-08 01:32:42,312:INFO:               numpy: 1.26.4
2024-09-08 01:32:42,312:INFO:              pandas: 2.1.4
2024-09-08 01:32:42,312:INFO:              jinja2: 3.1.4
2024-09-08 01:32:42,322:INFO:               scipy: 1.11.4
2024-09-08 01:32:42,322:INFO:              joblib: 1.3.2
2024-09-08 01:32:42,322:INFO:             sklearn: 1.4.2
2024-09-08 01:32:42,322:INFO:                pyod: 2.0.1
2024-09-08 01:32:42,322:INFO:            imblearn: 0.12.3
2024-09-08 01:32:42,322:INFO:   category_encoders: 2.6.3
2024-09-08 01:32:42,322:INFO:            lightgbm: 4.5.0
2024-09-08 01:32:42,326:INFO:               numba: 0.60.0
2024-09-08 01:32:42,326:INFO:            requests: 2.32.3
2024-09-08 01:32:42,326:INFO:          matplotlib: 3.7.5
2024-09-08 01:32:42,326:INFO:          scikitplot: 0.3.7
2024-09-08 01:32:42,326:INFO:         yellowbrick: 1.5
2024-09-08 01:32:42,326:INFO:              plotly: 5.24.0
2024-09-08 01:32:42,326:INFO:    plotly-resampler: Not installed
2024-09-08 01:32:42,326:INFO:             kaleido: 0.2.1
2024-09-08 01:32:42,326:INFO:           schemdraw: 0.15
2024-09-08 01:32:42,326:INFO:         statsmodels: 0.14.2
2024-09-08 01:32:42,326:INFO:              sktime: 0.26.0
2024-09-08 01:32:42,326:INFO:               tbats: 1.1.3
2024-09-08 01:32:42,326:INFO:            pmdarima: 2.0.4
2024-09-08 01:32:42,326:INFO:              psutil: 5.9.0
2024-09-08 01:32:42,326:INFO:          markupsafe: 2.1.3
2024-09-08 01:32:42,326:INFO:             pickle5: Not installed
2024-09-08 01:32:42,326:INFO:         cloudpickle: 3.0.0
2024-09-08 01:32:42,326:INFO:         deprecation: 2.1.0
2024-09-08 01:32:42,326:INFO:              xxhash: 3.5.0
2024-09-08 01:32:42,326:INFO:           wurlitzer: Not installed
2024-09-08 01:32:42,326:INFO:PyCaret optional dependencies:
2024-09-08 01:32:42,361:INFO:                shap: Not installed
2024-09-08 01:32:42,361:INFO:           interpret: Not installed
2024-09-08 01:32:42,361:INFO:                umap: Not installed
2024-09-08 01:32:42,361:INFO:     ydata_profiling: Not installed
2024-09-08 01:32:42,361:INFO:  explainerdashboard: Not installed
2024-09-08 01:32:42,361:INFO:             autoviz: Not installed
2024-09-08 01:32:42,361:INFO:           fairlearn: Not installed
2024-09-08 01:32:42,361:INFO:          deepchecks: Not installed
2024-09-08 01:32:42,361:INFO:             xgboost: 2.1.1
2024-09-08 01:32:42,361:INFO:            catboost: Not installed
2024-09-08 01:32:42,361:INFO:              kmodes: Not installed
2024-09-08 01:32:42,361:INFO:             mlxtend: Not installed
2024-09-08 01:32:42,361:INFO:       statsforecast: Not installed
2024-09-08 01:32:42,361:INFO:        tune_sklearn: Not installed
2024-09-08 01:32:42,361:INFO:                 ray: Not installed
2024-09-08 01:32:42,361:INFO:            hyperopt: 0.2.7
2024-09-08 01:32:42,361:INFO:              optuna: 4.0.0
2024-09-08 01:32:42,361:INFO:               skopt: 0.10.2
2024-09-08 01:32:42,361:INFO:              mlflow: Not installed
2024-09-08 01:32:42,361:INFO:              gradio: Not installed
2024-09-08 01:32:42,361:INFO:             fastapi: Not installed
2024-09-08 01:32:42,361:INFO:             uvicorn: Not installed
2024-09-08 01:32:42,361:INFO:              m2cgen: Not installed
2024-09-08 01:32:42,361:INFO:           evidently: Not installed
2024-09-08 01:32:42,361:INFO:               fugue: Not installed
2024-09-08 01:32:42,361:INFO:           streamlit: 1.38.0
2024-09-08 01:32:42,361:INFO:             prophet: Not installed
2024-09-08 01:32:42,361:INFO:None
2024-09-08 01:32:42,361:INFO:Set up data.
2024-09-08 01:32:42,703:INFO:Set up folding strategy.
2024-09-08 01:32:42,703:INFO:Set up train/test split.
2024-09-08 01:32:42,845:INFO:Set up index.
2024-09-08 01:32:42,852:INFO:Assigning column types.
2024-09-08 01:32:42,901:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 01:32:43,007:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 01:32:43,012:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 01:32:43,113:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 01:32:43,122:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 01:32:43,224:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 01:32:43,224:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 01:32:43,292:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 01:32:43,304:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 01:32:43,304:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 01:32:43,415:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 01:32:43,478:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 01:32:43,487:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 01:32:43,592:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 01:32:43,662:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 01:32:43,670:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 01:32:43,670:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 01:32:43,836:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 01:32:43,845:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 01:32:44,022:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 01:32:44,036:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 01:32:44,052:INFO:Preparing preprocessing pipeline...
2024-09-08 01:32:44,061:INFO:Set up simple imputation.
2024-09-08 01:32:44,131:INFO:Set up encoding of ordinal features.
2024-09-08 01:32:44,302:INFO:Set up encoding of categorical features.
2024-09-08 01:32:44,312:INFO:Set up column name cleaning.
2024-09-08 01:32:45,284:INFO:Finished creating preprocessing pipeline.
2024-09-08 01:32:46,477:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                             'Fathers occupation'],
                                    transformer=TargetEncoder(cols=[],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 01:32:46,480:INFO:Creating final display dataframe.
2024-09-08 01:32:47,780:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape       (76518, 38)
4        Transformed data shape      (76518, 122)
5   Transformed train set shape      (53562, 122)
6    Transformed test set shape      (22956, 122)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              9cf6
2024-09-08 01:32:48,039:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 01:32:48,042:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 01:32:48,292:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 01:32:48,307:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 01:32:48,312:INFO:setup() successfully completed in 6.24s...............
2024-09-08 01:32:48,312:INFO:Initializing compare_models()
2024-09-08 01:32:48,312:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 01:32:48,312:INFO:Checking exceptions
2024-09-08 01:32:48,392:INFO:Preparing display monitor
2024-09-08 01:32:48,405:INFO:Initializing Logistic Regression
2024-09-08 01:32:48,405:INFO:Total runtime is 0.0 minutes
2024-09-08 01:32:48,405:INFO:SubProcess create_model() called ==================================
2024-09-08 01:32:48,405:INFO:Initializing create_model()
2024-09-08 01:32:48,405:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:32:48,405:INFO:Checking exceptions
2024-09-08 01:32:48,405:INFO:Importing libraries
2024-09-08 01:32:48,405:INFO:Copying training dataset
2024-09-08 01:32:48,546:INFO:Defining folds
2024-09-08 01:32:48,546:INFO:Declaring metric variables
2024-09-08 01:32:48,546:INFO:Importing untrained model
2024-09-08 01:32:48,546:INFO:Logistic Regression Imported successfully
2024-09-08 01:32:48,552:INFO:Starting cross validation
2024-09-08 01:32:48,565:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:34:49,876:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 01:34:50,378:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 01:34:50,539:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:34:50,976:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:34:51,257:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 01:34:51,851:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:34:51,926:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 01:34:52,452:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:34:52,747:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 01:34:53,226:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:34:56,122:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 01:34:56,503:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 01:34:57,014:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:34:57,137:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:34:58,250:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 01:34:58,687:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:35:46,770:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 01:35:47,101:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:35:47,592:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 01:35:47,880:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:35:47,924:INFO:Calculating mean and std
2024-09-08 01:35:47,924:INFO:Creating metrics dataframe
2024-09-08 01:35:47,931:INFO:Uploading results into container
2024-09-08 01:35:47,931:INFO:Uploading model into container now
2024-09-08 01:35:47,931:INFO:_master_model_container: 1
2024-09-08 01:35:47,931:INFO:_display_container: 2
2024-09-08 01:35:47,931:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 01:35:47,931:INFO:create_model() successfully completed......................................
2024-09-08 01:35:48,104:INFO:SubProcess create_model() end ==================================
2024-09-08 01:35:48,104:INFO:Creating metrics dataframe
2024-09-08 01:35:48,104:INFO:Initializing K Neighbors Classifier
2024-09-08 01:35:48,104:INFO:Total runtime is 2.9949856400489807 minutes
2024-09-08 01:35:48,110:INFO:SubProcess create_model() called ==================================
2024-09-08 01:35:48,110:INFO:Initializing create_model()
2024-09-08 01:35:48,110:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:35:48,110:INFO:Checking exceptions
2024-09-08 01:35:48,110:INFO:Importing libraries
2024-09-08 01:35:48,110:INFO:Copying training dataset
2024-09-08 01:35:48,190:INFO:Defining folds
2024-09-08 01:35:48,190:INFO:Declaring metric variables
2024-09-08 01:35:48,190:INFO:Importing untrained model
2024-09-08 01:35:48,190:INFO:K Neighbors Classifier Imported successfully
2024-09-08 01:35:48,190:INFO:Starting cross validation
2024-09-08 01:35:48,202:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:36:37,375:INFO:Calculating mean and std
2024-09-08 01:36:37,375:INFO:Creating metrics dataframe
2024-09-08 01:36:37,379:INFO:Uploading results into container
2024-09-08 01:36:37,379:INFO:Uploading model into container now
2024-09-08 01:36:37,379:INFO:_master_model_container: 2
2024-09-08 01:36:37,379:INFO:_display_container: 2
2024-09-08 01:36:37,379:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 01:36:37,379:INFO:create_model() successfully completed......................................
2024-09-08 01:36:37,534:INFO:SubProcess create_model() end ==================================
2024-09-08 01:36:37,534:INFO:Creating metrics dataframe
2024-09-08 01:36:37,544:INFO:Initializing Naive Bayes
2024-09-08 01:36:37,544:INFO:Total runtime is 3.818988505999247 minutes
2024-09-08 01:36:37,544:INFO:SubProcess create_model() called ==================================
2024-09-08 01:36:37,544:INFO:Initializing create_model()
2024-09-08 01:36:37,544:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:36:37,544:INFO:Checking exceptions
2024-09-08 01:36:37,544:INFO:Importing libraries
2024-09-08 01:36:37,544:INFO:Copying training dataset
2024-09-08 01:36:37,624:INFO:Defining folds
2024-09-08 01:36:37,624:INFO:Declaring metric variables
2024-09-08 01:36:37,624:INFO:Importing untrained model
2024-09-08 01:36:37,624:INFO:Naive Bayes Imported successfully
2024-09-08 01:36:37,624:INFO:Starting cross validation
2024-09-08 01:36:37,637:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:36:48,600:INFO:Calculating mean and std
2024-09-08 01:36:48,600:INFO:Creating metrics dataframe
2024-09-08 01:36:48,605:INFO:Uploading results into container
2024-09-08 01:36:48,605:INFO:Uploading model into container now
2024-09-08 01:36:48,605:INFO:_master_model_container: 3
2024-09-08 01:36:48,605:INFO:_display_container: 2
2024-09-08 01:36:48,605:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 01:36:48,605:INFO:create_model() successfully completed......................................
2024-09-08 01:36:48,775:INFO:SubProcess create_model() end ==================================
2024-09-08 01:36:48,775:INFO:Creating metrics dataframe
2024-09-08 01:36:48,782:INFO:Initializing Decision Tree Classifier
2024-09-08 01:36:48,782:INFO:Total runtime is 4.006291997432709 minutes
2024-09-08 01:36:48,785:INFO:SubProcess create_model() called ==================================
2024-09-08 01:36:48,785:INFO:Initializing create_model()
2024-09-08 01:36:48,785:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:36:48,785:INFO:Checking exceptions
2024-09-08 01:36:48,785:INFO:Importing libraries
2024-09-08 01:36:48,785:INFO:Copying training dataset
2024-09-08 01:36:48,865:INFO:Defining folds
2024-09-08 01:36:48,865:INFO:Declaring metric variables
2024-09-08 01:36:48,865:INFO:Importing untrained model
2024-09-08 01:36:48,865:INFO:Decision Tree Classifier Imported successfully
2024-09-08 01:36:48,865:INFO:Starting cross validation
2024-09-08 01:36:48,880:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:37:05,066:INFO:Calculating mean and std
2024-09-08 01:37:05,066:INFO:Creating metrics dataframe
2024-09-08 01:37:05,071:INFO:Uploading results into container
2024-09-08 01:37:05,071:INFO:Uploading model into container now
2024-09-08 01:37:05,071:INFO:_master_model_container: 4
2024-09-08 01:37:05,071:INFO:_display_container: 2
2024-09-08 01:37:05,071:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 01:37:05,071:INFO:create_model() successfully completed......................................
2024-09-08 01:37:05,236:INFO:SubProcess create_model() end ==================================
2024-09-08 01:37:05,236:INFO:Creating metrics dataframe
2024-09-08 01:37:05,236:INFO:Initializing SVM - Linear Kernel
2024-09-08 01:37:05,236:INFO:Total runtime is 4.280518913269043 minutes
2024-09-08 01:37:05,236:INFO:SubProcess create_model() called ==================================
2024-09-08 01:37:05,236:INFO:Initializing create_model()
2024-09-08 01:37:05,236:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:37:05,236:INFO:Checking exceptions
2024-09-08 01:37:05,243:INFO:Importing libraries
2024-09-08 01:37:05,243:INFO:Copying training dataset
2024-09-08 01:37:05,328:INFO:Defining folds
2024-09-08 01:37:05,328:INFO:Declaring metric variables
2024-09-08 01:37:05,328:INFO:Importing untrained model
2024-09-08 01:37:05,330:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 01:37:05,330:INFO:Starting cross validation
2024-09-08 01:37:05,336:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:38:06,781:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:07,660:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:09,673:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:10,491:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:11,087:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:12,742:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:12,976:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:15,101:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:27,287:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:28,247:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:28,281:INFO:Calculating mean and std
2024-09-08 01:38:28,281:INFO:Creating metrics dataframe
2024-09-08 01:38:28,287:INFO:Uploading results into container
2024-09-08 01:38:28,287:INFO:Uploading model into container now
2024-09-08 01:38:28,287:INFO:_master_model_container: 5
2024-09-08 01:38:28,287:INFO:_display_container: 2
2024-09-08 01:38:28,287:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 01:38:28,287:INFO:create_model() successfully completed......................................
2024-09-08 01:38:28,457:INFO:SubProcess create_model() end ==================================
2024-09-08 01:38:28,460:INFO:Creating metrics dataframe
2024-09-08 01:38:28,460:INFO:Initializing Ridge Classifier
2024-09-08 01:38:28,460:INFO:Total runtime is 5.667589402198792 minutes
2024-09-08 01:38:28,460:INFO:SubProcess create_model() called ==================================
2024-09-08 01:38:28,460:INFO:Initializing create_model()
2024-09-08 01:38:28,460:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:38:28,460:INFO:Checking exceptions
2024-09-08 01:38:28,460:INFO:Importing libraries
2024-09-08 01:38:28,460:INFO:Copying training dataset
2024-09-08 01:38:28,547:INFO:Defining folds
2024-09-08 01:38:28,547:INFO:Declaring metric variables
2024-09-08 01:38:28,547:INFO:Importing untrained model
2024-09-08 01:38:28,552:INFO:Ridge Classifier Imported successfully
2024-09-08 01:38:28,552:INFO:Starting cross validation
2024-09-08 01:38:28,557:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:38:35,414:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:35,445:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:35,564:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:35,594:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:35,664:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:35,744:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:35,784:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:35,853:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:38,973:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:39,033:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:38:39,087:INFO:Calculating mean and std
2024-09-08 01:38:39,087:INFO:Creating metrics dataframe
2024-09-08 01:38:39,093:INFO:Uploading results into container
2024-09-08 01:38:39,093:INFO:Uploading model into container now
2024-09-08 01:38:39,093:INFO:_master_model_container: 6
2024-09-08 01:38:39,093:INFO:_display_container: 2
2024-09-08 01:38:39,093:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 01:38:39,093:INFO:create_model() successfully completed......................................
2024-09-08 01:38:39,265:INFO:SubProcess create_model() end ==================================
2024-09-08 01:38:39,265:INFO:Creating metrics dataframe
2024-09-08 01:38:39,265:INFO:Initializing Random Forest Classifier
2024-09-08 01:38:39,265:INFO:Total runtime is 5.847674099604289 minutes
2024-09-08 01:38:39,265:INFO:SubProcess create_model() called ==================================
2024-09-08 01:38:39,265:INFO:Initializing create_model()
2024-09-08 01:38:39,273:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:38:39,273:INFO:Checking exceptions
2024-09-08 01:38:39,273:INFO:Importing libraries
2024-09-08 01:38:39,273:INFO:Copying training dataset
2024-09-08 01:38:39,363:INFO:Defining folds
2024-09-08 01:38:39,363:INFO:Declaring metric variables
2024-09-08 01:38:39,363:INFO:Importing untrained model
2024-09-08 01:38:39,373:INFO:Random Forest Classifier Imported successfully
2024-09-08 01:38:39,373:INFO:Starting cross validation
2024-09-08 01:38:39,383:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:39:30,394:INFO:Calculating mean and std
2024-09-08 01:39:30,397:INFO:Creating metrics dataframe
2024-09-08 01:39:30,402:INFO:Uploading results into container
2024-09-08 01:39:30,402:INFO:Uploading model into container now
2024-09-08 01:39:30,407:INFO:_master_model_container: 7
2024-09-08 01:39:30,407:INFO:_display_container: 2
2024-09-08 01:39:30,407:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 01:39:30,407:INFO:create_model() successfully completed......................................
2024-09-08 01:39:30,602:INFO:SubProcess create_model() end ==================================
2024-09-08 01:39:30,602:INFO:Creating metrics dataframe
2024-09-08 01:39:30,610:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 01:39:30,610:INFO:Total runtime is 6.7034274458885195 minutes
2024-09-08 01:39:30,610:INFO:SubProcess create_model() called ==================================
2024-09-08 01:39:30,610:INFO:Initializing create_model()
2024-09-08 01:39:30,610:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:39:30,610:INFO:Checking exceptions
2024-09-08 01:39:30,610:INFO:Importing libraries
2024-09-08 01:39:30,610:INFO:Copying training dataset
2024-09-08 01:39:30,718:INFO:Defining folds
2024-09-08 01:39:30,718:INFO:Declaring metric variables
2024-09-08 01:39:30,718:INFO:Importing untrained model
2024-09-08 01:39:30,718:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 01:39:30,718:INFO:Starting cross validation
2024-09-08 01:39:30,727:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:39:38,391:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 01:39:38,589:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 01:39:39,279:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 01:39:41,010:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 01:39:43,448:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 01:39:43,589:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 01:39:43,861:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:39:43,969:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:39:45,848:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 01:39:46,598:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:39:46,870:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 01:39:47,081:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:39:47,628:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:39:47,939:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:39:48,679:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:39:49,350:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:39:51,284:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 01:39:51,364:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 01:39:52,742:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:39:52,820:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:39:52,868:INFO:Calculating mean and std
2024-09-08 01:39:52,869:INFO:Creating metrics dataframe
2024-09-08 01:39:52,869:INFO:Uploading results into container
2024-09-08 01:39:52,874:INFO:Uploading model into container now
2024-09-08 01:39:52,874:INFO:_master_model_container: 8
2024-09-08 01:39:52,874:INFO:_display_container: 2
2024-09-08 01:39:52,874:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 01:39:52,874:INFO:create_model() successfully completed......................................
2024-09-08 01:39:53,039:INFO:SubProcess create_model() end ==================================
2024-09-08 01:39:53,039:INFO:Creating metrics dataframe
2024-09-08 01:39:53,044:INFO:Initializing Ada Boost Classifier
2024-09-08 01:39:53,044:INFO:Total runtime is 7.077320750554403 minutes
2024-09-08 01:39:53,044:INFO:SubProcess create_model() called ==================================
2024-09-08 01:39:53,044:INFO:Initializing create_model()
2024-09-08 01:39:53,044:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:39:53,044:INFO:Checking exceptions
2024-09-08 01:39:53,044:INFO:Importing libraries
2024-09-08 01:39:53,044:INFO:Copying training dataset
2024-09-08 01:39:53,128:INFO:Defining folds
2024-09-08 01:39:53,128:INFO:Declaring metric variables
2024-09-08 01:39:53,128:INFO:Importing untrained model
2024-09-08 01:39:53,128:INFO:Ada Boost Classifier Imported successfully
2024-09-08 01:39:53,128:INFO:Starting cross validation
2024-09-08 01:39:53,138:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:39:58,543:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 01:39:58,543:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 01:39:58,659:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 01:39:58,817:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 01:39:58,817:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 01:39:59,129:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 01:39:59,191:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 01:39:59,312:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 01:40:17,695:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:40:18,310:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:40:18,574:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:40:19,016:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:40:19,135:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:40:20,316:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:40:20,455:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:40:20,596:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:40:21,524:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 01:40:21,916:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 01:40:32,387:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:40:32,738:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:40:32,779:INFO:Calculating mean and std
2024-09-08 01:40:32,781:INFO:Creating metrics dataframe
2024-09-08 01:40:32,781:INFO:Uploading results into container
2024-09-08 01:40:32,781:INFO:Uploading model into container now
2024-09-08 01:40:32,787:INFO:_master_model_container: 9
2024-09-08 01:40:32,787:INFO:_display_container: 2
2024-09-08 01:40:32,787:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 01:40:32,787:INFO:create_model() successfully completed......................................
2024-09-08 01:40:32,947:INFO:SubProcess create_model() end ==================================
2024-09-08 01:40:32,947:INFO:Creating metrics dataframe
2024-09-08 01:40:32,952:INFO:Initializing Gradient Boosting Classifier
2024-09-08 01:40:32,952:INFO:Total runtime is 7.742447845141093 minutes
2024-09-08 01:40:32,952:INFO:SubProcess create_model() called ==================================
2024-09-08 01:40:32,952:INFO:Initializing create_model()
2024-09-08 01:40:32,952:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:40:32,952:INFO:Checking exceptions
2024-09-08 01:40:32,957:INFO:Importing libraries
2024-09-08 01:40:32,957:INFO:Copying training dataset
2024-09-08 01:40:33,036:INFO:Defining folds
2024-09-08 01:40:33,036:INFO:Declaring metric variables
2024-09-08 01:40:33,036:INFO:Importing untrained model
2024-09-08 01:40:33,036:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 01:40:33,036:INFO:Starting cross validation
2024-09-08 01:40:33,046:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:43:29,924:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:43:31,224:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:43:34,814:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:43:38,954:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:43:39,408:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:43:40,365:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:43:40,539:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:43:42,615:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:24,716:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:26,039:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:26,073:INFO:Calculating mean and std
2024-09-08 01:45:26,073:INFO:Creating metrics dataframe
2024-09-08 01:45:26,073:INFO:Uploading results into container
2024-09-08 01:45:26,080:INFO:Uploading model into container now
2024-09-08 01:45:26,080:INFO:_master_model_container: 10
2024-09-08 01:45:26,082:INFO:_display_container: 2
2024-09-08 01:45:26,083:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 01:45:26,083:INFO:create_model() successfully completed......................................
2024-09-08 01:45:26,239:INFO:SubProcess create_model() end ==================================
2024-09-08 01:45:26,239:INFO:Creating metrics dataframe
2024-09-08 01:45:26,247:INFO:Initializing Linear Discriminant Analysis
2024-09-08 01:45:26,247:INFO:Total runtime is 12.630712044239043 minutes
2024-09-08 01:45:26,247:INFO:SubProcess create_model() called ==================================
2024-09-08 01:45:26,247:INFO:Initializing create_model()
2024-09-08 01:45:26,247:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:45:26,247:INFO:Checking exceptions
2024-09-08 01:45:26,247:INFO:Importing libraries
2024-09-08 01:45:26,247:INFO:Copying training dataset
2024-09-08 01:45:26,331:INFO:Defining folds
2024-09-08 01:45:26,331:INFO:Declaring metric variables
2024-09-08 01:45:26,331:INFO:Importing untrained model
2024-09-08 01:45:26,333:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 01:45:26,333:INFO:Starting cross validation
2024-09-08 01:45:26,339:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:45:42,184:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:42,523:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:42,668:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:42,894:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:42,908:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:43,097:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:43,254:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:43,432:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:48,909:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:49,089:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:45:49,145:INFO:Calculating mean and std
2024-09-08 01:45:49,145:INFO:Creating metrics dataframe
2024-09-08 01:45:49,145:INFO:Uploading results into container
2024-09-08 01:45:49,145:INFO:Uploading model into container now
2024-09-08 01:45:49,145:INFO:_master_model_container: 11
2024-09-08 01:45:49,154:INFO:_display_container: 2
2024-09-08 01:45:49,154:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 01:45:49,154:INFO:create_model() successfully completed......................................
2024-09-08 01:45:49,327:INFO:SubProcess create_model() end ==================================
2024-09-08 01:45:49,327:INFO:Creating metrics dataframe
2024-09-08 01:45:49,335:INFO:Initializing Extra Trees Classifier
2024-09-08 01:45:49,338:INFO:Total runtime is 13.015547521909077 minutes
2024-09-08 01:45:49,338:INFO:SubProcess create_model() called ==================================
2024-09-08 01:45:49,338:INFO:Initializing create_model()
2024-09-08 01:45:49,338:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:45:49,338:INFO:Checking exceptions
2024-09-08 01:45:49,338:INFO:Importing libraries
2024-09-08 01:45:49,338:INFO:Copying training dataset
2024-09-08 01:45:49,424:INFO:Defining folds
2024-09-08 01:45:49,424:INFO:Declaring metric variables
2024-09-08 01:45:49,424:INFO:Importing untrained model
2024-09-08 01:45:49,424:INFO:Extra Trees Classifier Imported successfully
2024-09-08 01:45:49,427:INFO:Starting cross validation
2024-09-08 01:45:49,436:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:47:01,279:INFO:Calculating mean and std
2024-09-08 01:47:01,279:INFO:Creating metrics dataframe
2024-09-08 01:47:01,288:INFO:Uploading results into container
2024-09-08 01:47:01,294:INFO:Uploading model into container now
2024-09-08 01:47:01,295:INFO:_master_model_container: 12
2024-09-08 01:47:01,295:INFO:_display_container: 2
2024-09-08 01:47:01,295:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 01:47:01,299:INFO:create_model() successfully completed......................................
2024-09-08 01:47:01,623:INFO:SubProcess create_model() end ==================================
2024-09-08 01:47:01,624:INFO:Creating metrics dataframe
2024-09-08 01:47:01,629:INFO:Initializing Extreme Gradient Boosting
2024-09-08 01:47:01,629:INFO:Total runtime is 14.220407799879709 minutes
2024-09-08 01:47:01,634:INFO:SubProcess create_model() called ==================================
2024-09-08 01:47:01,634:INFO:Initializing create_model()
2024-09-08 01:47:01,634:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:47:01,634:INFO:Checking exceptions
2024-09-08 01:47:01,634:INFO:Importing libraries
2024-09-08 01:47:01,634:INFO:Copying training dataset
2024-09-08 01:47:01,789:INFO:Defining folds
2024-09-08 01:47:01,789:INFO:Declaring metric variables
2024-09-08 01:47:01,789:INFO:Importing untrained model
2024-09-08 01:47:01,795:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 01:47:01,795:INFO:Starting cross validation
2024-09-08 01:47:01,811:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:47:42,307:INFO:Calculating mean and std
2024-09-08 01:47:42,311:INFO:Creating metrics dataframe
2024-09-08 01:47:42,317:INFO:Uploading results into container
2024-09-08 01:47:42,317:INFO:Uploading model into container now
2024-09-08 01:47:42,322:INFO:_master_model_container: 13
2024-09-08 01:47:42,322:INFO:_display_container: 2
2024-09-08 01:47:42,322:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 01:47:42,322:INFO:create_model() successfully completed......................................
2024-09-08 01:47:42,547:INFO:SubProcess create_model() end ==================================
2024-09-08 01:47:42,547:INFO:Creating metrics dataframe
2024-09-08 01:47:42,562:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 01:47:42,562:INFO:Total runtime is 14.902627551555632 minutes
2024-09-08 01:47:42,562:INFO:SubProcess create_model() called ==================================
2024-09-08 01:47:42,562:INFO:Initializing create_model()
2024-09-08 01:47:42,562:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:47:42,562:INFO:Checking exceptions
2024-09-08 01:47:42,562:INFO:Importing libraries
2024-09-08 01:47:42,562:INFO:Copying training dataset
2024-09-08 01:47:42,697:INFO:Defining folds
2024-09-08 01:47:42,697:INFO:Declaring metric variables
2024-09-08 01:47:42,708:INFO:Importing untrained model
2024-09-08 01:47:42,710:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 01:47:42,711:INFO:Starting cross validation
2024-09-08 01:47:42,717:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:48:15,155:INFO:Calculating mean and std
2024-09-08 01:48:15,157:INFO:Creating metrics dataframe
2024-09-08 01:48:15,163:INFO:Uploading results into container
2024-09-08 01:48:15,164:INFO:Uploading model into container now
2024-09-08 01:48:15,164:INFO:_master_model_container: 14
2024-09-08 01:48:15,164:INFO:_display_container: 2
2024-09-08 01:48:15,164:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 01:48:15,164:INFO:create_model() successfully completed......................................
2024-09-08 01:48:15,340:INFO:SubProcess create_model() end ==================================
2024-09-08 01:48:15,340:INFO:Creating metrics dataframe
2024-09-08 01:48:15,354:INFO:Initializing Dummy Classifier
2024-09-08 01:48:15,354:INFO:Total runtime is 15.449149290720621 minutes
2024-09-08 01:48:15,354:INFO:SubProcess create_model() called ==================================
2024-09-08 01:48:15,354:INFO:Initializing create_model()
2024-09-08 01:48:15,354:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FEF6AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:48:15,354:INFO:Checking exceptions
2024-09-08 01:48:15,354:INFO:Importing libraries
2024-09-08 01:48:15,354:INFO:Copying training dataset
2024-09-08 01:48:15,460:INFO:Defining folds
2024-09-08 01:48:15,460:INFO:Declaring metric variables
2024-09-08 01:48:15,460:INFO:Importing untrained model
2024-09-08 01:48:15,460:INFO:Dummy Classifier Imported successfully
2024-09-08 01:48:15,460:INFO:Starting cross validation
2024-09-08 01:48:15,475:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:48:21,781:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:48:21,892:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:48:21,958:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:48:22,137:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:48:22,270:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:48:22,350:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:48:22,418:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:48:22,510:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:48:25,074:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:48:25,170:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 01:48:25,211:INFO:Calculating mean and std
2024-09-08 01:48:25,211:INFO:Creating metrics dataframe
2024-09-08 01:48:25,216:INFO:Uploading results into container
2024-09-08 01:48:25,221:INFO:Uploading model into container now
2024-09-08 01:48:25,221:INFO:_master_model_container: 15
2024-09-08 01:48:25,221:INFO:_display_container: 2
2024-09-08 01:48:25,221:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 01:48:25,221:INFO:create_model() successfully completed......................................
2024-09-08 01:48:25,401:INFO:SubProcess create_model() end ==================================
2024-09-08 01:48:25,401:INFO:Creating metrics dataframe
2024-09-08 01:48:25,410:INFO:Initializing create_model()
2024-09-08 01:48:25,410:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:48:25,410:INFO:Checking exceptions
2024-09-08 01:48:25,412:INFO:Importing libraries
2024-09-08 01:48:25,412:INFO:Copying training dataset
2024-09-08 01:48:25,584:INFO:Defining folds
2024-09-08 01:48:25,584:INFO:Declaring metric variables
2024-09-08 01:48:25,584:INFO:Importing untrained model
2024-09-08 01:48:25,584:INFO:Declaring custom model
2024-09-08 01:48:25,589:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 01:48:25,606:INFO:Cross validation set to False
2024-09-08 01:48:25,606:INFO:Fitting Model
2024-09-08 01:48:27,861:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:48:28,305:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 01:48:28,346:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011734 seconds.
2024-09-08 01:48:28,346:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 01:48:28,346:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 01:48:28,358:INFO:[LightGBM] [Info] Total Bins 1658
2024-09-08 01:48:28,358:INFO:[LightGBM] [Info] Number of data points in the train set: 53562, number of used features: 91
2024-09-08 01:48:28,361:INFO:[LightGBM] [Info] Start training from score -0.746209
2024-09-08 01:48:28,362:INFO:[LightGBM] [Info] Start training from score -1.106880
2024-09-08 01:48:28,362:INFO:[LightGBM] [Info] Start training from score -1.633473
2024-09-08 01:48:30,451:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 01:48:30,451:INFO:create_model() successfully completed......................................
2024-09-08 01:48:30,762:INFO:Initializing create_model()
2024-09-08 01:48:30,762:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:48:30,762:INFO:Checking exceptions
2024-09-08 01:48:30,762:INFO:Importing libraries
2024-09-08 01:48:30,762:INFO:Copying training dataset
2024-09-08 01:48:30,919:INFO:Defining folds
2024-09-08 01:48:30,919:INFO:Declaring metric variables
2024-09-08 01:48:30,921:INFO:Importing untrained model
2024-09-08 01:48:30,921:INFO:Declaring custom model
2024-09-08 01:48:30,925:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 01:48:30,941:INFO:Cross validation set to False
2024-09-08 01:48:30,941:INFO:Fitting Model
2024-09-08 01:48:33,329:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:48:36,952:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 01:48:36,952:INFO:create_model() successfully completed......................................
2024-09-08 01:48:37,132:INFO:Initializing create_model()
2024-09-08 01:48:37,132:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:48:37,132:INFO:Checking exceptions
2024-09-08 01:48:37,132:INFO:Importing libraries
2024-09-08 01:48:37,132:INFO:Copying training dataset
2024-09-08 01:48:37,246:INFO:Defining folds
2024-09-08 01:48:37,246:INFO:Declaring metric variables
2024-09-08 01:48:37,246:INFO:Importing untrained model
2024-09-08 01:48:37,246:INFO:Declaring custom model
2024-09-08 01:48:37,252:INFO:Random Forest Classifier Imported successfully
2024-09-08 01:48:37,255:INFO:Cross validation set to False
2024-09-08 01:48:37,255:INFO:Fitting Model
2024-09-08 01:48:39,580:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:48:45,542:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 01:48:45,542:INFO:create_model() successfully completed......................................
2024-09-08 01:48:45,772:INFO:_master_model_container: 15
2024-09-08 01:48:45,772:INFO:_display_container: 2
2024-09-08 01:48:45,772:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)]
2024-09-08 01:48:45,772:INFO:compare_models() successfully completed......................................
2024-09-08 01:48:45,772:INFO:Initializing create_model()
2024-09-08 01:48:45,780:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:48:45,780:INFO:Checking exceptions
2024-09-08 01:48:45,782:INFO:Importing libraries
2024-09-08 01:48:45,782:INFO:Copying training dataset
2024-09-08 01:48:45,870:INFO:Defining folds
2024-09-08 01:48:45,870:INFO:Declaring metric variables
2024-09-08 01:48:45,870:INFO:Importing untrained model
2024-09-08 01:48:45,872:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 01:48:45,872:INFO:Starting cross validation
2024-09-08 01:48:45,877:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:49:14,962:INFO:Calculating mean and std
2024-09-08 01:49:14,965:INFO:Creating metrics dataframe
2024-09-08 01:49:14,969:INFO:Finalizing model
2024-09-08 01:49:17,274:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:49:17,738:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 01:49:17,779:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011463 seconds.
2024-09-08 01:49:17,779:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 01:49:17,779:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 01:49:17,779:INFO:[LightGBM] [Info] Total Bins 1658
2024-09-08 01:49:17,779:INFO:[LightGBM] [Info] Number of data points in the train set: 53562, number of used features: 91
2024-09-08 01:49:17,786:INFO:[LightGBM] [Info] Start training from score -0.746209
2024-09-08 01:49:17,786:INFO:[LightGBM] [Info] Start training from score -1.106880
2024-09-08 01:49:17,786:INFO:[LightGBM] [Info] Start training from score -1.633473
2024-09-08 01:49:19,740:INFO:Uploading results into container
2024-09-08 01:49:19,746:INFO:Uploading model into container now
2024-09-08 01:49:19,795:INFO:_master_model_container: 16
2024-09-08 01:49:19,795:INFO:_display_container: 3
2024-09-08 01:49:19,795:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 01:49:19,795:INFO:create_model() successfully completed......................................
2024-09-08 01:49:20,015:INFO:Initializing create_model()
2024-09-08 01:49:20,015:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:49:20,015:INFO:Checking exceptions
2024-09-08 01:49:20,015:INFO:Importing libraries
2024-09-08 01:49:20,015:INFO:Copying training dataset
2024-09-08 01:49:20,114:INFO:Defining folds
2024-09-08 01:49:20,114:INFO:Declaring metric variables
2024-09-08 01:49:20,114:INFO:Importing untrained model
2024-09-08 01:49:20,114:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 01:49:20,114:INFO:Starting cross validation
2024-09-08 01:49:20,129:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:49:55,402:INFO:Calculating mean and std
2024-09-08 01:49:55,402:INFO:Creating metrics dataframe
2024-09-08 01:49:55,402:INFO:Finalizing model
2024-09-08 01:49:57,543:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:50:01,123:INFO:Uploading results into container
2024-09-08 01:50:01,128:INFO:Uploading model into container now
2024-09-08 01:50:01,157:INFO:_master_model_container: 17
2024-09-08 01:50:01,157:INFO:_display_container: 4
2024-09-08 01:50:01,163:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 01:50:01,163:INFO:create_model() successfully completed......................................
2024-09-08 01:50:01,333:INFO:Initializing create_model()
2024-09-08 01:50:01,338:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:50:01,338:INFO:Checking exceptions
2024-09-08 01:50:01,338:INFO:Importing libraries
2024-09-08 01:50:01,341:INFO:Copying training dataset
2024-09-08 01:50:01,423:INFO:Defining folds
2024-09-08 01:50:01,423:INFO:Declaring metric variables
2024-09-08 01:50:01,423:INFO:Importing untrained model
2024-09-08 01:50:01,427:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 01:50:01,428:INFO:Starting cross validation
2024-09-08 01:50:01,433:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:52:56,225:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:53:01,713:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:53:05,911:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:53:08,513:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:53:08,695:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:53:11,115:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:53:11,132:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:53:13,359:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:54:49,712:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:54:53,042:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 01:54:53,082:INFO:Calculating mean and std
2024-09-08 01:54:53,084:INFO:Creating metrics dataframe
2024-09-08 01:54:53,084:INFO:Finalizing model
2024-09-08 01:54:55,504:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:56:44,693:INFO:Uploading results into container
2024-09-08 01:56:44,693:INFO:Uploading model into container now
2024-09-08 01:56:44,724:INFO:_master_model_container: 18
2024-09-08 01:56:44,724:INFO:_display_container: 5
2024-09-08 01:56:44,724:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 01:56:44,724:INFO:create_model() successfully completed......................................
2024-09-08 01:56:44,905:INFO:Initializing tune_model()
2024-09-08 01:56:44,905:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>)
2024-09-08 01:56:44,905:INFO:Checking exceptions
2024-09-08 01:56:44,905:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 01:56:45,062:INFO:Copying training dataset
2024-09-08 01:56:45,122:INFO:Checking base model
2024-09-08 01:56:45,122:INFO:Base model : Extreme Gradient Boosting
2024-09-08 01:56:45,122:INFO:Declaring metric variables
2024-09-08 01:56:45,122:INFO:Defining Hyperparameters
2024-09-08 01:56:45,303:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 01:56:45,303:INFO:Tuning with n_jobs=-1
2024-09-08 01:56:45,309:INFO:Initializing skopt.BayesSearchCV
2024-09-08 01:59:11,391:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 1.0), ('actual_estimator__gamma', 0.3), ('actual_estimator__learning_rate', 0.2), ('actual_estimator__max_depth', 3), ('actual_estimator__min_child_weight', 5), ('actual_estimator__n_estimators', 50), ('actual_estimator__subsample', 0.6)])
2024-09-08 01:59:11,391:INFO:Hyperparameter search completed
2024-09-08 01:59:11,391:INFO:SubProcess create_model() called ==================================
2024-09-08 01:59:11,391:INFO:Initializing create_model()
2024-09-08 01:59:11,391:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002120FD30F40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 1.0, 'gamma': 0.3, 'learning_rate': 0.2, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 50, 'subsample': 0.6})
2024-09-08 01:59:11,391:INFO:Checking exceptions
2024-09-08 01:59:11,391:INFO:Importing libraries
2024-09-08 01:59:11,391:INFO:Copying training dataset
2024-09-08 01:59:11,492:INFO:Defining folds
2024-09-08 01:59:11,492:INFO:Declaring metric variables
2024-09-08 01:59:11,492:INFO:Importing untrained model
2024-09-08 01:59:11,492:INFO:Declaring custom model
2024-09-08 01:59:11,500:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 01:59:11,500:INFO:Starting cross validation
2024-09-08 01:59:11,510:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:59:22,531:INFO:Calculating mean and std
2024-09-08 01:59:22,531:INFO:Creating metrics dataframe
2024-09-08 01:59:22,531:INFO:Finalizing model
2024-09-08 01:59:25,193:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:59:27,471:INFO:Uploading results into container
2024-09-08 01:59:27,471:INFO:Uploading model into container now
2024-09-08 01:59:27,475:INFO:_master_model_container: 19
2024-09-08 01:59:27,475:INFO:_display_container: 6
2024-09-08 01:59:27,478:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=1.0, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0.3, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.2, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=3, max_leaves=None,
              min_child_weight=5, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=50, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 01:59:27,478:INFO:create_model() successfully completed......................................
2024-09-08 01:59:27,652:INFO:SubProcess create_model() end ==================================
2024-09-08 01:59:27,652:INFO:choose_better activated
2024-09-08 01:59:27,652:INFO:SubProcess create_model() called ==================================
2024-09-08 01:59:27,652:INFO:Initializing create_model()
2024-09-08 01:59:27,660:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:59:27,660:INFO:Checking exceptions
2024-09-08 01:59:27,662:INFO:Importing libraries
2024-09-08 01:59:27,662:INFO:Copying training dataset
2024-09-08 01:59:27,751:INFO:Defining folds
2024-09-08 01:59:27,751:INFO:Declaring metric variables
2024-09-08 01:59:27,751:INFO:Importing untrained model
2024-09-08 01:59:27,751:INFO:Declaring custom model
2024-09-08 01:59:27,760:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 01:59:27,763:INFO:Starting cross validation
2024-09-08 01:59:27,770:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 01:59:44,701:INFO:Calculating mean and std
2024-09-08 01:59:44,701:INFO:Creating metrics dataframe
2024-09-08 01:59:44,705:INFO:Finalizing model
2024-09-08 01:59:47,484:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 01:59:51,331:INFO:Uploading results into container
2024-09-08 01:59:51,334:INFO:Uploading model into container now
2024-09-08 01:59:51,336:INFO:_master_model_container: 20
2024-09-08 01:59:51,336:INFO:_display_container: 7
2024-09-08 01:59:51,336:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 01:59:51,336:INFO:create_model() successfully completed......................................
2024-09-08 01:59:51,510:INFO:SubProcess create_model() end ==================================
2024-09-08 01:59:51,512:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8279
2024-09-08 01:59:51,512:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=1.0, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0.3, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.2, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=3, max_leaves=None,
              min_child_weight=5, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=50, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.826
2024-09-08 01:59:51,512:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 01:59:51,512:INFO:choose_better completed
2024-09-08 01:59:51,516:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-09-08 01:59:51,536:INFO:_master_model_container: 20
2024-09-08 01:59:51,536:INFO:_display_container: 6
2024-09-08 01:59:51,536:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 01:59:51,536:INFO:tune_model() successfully completed......................................
2024-09-08 01:59:51,696:INFO:Initializing predict_model()
2024-09-08 01:59:51,696:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000210C775D480>)
2024-09-08 01:59:51,696:INFO:Checking exceptions
2024-09-08 01:59:51,696:INFO:Preloading libraries
2024-09-08 01:59:52,966:INFO:Initializing get_config()
2024-09-08 01:59:52,968:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, variable=X_train)
2024-09-08 01:59:52,968:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 01:59:53,068:INFO:Variable:  returned as           id Marital status Application mode  ... Unemployment rate Inflation rate   GDP
63473  63473              1               39  ...              13.9           -0.3  0.79
58279  58279              1               39  ...              15.5            2.8 -4.06
68323  68323              1               18  ...              13.9           -0.3  0.79
27374  27374              1                1  ...              13.9           -0.3  0.79
46832  46832              1               17  ...              13.9           -0.3  0.79
...      ...            ...              ...  ...               ...            ...   ...
71758  71758              1                1  ...              12.7            3.7 -1.70
38253  38253              1               17  ...               9.4           -0.8 -3.12
71420  71420              1                1  ...              15.5            2.8 -4.06
74640  74640              1               17  ...               9.4           -0.8 -3.12
13710  13710              1               17  ...               8.9            1.4  3.51

[53562 rows x 37 columns]
2024-09-08 01:59:53,068:INFO:get_config() successfully completed......................................
2024-09-08 01:59:53,068:INFO:Initializing predict_model()
2024-09-08 01:59:53,068:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000212106041F0>)
2024-09-08 01:59:53,068:INFO:Checking exceptions
2024-09-08 01:59:53,068:INFO:Preloading libraries
2024-09-08 01:59:53,068:INFO:Set up data.
2024-09-08 01:59:53,096:INFO:Set up index.
2024-09-08 01:59:54,175:INFO:Initializing get_config()
2024-09-08 01:59:54,175:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, variable=y_train)
2024-09-08 01:59:54,176:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 01:59:54,200:INFO:Variable:  returned as 63473    1
58279    1
68323    0
27374    0
46832    0
        ..
71758    1
38253    1
71420    0
74640    2
13710    0
Name: Target, Length: 53562, dtype: int8
2024-09-08 01:59:54,200:INFO:get_config() successfully completed......................................
2024-09-08 01:59:54,200:INFO:Initializing get_config()
2024-09-08 01:59:54,200:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, variable=y_test)
2024-09-08 01:59:54,200:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 01:59:54,216:INFO:Variable:  returned as 36387    0
29621    1
22002    1
4939     0
66101    0
        ..
65620    0
60576    0
10486    2
62584    2
39904    2
Name: Target, Length: 22956, dtype: int8
2024-09-08 01:59:54,216:INFO:get_config() successfully completed......................................
2024-09-08 01:59:54,224:INFO:Initializing finalize_model()
2024-09-08 01:59:54,224:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 01:59:54,226:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 01:59:54,266:INFO:Initializing create_model()
2024-09-08 01:59:54,266:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 01:59:54,266:INFO:Checking exceptions
2024-09-08 01:59:54,266:INFO:Importing libraries
2024-09-08 01:59:54,266:INFO:Copying training dataset
2024-09-08 01:59:54,273:INFO:Defining folds
2024-09-08 01:59:54,273:INFO:Declaring metric variables
2024-09-08 01:59:54,273:INFO:Importing untrained model
2024-09-08 01:59:54,273:INFO:Declaring custom model
2024-09-08 01:59:54,276:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 01:59:54,286:INFO:Cross validation set to False
2024-09-08 01:59:54,286:INFO:Fitting Model
2024-09-08 01:59:57,360:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 02:00:03,579:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=None, max_leaves=None,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=None, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 02:00:03,579:INFO:create_model() successfully completed......................................
2024-09-08 02:00:03,732:INFO:_master_model_container: 20
2024-09-08 02:00:03,732:INFO:_display_container: 7
2024-09-08 02:00:04,107:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=None, max_leaves=None,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=None, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 02:00:04,107:INFO:finalize_model() successfully completed......................................
2024-09-08 02:00:05,777:INFO:Initializing predict_model()
2024-09-08 02:00:05,777:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000210822686D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=None, max_leaves=None,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=None, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021215A76320>)
2024-09-08 02:00:05,777:INFO:Checking exceptions
2024-09-08 02:00:05,777:INFO:Preloading libraries
2024-09-08 02:00:05,777:INFO:Set up data.
2024-09-08 02:00:05,957:INFO:Set up index.
2024-09-08 02:00:21,028:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:00:21,036:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:00:21,036:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:00:21,038:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:08:24,854:INFO:PyCaret ClassificationExperiment
2024-09-08 02:08:24,854:INFO:Logging name: clf-default-name
2024-09-08 02:08:24,854:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 02:08:24,854:INFO:version 3.3.2
2024-09-08 02:08:24,858:INFO:Initializing setup()
2024-09-08 02:08:24,858:INFO:self.USI: 4c1b
2024-09-08 02:08:24,858:INFO:self._variable_keys: {'data', 'gpu_n_jobs_param', 'exp_id', 'idx', 'memory', 'html_param', 'target_param', 'is_multiclass', 'exp_name_log', 'seed', '_ml_usecase', 'pipeline', 'y', 'gpu_param', 'fold_shuffle_param', 'fold_groups_param', 'log_plots_param', 'X', 'fold_generator', 'fix_imbalance', 'n_jobs_param', 'USI', 'y_train', 'X_train', 'logging_param', '_available_plots', 'X_test', 'y_test'}
2024-09-08 02:08:24,858:INFO:Checking environment
2024-09-08 02:08:24,858:INFO:python_version: 3.10.11
2024-09-08 02:08:24,858:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 02:08:24,858:INFO:machine: AMD64
2024-09-08 02:08:24,874:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 02:08:24,874:INFO:Memory: svmem(total=16407719936, available=3191357440, percent=80.5, used=13216362496, free=3191357440)
2024-09-08 02:08:24,874:INFO:Physical Core: 4
2024-09-08 02:08:24,874:INFO:Logical Core: 8
2024-09-08 02:08:24,874:INFO:Checking libraries
2024-09-08 02:08:24,874:INFO:System:
2024-09-08 02:08:24,874:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 02:08:24,874:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 02:08:24,874:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 02:08:24,874:INFO:PyCaret required dependencies:
2024-09-08 02:08:25,029:INFO:                 pip: 24.2
2024-09-08 02:08:25,029:INFO:          setuptools: 72.1.0
2024-09-08 02:08:25,029:INFO:             pycaret: 3.3.2
2024-09-08 02:08:25,029:INFO:             IPython: 8.25.0
2024-09-08 02:08:25,029:INFO:          ipywidgets: 8.1.5
2024-09-08 02:08:25,029:INFO:                tqdm: 4.66.5
2024-09-08 02:08:25,029:INFO:               numpy: 1.26.4
2024-09-08 02:08:25,029:INFO:              pandas: 2.1.4
2024-09-08 02:08:25,029:INFO:              jinja2: 3.1.4
2024-09-08 02:08:25,029:INFO:               scipy: 1.11.4
2024-09-08 02:08:25,029:INFO:              joblib: 1.3.2
2024-09-08 02:08:25,029:INFO:             sklearn: 1.4.2
2024-09-08 02:08:25,029:INFO:                pyod: 2.0.1
2024-09-08 02:08:25,029:INFO:            imblearn: 0.12.3
2024-09-08 02:08:25,029:INFO:   category_encoders: 2.6.3
2024-09-08 02:08:25,029:INFO:            lightgbm: 4.5.0
2024-09-08 02:08:25,029:INFO:               numba: 0.60.0
2024-09-08 02:08:25,029:INFO:            requests: 2.32.3
2024-09-08 02:08:25,029:INFO:          matplotlib: 3.7.5
2024-09-08 02:08:25,029:INFO:          scikitplot: 0.3.7
2024-09-08 02:08:25,029:INFO:         yellowbrick: 1.5
2024-09-08 02:08:25,029:INFO:              plotly: 5.24.0
2024-09-08 02:08:25,029:INFO:    plotly-resampler: Not installed
2024-09-08 02:08:25,029:INFO:             kaleido: 0.2.1
2024-09-08 02:08:25,029:INFO:           schemdraw: 0.15
2024-09-08 02:08:25,029:INFO:         statsmodels: 0.14.2
2024-09-08 02:08:25,029:INFO:              sktime: 0.26.0
2024-09-08 02:08:25,029:INFO:               tbats: 1.1.3
2024-09-08 02:08:25,029:INFO:            pmdarima: 2.0.4
2024-09-08 02:08:25,029:INFO:              psutil: 5.9.0
2024-09-08 02:08:25,029:INFO:          markupsafe: 2.1.3
2024-09-08 02:08:25,029:INFO:             pickle5: Not installed
2024-09-08 02:08:25,029:INFO:         cloudpickle: 3.0.0
2024-09-08 02:08:25,029:INFO:         deprecation: 2.1.0
2024-09-08 02:08:25,033:INFO:              xxhash: 3.5.0
2024-09-08 02:08:25,033:INFO:           wurlitzer: Not installed
2024-09-08 02:08:25,033:INFO:PyCaret optional dependencies:
2024-09-08 02:08:25,057:INFO:                shap: Not installed
2024-09-08 02:08:25,057:INFO:           interpret: Not installed
2024-09-08 02:08:25,057:INFO:                umap: Not installed
2024-09-08 02:08:25,057:INFO:     ydata_profiling: Not installed
2024-09-08 02:08:25,057:INFO:  explainerdashboard: Not installed
2024-09-08 02:08:25,057:INFO:             autoviz: Not installed
2024-09-08 02:08:25,057:INFO:           fairlearn: Not installed
2024-09-08 02:08:25,064:INFO:          deepchecks: Not installed
2024-09-08 02:08:25,064:INFO:             xgboost: 2.1.1
2024-09-08 02:08:25,064:INFO:            catboost: Not installed
2024-09-08 02:08:25,064:INFO:              kmodes: Not installed
2024-09-08 02:08:25,064:INFO:             mlxtend: Not installed
2024-09-08 02:08:25,064:INFO:       statsforecast: Not installed
2024-09-08 02:08:25,064:INFO:        tune_sklearn: Not installed
2024-09-08 02:08:25,064:INFO:                 ray: Not installed
2024-09-08 02:08:25,064:INFO:            hyperopt: 0.2.7
2024-09-08 02:08:25,066:INFO:              optuna: 4.0.0
2024-09-08 02:08:25,066:INFO:               skopt: 0.10.2
2024-09-08 02:08:25,066:INFO:              mlflow: Not installed
2024-09-08 02:08:25,066:INFO:              gradio: Not installed
2024-09-08 02:08:25,066:INFO:             fastapi: Not installed
2024-09-08 02:08:25,066:INFO:             uvicorn: Not installed
2024-09-08 02:08:25,066:INFO:              m2cgen: Not installed
2024-09-08 02:08:25,066:INFO:           evidently: Not installed
2024-09-08 02:08:25,066:INFO:               fugue: Not installed
2024-09-08 02:08:25,066:INFO:           streamlit: 1.38.0
2024-09-08 02:08:25,066:INFO:             prophet: Not installed
2024-09-08 02:08:25,066:INFO:None
2024-09-08 02:08:25,066:INFO:Set up data.
2024-09-08 02:08:25,334:INFO:Set up folding strategy.
2024-09-08 02:08:25,334:INFO:Set up train/test split.
2024-09-08 02:08:25,445:INFO:Set up index.
2024-09-08 02:08:25,448:INFO:Assigning column types.
2024-09-08 02:08:25,494:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 02:08:25,604:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 02:08:25,606:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 02:08:25,709:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:08:25,719:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:08:25,824:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 02:08:25,830:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 02:08:25,899:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:08:25,904:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:08:25,904:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 02:08:26,012:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 02:08:26,084:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:08:26,087:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:08:26,196:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 02:08:26,264:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:08:26,271:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:08:26,271:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 02:08:26,444:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:08:26,457:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:08:26,628:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:08:26,636:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:08:26,654:INFO:Preparing preprocessing pipeline...
2024-09-08 02:08:26,660:INFO:Set up simple imputation.
2024-09-08 02:08:26,728:INFO:Set up encoding of ordinal features.
2024-09-08 02:08:26,907:INFO:Set up encoding of categorical features.
2024-09-08 02:08:26,916:INFO:Set up column name cleaning.
2024-09-08 02:08:27,766:INFO:Finished creating preprocessing pipeline.
2024-09-08 02:08:28,106:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                             'Fathers occupation'],
                                    transformer=TargetEncoder(cols=[],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 02:08:28,106:INFO:Creating final display dataframe.
2024-09-08 02:08:29,479:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape       (76518, 38)
4        Transformed data shape      (76518, 122)
5   Transformed train set shape      (53562, 122)
6    Transformed test set shape      (22956, 122)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              4c1b
2024-09-08 02:08:29,705:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:08:29,712:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:08:29,886:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:08:29,895:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:08:29,895:INFO:setup() successfully completed in 5.08s...............
2024-09-08 02:08:29,895:INFO:Initializing compare_models()
2024-09-08 02:08:29,895:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 02:08:29,895:INFO:Checking exceptions
2024-09-08 02:08:29,955:INFO:Preparing display monitor
2024-09-08 02:08:29,961:INFO:Initializing Logistic Regression
2024-09-08 02:08:29,961:INFO:Total runtime is 0.0 minutes
2024-09-08 02:08:29,961:INFO:SubProcess create_model() called ==================================
2024-09-08 02:08:29,961:INFO:Initializing create_model()
2024-09-08 02:08:29,961:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:08:29,961:INFO:Checking exceptions
2024-09-08 02:08:29,961:INFO:Importing libraries
2024-09-08 02:08:29,961:INFO:Copying training dataset
2024-09-08 02:08:30,055:INFO:Defining folds
2024-09-08 02:08:30,055:INFO:Declaring metric variables
2024-09-08 02:08:30,055:INFO:Importing untrained model
2024-09-08 02:08:30,055:INFO:Logistic Regression Imported successfully
2024-09-08 02:08:30,055:INFO:Starting cross validation
2024-09-08 02:08:30,065:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:10:08,141:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:10:08,694:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:10:08,984:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:10:09,527:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:10:09,751:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:10:10,277:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:10:11,789:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:10:12,332:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:10:12,367:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:10:12,927:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:10:13,227:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:10:13,892:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:10:15,421:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:10:15,875:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:11:05,707:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:11:05,891:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:11:06,026:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:11:06,211:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:11:06,264:INFO:Calculating mean and std
2024-09-08 02:11:06,264:INFO:Creating metrics dataframe
2024-09-08 02:11:06,264:INFO:Uploading results into container
2024-09-08 02:11:06,271:INFO:Uploading model into container now
2024-09-08 02:11:06,271:INFO:_master_model_container: 1
2024-09-08 02:11:06,271:INFO:_display_container: 2
2024-09-08 02:11:06,271:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 02:11:06,271:INFO:create_model() successfully completed......................................
2024-09-08 02:11:06,421:INFO:SubProcess create_model() end ==================================
2024-09-08 02:11:06,421:INFO:Creating metrics dataframe
2024-09-08 02:11:06,424:INFO:Initializing K Neighbors Classifier
2024-09-08 02:11:06,424:INFO:Total runtime is 2.6077124158541363 minutes
2024-09-08 02:11:06,424:INFO:SubProcess create_model() called ==================================
2024-09-08 02:11:06,424:INFO:Initializing create_model()
2024-09-08 02:11:06,424:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:11:06,424:INFO:Checking exceptions
2024-09-08 02:11:06,424:INFO:Importing libraries
2024-09-08 02:11:06,424:INFO:Copying training dataset
2024-09-08 02:11:06,506:INFO:Defining folds
2024-09-08 02:11:06,506:INFO:Declaring metric variables
2024-09-08 02:11:06,506:INFO:Importing untrained model
2024-09-08 02:11:06,506:INFO:K Neighbors Classifier Imported successfully
2024-09-08 02:11:06,506:INFO:Starting cross validation
2024-09-08 02:11:06,516:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:11:54,715:INFO:Calculating mean and std
2024-09-08 02:11:54,718:INFO:Creating metrics dataframe
2024-09-08 02:11:54,720:INFO:Uploading results into container
2024-09-08 02:11:54,723:INFO:Uploading model into container now
2024-09-08 02:11:54,723:INFO:_master_model_container: 2
2024-09-08 02:11:54,725:INFO:_display_container: 2
2024-09-08 02:11:54,725:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 02:11:54,725:INFO:create_model() successfully completed......................................
2024-09-08 02:11:54,890:INFO:SubProcess create_model() end ==================================
2024-09-08 02:11:54,890:INFO:Creating metrics dataframe
2024-09-08 02:11:54,899:INFO:Initializing Naive Bayes
2024-09-08 02:11:54,899:INFO:Total runtime is 3.4156431674957277 minutes
2024-09-08 02:11:54,899:INFO:SubProcess create_model() called ==================================
2024-09-08 02:11:54,899:INFO:Initializing create_model()
2024-09-08 02:11:54,899:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:11:54,899:INFO:Checking exceptions
2024-09-08 02:11:54,899:INFO:Importing libraries
2024-09-08 02:11:54,899:INFO:Copying training dataset
2024-09-08 02:11:55,003:INFO:Defining folds
2024-09-08 02:11:55,003:INFO:Declaring metric variables
2024-09-08 02:11:55,003:INFO:Importing untrained model
2024-09-08 02:11:55,003:INFO:Naive Bayes Imported successfully
2024-09-08 02:11:55,003:INFO:Starting cross validation
2024-09-08 02:11:55,014:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:12:05,635:INFO:Calculating mean and std
2024-09-08 02:12:05,635:INFO:Creating metrics dataframe
2024-09-08 02:12:05,641:INFO:Uploading results into container
2024-09-08 02:12:05,641:INFO:Uploading model into container now
2024-09-08 02:12:05,641:INFO:_master_model_container: 3
2024-09-08 02:12:05,641:INFO:_display_container: 2
2024-09-08 02:12:05,641:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 02:12:05,641:INFO:create_model() successfully completed......................................
2024-09-08 02:12:05,802:INFO:SubProcess create_model() end ==================================
2024-09-08 02:12:05,802:INFO:Creating metrics dataframe
2024-09-08 02:12:05,811:INFO:Initializing Decision Tree Classifier
2024-09-08 02:12:05,811:INFO:Total runtime is 3.597494101524353 minutes
2024-09-08 02:12:05,811:INFO:SubProcess create_model() called ==================================
2024-09-08 02:12:05,811:INFO:Initializing create_model()
2024-09-08 02:12:05,811:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:12:05,811:INFO:Checking exceptions
2024-09-08 02:12:05,811:INFO:Importing libraries
2024-09-08 02:12:05,811:INFO:Copying training dataset
2024-09-08 02:12:05,900:INFO:Defining folds
2024-09-08 02:12:05,900:INFO:Declaring metric variables
2024-09-08 02:12:05,900:INFO:Importing untrained model
2024-09-08 02:12:05,900:INFO:Decision Tree Classifier Imported successfully
2024-09-08 02:12:05,900:INFO:Starting cross validation
2024-09-08 02:12:05,910:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:12:21,167:INFO:Calculating mean and std
2024-09-08 02:12:21,167:INFO:Creating metrics dataframe
2024-09-08 02:12:21,167:INFO:Uploading results into container
2024-09-08 02:12:21,167:INFO:Uploading model into container now
2024-09-08 02:12:21,175:INFO:_master_model_container: 4
2024-09-08 02:12:21,175:INFO:_display_container: 2
2024-09-08 02:12:21,176:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 02:12:21,176:INFO:create_model() successfully completed......................................
2024-09-08 02:12:21,317:INFO:SubProcess create_model() end ==================================
2024-09-08 02:12:21,317:INFO:Creating metrics dataframe
2024-09-08 02:12:21,327:INFO:Initializing SVM - Linear Kernel
2024-09-08 02:12:21,327:INFO:Total runtime is 3.8561034401257834 minutes
2024-09-08 02:12:21,327:INFO:SubProcess create_model() called ==================================
2024-09-08 02:12:21,327:INFO:Initializing create_model()
2024-09-08 02:12:21,327:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:12:21,327:INFO:Checking exceptions
2024-09-08 02:12:21,327:INFO:Importing libraries
2024-09-08 02:12:21,327:INFO:Copying training dataset
2024-09-08 02:12:21,408:INFO:Defining folds
2024-09-08 02:12:21,408:INFO:Declaring metric variables
2024-09-08 02:12:21,408:INFO:Importing untrained model
2024-09-08 02:12:21,408:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 02:12:21,408:INFO:Starting cross validation
2024-09-08 02:12:21,417:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:13:17,981:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:21,202:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:21,914:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:23,642:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:23,945:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:26,891:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:27,301:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:29,942:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:39,513:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:41,033:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:41,080:INFO:Calculating mean and std
2024-09-08 02:13:41,083:INFO:Creating metrics dataframe
2024-09-08 02:13:41,083:INFO:Uploading results into container
2024-09-08 02:13:41,087:INFO:Uploading model into container now
2024-09-08 02:13:41,087:INFO:_master_model_container: 5
2024-09-08 02:13:41,087:INFO:_display_container: 2
2024-09-08 02:13:41,087:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 02:13:41,087:INFO:create_model() successfully completed......................................
2024-09-08 02:13:41,238:INFO:SubProcess create_model() end ==================================
2024-09-08 02:13:41,238:INFO:Creating metrics dataframe
2024-09-08 02:13:41,246:INFO:Initializing Ridge Classifier
2024-09-08 02:13:41,246:INFO:Total runtime is 5.188085814317068 minutes
2024-09-08 02:13:41,246:INFO:SubProcess create_model() called ==================================
2024-09-08 02:13:41,246:INFO:Initializing create_model()
2024-09-08 02:13:41,246:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:13:41,246:INFO:Checking exceptions
2024-09-08 02:13:41,246:INFO:Importing libraries
2024-09-08 02:13:41,246:INFO:Copying training dataset
2024-09-08 02:13:41,330:INFO:Defining folds
2024-09-08 02:13:41,330:INFO:Declaring metric variables
2024-09-08 02:13:41,332:INFO:Importing untrained model
2024-09-08 02:13:41,332:INFO:Ridge Classifier Imported successfully
2024-09-08 02:13:41,332:INFO:Starting cross validation
2024-09-08 02:13:41,337:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:13:48,181:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:48,261:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:48,291:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:48,331:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:48,513:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:48,541:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:48,600:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:48,657:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:52,108:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:52,272:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:13:52,329:INFO:Calculating mean and std
2024-09-08 02:13:52,338:INFO:Creating metrics dataframe
2024-09-08 02:13:52,348:INFO:Uploading results into container
2024-09-08 02:13:52,353:INFO:Uploading model into container now
2024-09-08 02:13:52,355:INFO:_master_model_container: 6
2024-09-08 02:13:52,355:INFO:_display_container: 2
2024-09-08 02:13:52,355:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 02:13:52,355:INFO:create_model() successfully completed......................................
2024-09-08 02:13:52,568:INFO:SubProcess create_model() end ==================================
2024-09-08 02:13:52,568:INFO:Creating metrics dataframe
2024-09-08 02:13:52,588:INFO:Initializing Random Forest Classifier
2024-09-08 02:13:52,588:INFO:Total runtime is 5.37712235848109 minutes
2024-09-08 02:13:52,588:INFO:SubProcess create_model() called ==================================
2024-09-08 02:13:52,588:INFO:Initializing create_model()
2024-09-08 02:13:52,588:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:13:52,588:INFO:Checking exceptions
2024-09-08 02:13:52,588:INFO:Importing libraries
2024-09-08 02:13:52,588:INFO:Copying training dataset
2024-09-08 02:13:52,693:INFO:Defining folds
2024-09-08 02:13:52,693:INFO:Declaring metric variables
2024-09-08 02:13:52,698:INFO:Importing untrained model
2024-09-08 02:13:52,698:INFO:Random Forest Classifier Imported successfully
2024-09-08 02:13:52,698:INFO:Starting cross validation
2024-09-08 02:13:52,708:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:14:44,977:INFO:Calculating mean and std
2024-09-08 02:14:44,977:INFO:Creating metrics dataframe
2024-09-08 02:14:44,985:INFO:Uploading results into container
2024-09-08 02:14:44,985:INFO:Uploading model into container now
2024-09-08 02:14:44,985:INFO:_master_model_container: 7
2024-09-08 02:14:44,985:INFO:_display_container: 2
2024-09-08 02:14:44,985:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 02:14:44,992:INFO:create_model() successfully completed......................................
2024-09-08 02:14:45,162:INFO:SubProcess create_model() end ==================================
2024-09-08 02:14:45,162:INFO:Creating metrics dataframe
2024-09-08 02:14:45,172:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 02:14:45,172:INFO:Total runtime is 6.2535210529963186 minutes
2024-09-08 02:14:45,172:INFO:SubProcess create_model() called ==================================
2024-09-08 02:14:45,172:INFO:Initializing create_model()
2024-09-08 02:14:45,172:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:14:45,172:INFO:Checking exceptions
2024-09-08 02:14:45,172:INFO:Importing libraries
2024-09-08 02:14:45,172:INFO:Copying training dataset
2024-09-08 02:14:45,257:INFO:Defining folds
2024-09-08 02:14:45,257:INFO:Declaring metric variables
2024-09-08 02:14:45,257:INFO:Importing untrained model
2024-09-08 02:14:45,261:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 02:14:45,261:INFO:Starting cross validation
2024-09-08 02:14:45,269:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:14:53,654:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:14:53,844:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:14:53,878:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:14:53,913:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:14:54,190:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:14:54,949:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:14:55,238:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:14:55,269:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:14:58,548:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:14:58,594:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:14:58,783:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:14:58,801:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:14:58,981:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:14:59,248:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:14:59,333:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:14:59,348:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:02,474:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:15:02,634:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:15:03,762:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:03,861:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:03,902:INFO:Calculating mean and std
2024-09-08 02:15:03,902:INFO:Creating metrics dataframe
2024-09-08 02:15:03,908:INFO:Uploading results into container
2024-09-08 02:15:03,910:INFO:Uploading model into container now
2024-09-08 02:15:03,910:INFO:_master_model_container: 8
2024-09-08 02:15:03,910:INFO:_display_container: 2
2024-09-08 02:15:03,910:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 02:15:03,910:INFO:create_model() successfully completed......................................
2024-09-08 02:15:04,050:INFO:SubProcess create_model() end ==================================
2024-09-08 02:15:04,050:INFO:Creating metrics dataframe
2024-09-08 02:15:04,059:INFO:Initializing Ada Boost Classifier
2024-09-08 02:15:04,059:INFO:Total runtime is 6.5683046380678825 minutes
2024-09-08 02:15:04,059:INFO:SubProcess create_model() called ==================================
2024-09-08 02:15:04,064:INFO:Initializing create_model()
2024-09-08 02:15:04,064:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:15:04,064:INFO:Checking exceptions
2024-09-08 02:15:04,064:INFO:Importing libraries
2024-09-08 02:15:04,064:INFO:Copying training dataset
2024-09-08 02:15:04,141:INFO:Defining folds
2024-09-08 02:15:04,141:INFO:Declaring metric variables
2024-09-08 02:15:04,141:INFO:Importing untrained model
2024-09-08 02:15:04,141:INFO:Ada Boost Classifier Imported successfully
2024-09-08 02:15:04,141:INFO:Starting cross validation
2024-09-08 02:15:04,149:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:15:09,494:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:15:09,682:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:15:09,762:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:15:09,782:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:15:09,795:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:15:09,801:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:15:09,936:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:15:10,009:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:15:26,844:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:26,893:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:26,961:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:28,170:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:28,291:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:29,226:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:29,385:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:29,821:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:30,716:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:15:30,764:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:15:41,492:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:41,546:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:15:41,591:INFO:Calculating mean and std
2024-09-08 02:15:41,591:INFO:Creating metrics dataframe
2024-09-08 02:15:41,596:INFO:Uploading results into container
2024-09-08 02:15:41,596:INFO:Uploading model into container now
2024-09-08 02:15:41,596:INFO:_master_model_container: 9
2024-09-08 02:15:41,596:INFO:_display_container: 2
2024-09-08 02:15:41,596:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 02:15:41,596:INFO:create_model() successfully completed......................................
2024-09-08 02:15:41,761:INFO:SubProcess create_model() end ==================================
2024-09-08 02:15:41,761:INFO:Creating metrics dataframe
2024-09-08 02:15:41,772:INFO:Initializing Gradient Boosting Classifier
2024-09-08 02:15:41,772:INFO:Total runtime is 7.196848479906719 minutes
2024-09-08 02:15:41,772:INFO:SubProcess create_model() called ==================================
2024-09-08 02:15:41,772:INFO:Initializing create_model()
2024-09-08 02:15:41,772:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:15:41,772:INFO:Checking exceptions
2024-09-08 02:15:41,772:INFO:Importing libraries
2024-09-08 02:15:41,772:INFO:Copying training dataset
2024-09-08 02:15:41,860:INFO:Defining folds
2024-09-08 02:15:41,860:INFO:Declaring metric variables
2024-09-08 02:15:41,860:INFO:Importing untrained model
2024-09-08 02:15:41,861:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 02:15:41,861:INFO:Starting cross validation
2024-09-08 02:15:41,868:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:18:28,859:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:18:29,559:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:18:31,069:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:18:31,464:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:18:31,859:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:18:33,146:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:18:33,919:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:18:35,329:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:36,636:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:37,284:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:37,314:INFO:Calculating mean and std
2024-09-08 02:19:37,314:INFO:Creating metrics dataframe
2024-09-08 02:19:37,316:INFO:Uploading results into container
2024-09-08 02:19:37,316:INFO:Uploading model into container now
2024-09-08 02:19:37,316:INFO:_master_model_container: 10
2024-09-08 02:19:37,316:INFO:_display_container: 2
2024-09-08 02:19:37,316:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 02:19:37,316:INFO:create_model() successfully completed......................................
2024-09-08 02:19:37,441:INFO:SubProcess create_model() end ==================================
2024-09-08 02:19:37,441:INFO:Creating metrics dataframe
2024-09-08 02:19:37,444:INFO:Initializing Linear Discriminant Analysis
2024-09-08 02:19:37,444:INFO:Total runtime is 11.12471381823222 minutes
2024-09-08 02:19:37,444:INFO:SubProcess create_model() called ==================================
2024-09-08 02:19:37,444:INFO:Initializing create_model()
2024-09-08 02:19:37,444:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:19:37,444:INFO:Checking exceptions
2024-09-08 02:19:37,444:INFO:Importing libraries
2024-09-08 02:19:37,444:INFO:Copying training dataset
2024-09-08 02:19:37,484:INFO:Defining folds
2024-09-08 02:19:37,484:INFO:Declaring metric variables
2024-09-08 02:19:37,489:INFO:Importing untrained model
2024-09-08 02:19:37,489:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 02:19:37,489:INFO:Starting cross validation
2024-09-08 02:19:37,494:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:19:49,265:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:49,280:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:49,397:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:49,488:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:49,515:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:49,614:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:49,738:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:49,790:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:53,527:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:53,609:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:19:53,625:INFO:Calculating mean and std
2024-09-08 02:19:53,627:INFO:Creating metrics dataframe
2024-09-08 02:19:53,628:INFO:Uploading results into container
2024-09-08 02:19:53,628:INFO:Uploading model into container now
2024-09-08 02:19:53,628:INFO:_master_model_container: 11
2024-09-08 02:19:53,628:INFO:_display_container: 2
2024-09-08 02:19:53,628:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 02:19:53,628:INFO:create_model() successfully completed......................................
2024-09-08 02:19:53,750:INFO:SubProcess create_model() end ==================================
2024-09-08 02:19:53,750:INFO:Creating metrics dataframe
2024-09-08 02:19:53,750:INFO:Initializing Extra Trees Classifier
2024-09-08 02:19:53,750:INFO:Total runtime is 11.396481251716615 minutes
2024-09-08 02:19:53,755:INFO:SubProcess create_model() called ==================================
2024-09-08 02:19:53,755:INFO:Initializing create_model()
2024-09-08 02:19:53,755:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:19:53,755:INFO:Checking exceptions
2024-09-08 02:19:53,755:INFO:Importing libraries
2024-09-08 02:19:53,755:INFO:Copying training dataset
2024-09-08 02:19:53,795:INFO:Defining folds
2024-09-08 02:19:53,795:INFO:Declaring metric variables
2024-09-08 02:19:53,795:INFO:Importing untrained model
2024-09-08 02:19:53,795:INFO:Extra Trees Classifier Imported successfully
2024-09-08 02:19:53,795:INFO:Starting cross validation
2024-09-08 02:19:53,795:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:20:49,477:INFO:Calculating mean and std
2024-09-08 02:20:49,481:INFO:Creating metrics dataframe
2024-09-08 02:20:49,511:INFO:Uploading results into container
2024-09-08 02:20:49,514:INFO:Uploading model into container now
2024-09-08 02:20:49,517:INFO:_master_model_container: 12
2024-09-08 02:20:49,518:INFO:_display_container: 2
2024-09-08 02:20:49,520:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 02:20:49,520:INFO:create_model() successfully completed......................................
2024-09-08 02:20:49,851:INFO:SubProcess create_model() end ==================================
2024-09-08 02:20:49,851:INFO:Creating metrics dataframe
2024-09-08 02:20:49,864:INFO:Initializing Extreme Gradient Boosting
2024-09-08 02:20:49,865:INFO:Total runtime is 12.331729710102083 minutes
2024-09-08 02:20:49,865:INFO:SubProcess create_model() called ==================================
2024-09-08 02:20:49,866:INFO:Initializing create_model()
2024-09-08 02:20:49,866:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:20:49,867:INFO:Checking exceptions
2024-09-08 02:20:49,867:INFO:Importing libraries
2024-09-08 02:20:49,868:INFO:Copying training dataset
2024-09-08 02:20:49,994:INFO:Defining folds
2024-09-08 02:20:49,994:INFO:Declaring metric variables
2024-09-08 02:20:49,995:INFO:Importing untrained model
2024-09-08 02:20:49,997:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 02:20:49,998:INFO:Starting cross validation
2024-09-08 02:20:50,011:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:21:20,353:INFO:Calculating mean and std
2024-09-08 02:21:20,354:INFO:Creating metrics dataframe
2024-09-08 02:21:20,357:INFO:Uploading results into container
2024-09-08 02:21:20,357:INFO:Uploading model into container now
2024-09-08 02:21:20,357:INFO:_master_model_container: 13
2024-09-08 02:21:20,357:INFO:_display_container: 2
2024-09-08 02:21:20,357:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 02:21:20,357:INFO:create_model() successfully completed......................................
2024-09-08 02:21:20,502:INFO:SubProcess create_model() end ==================================
2024-09-08 02:21:20,502:INFO:Creating metrics dataframe
2024-09-08 02:21:20,508:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 02:21:20,508:INFO:Total runtime is 12.8424564520518 minutes
2024-09-08 02:21:20,508:INFO:SubProcess create_model() called ==================================
2024-09-08 02:21:20,508:INFO:Initializing create_model()
2024-09-08 02:21:20,508:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:21:20,508:INFO:Checking exceptions
2024-09-08 02:21:20,508:INFO:Importing libraries
2024-09-08 02:21:20,508:INFO:Copying training dataset
2024-09-08 02:21:20,554:INFO:Defining folds
2024-09-08 02:21:20,554:INFO:Declaring metric variables
2024-09-08 02:21:20,562:INFO:Importing untrained model
2024-09-08 02:21:20,562:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 02:21:20,562:INFO:Starting cross validation
2024-09-08 02:21:20,562:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:21:40,709:INFO:Calculating mean and std
2024-09-08 02:21:40,711:INFO:Creating metrics dataframe
2024-09-08 02:21:40,711:INFO:Uploading results into container
2024-09-08 02:21:40,711:INFO:Uploading model into container now
2024-09-08 02:21:40,716:INFO:_master_model_container: 14
2024-09-08 02:21:40,716:INFO:_display_container: 2
2024-09-08 02:21:40,716:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 02:21:40,716:INFO:create_model() successfully completed......................................
2024-09-08 02:21:40,862:INFO:SubProcess create_model() end ==================================
2024-09-08 02:21:40,862:INFO:Creating metrics dataframe
2024-09-08 02:21:40,867:INFO:Initializing Dummy Classifier
2024-09-08 02:21:40,867:INFO:Total runtime is 13.1817666610082 minutes
2024-09-08 02:21:40,867:INFO:SubProcess create_model() called ==================================
2024-09-08 02:21:40,867:INFO:Initializing create_model()
2024-09-08 02:21:40,867:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000246147B8670>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:21:40,867:INFO:Checking exceptions
2024-09-08 02:21:40,867:INFO:Importing libraries
2024-09-08 02:21:40,867:INFO:Copying training dataset
2024-09-08 02:21:40,909:INFO:Defining folds
2024-09-08 02:21:40,909:INFO:Declaring metric variables
2024-09-08 02:21:40,909:INFO:Importing untrained model
2024-09-08 02:21:40,909:INFO:Dummy Classifier Imported successfully
2024-09-08 02:21:40,909:INFO:Starting cross validation
2024-09-08 02:21:40,917:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:21:45,108:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:21:45,230:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:21:45,270:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:21:45,293:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:21:45,341:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:21:45,351:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:21:45,351:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:21:45,401:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:21:47,309:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:21:47,315:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:21:47,331:INFO:Calculating mean and std
2024-09-08 02:21:47,331:INFO:Creating metrics dataframe
2024-09-08 02:21:47,334:INFO:Uploading results into container
2024-09-08 02:21:47,334:INFO:Uploading model into container now
2024-09-08 02:21:47,334:INFO:_master_model_container: 15
2024-09-08 02:21:47,334:INFO:_display_container: 2
2024-09-08 02:21:47,334:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 02:21:47,334:INFO:create_model() successfully completed......................................
2024-09-08 02:21:47,457:INFO:SubProcess create_model() end ==================================
2024-09-08 02:21:47,457:INFO:Creating metrics dataframe
2024-09-08 02:21:47,459:INFO:Initializing create_model()
2024-09-08 02:21:47,459:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:21:47,459:INFO:Checking exceptions
2024-09-08 02:21:47,459:INFO:Importing libraries
2024-09-08 02:21:47,459:INFO:Copying training dataset
2024-09-08 02:21:47,507:INFO:Defining folds
2024-09-08 02:21:47,507:INFO:Declaring metric variables
2024-09-08 02:21:47,508:INFO:Importing untrained model
2024-09-08 02:21:47,508:INFO:Declaring custom model
2024-09-08 02:21:47,509:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 02:21:47,513:INFO:Cross validation set to False
2024-09-08 02:21:47,513:INFO:Fitting Model
2024-09-08 02:21:48,799:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 02:21:49,115:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 02:21:49,148:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008362 seconds.
2024-09-08 02:21:49,148:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 02:21:49,148:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 02:21:49,148:INFO:[LightGBM] [Info] Total Bins 1658
2024-09-08 02:21:49,148:INFO:[LightGBM] [Info] Number of data points in the train set: 53562, number of used features: 91
2024-09-08 02:21:49,148:INFO:[LightGBM] [Info] Start training from score -0.746209
2024-09-08 02:21:49,148:INFO:[LightGBM] [Info] Start training from score -1.106880
2024-09-08 02:21:49,148:INFO:[LightGBM] [Info] Start training from score -1.633473
2024-09-08 02:21:50,509:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 02:21:50,509:INFO:create_model() successfully completed......................................
2024-09-08 02:21:50,662:INFO:Initializing create_model()
2024-09-08 02:21:50,662:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:21:50,662:INFO:Checking exceptions
2024-09-08 02:21:50,664:INFO:Importing libraries
2024-09-08 02:21:50,664:INFO:Copying training dataset
2024-09-08 02:21:50,709:INFO:Defining folds
2024-09-08 02:21:50,709:INFO:Declaring metric variables
2024-09-08 02:21:50,709:INFO:Importing untrained model
2024-09-08 02:21:50,709:INFO:Declaring custom model
2024-09-08 02:21:50,715:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 02:21:50,719:INFO:Cross validation set to False
2024-09-08 02:21:50,719:INFO:Fitting Model
2024-09-08 02:21:52,149:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 02:21:54,679:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 02:21:54,679:INFO:create_model() successfully completed......................................
2024-09-08 02:21:54,826:INFO:Initializing create_model()
2024-09-08 02:21:54,834:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:21:54,834:INFO:Checking exceptions
2024-09-08 02:21:54,834:INFO:Importing libraries
2024-09-08 02:21:54,834:INFO:Copying training dataset
2024-09-08 02:21:54,874:INFO:Defining folds
2024-09-08 02:21:54,874:INFO:Declaring metric variables
2024-09-08 02:21:54,874:INFO:Importing untrained model
2024-09-08 02:21:54,874:INFO:Declaring custom model
2024-09-08 02:21:54,879:INFO:Random Forest Classifier Imported successfully
2024-09-08 02:21:54,882:INFO:Cross validation set to False
2024-09-08 02:21:54,882:INFO:Fitting Model
2024-09-08 02:21:56,099:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 02:21:59,570:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 02:21:59,570:INFO:create_model() successfully completed......................................
2024-09-08 02:21:59,702:INFO:_master_model_container: 15
2024-09-08 02:21:59,702:INFO:_display_container: 2
2024-09-08 02:21:59,707:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)]
2024-09-08 02:21:59,707:INFO:compare_models() successfully completed......................................
2024-09-08 02:21:59,707:INFO:Initializing create_model()
2024-09-08 02:21:59,707:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:21:59,707:INFO:Checking exceptions
2024-09-08 02:21:59,709:INFO:Importing libraries
2024-09-08 02:21:59,709:INFO:Copying training dataset
2024-09-08 02:21:59,758:INFO:Defining folds
2024-09-08 02:21:59,758:INFO:Declaring metric variables
2024-09-08 02:21:59,758:INFO:Importing untrained model
2024-09-08 02:21:59,759:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 02:21:59,759:INFO:Starting cross validation
2024-09-08 02:21:59,759:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:22:20,073:INFO:Calculating mean and std
2024-09-08 02:22:20,073:INFO:Creating metrics dataframe
2024-09-08 02:22:20,073:INFO:Finalizing model
2024-09-08 02:22:21,487:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 02:22:21,801:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 02:22:21,831:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007828 seconds.
2024-09-08 02:22:21,831:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 02:22:21,831:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 02:22:21,832:INFO:[LightGBM] [Info] Total Bins 1658
2024-09-08 02:22:21,832:INFO:[LightGBM] [Info] Number of data points in the train set: 53562, number of used features: 91
2024-09-08 02:22:21,832:INFO:[LightGBM] [Info] Start training from score -0.746209
2024-09-08 02:22:21,832:INFO:[LightGBM] [Info] Start training from score -1.106880
2024-09-08 02:22:21,832:INFO:[LightGBM] [Info] Start training from score -1.633473
2024-09-08 02:22:23,171:INFO:Uploading results into container
2024-09-08 02:22:23,172:INFO:Uploading model into container now
2024-09-08 02:22:23,195:INFO:_master_model_container: 16
2024-09-08 02:22:23,195:INFO:_display_container: 3
2024-09-08 02:22:23,195:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 02:22:23,195:INFO:create_model() successfully completed......................................
2024-09-08 02:22:23,431:INFO:Initializing create_model()
2024-09-08 02:22:23,431:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:22:23,431:INFO:Checking exceptions
2024-09-08 02:22:23,431:INFO:Importing libraries
2024-09-08 02:22:23,431:INFO:Copying training dataset
2024-09-08 02:22:23,496:INFO:Defining folds
2024-09-08 02:22:23,496:INFO:Declaring metric variables
2024-09-08 02:22:23,496:INFO:Importing untrained model
2024-09-08 02:22:23,496:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 02:22:23,496:INFO:Starting cross validation
2024-09-08 02:22:23,501:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:22:49,806:INFO:Calculating mean and std
2024-09-08 02:22:49,808:INFO:Creating metrics dataframe
2024-09-08 02:22:49,809:INFO:Finalizing model
2024-09-08 02:22:51,245:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 02:22:54,676:INFO:Uploading results into container
2024-09-08 02:22:54,677:INFO:Uploading model into container now
2024-09-08 02:22:54,707:INFO:_master_model_container: 17
2024-09-08 02:22:54,708:INFO:_display_container: 4
2024-09-08 02:22:54,710:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 02:22:54,710:INFO:create_model() successfully completed......................................
2024-09-08 02:22:54,894:INFO:Initializing create_model()
2024-09-08 02:22:54,897:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:22:54,897:INFO:Checking exceptions
2024-09-08 02:22:54,899:INFO:Importing libraries
2024-09-08 02:22:54,899:INFO:Copying training dataset
2024-09-08 02:22:54,971:INFO:Defining folds
2024-09-08 02:22:54,971:INFO:Declaring metric variables
2024-09-08 02:22:54,971:INFO:Importing untrained model
2024-09-08 02:22:54,973:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 02:22:54,973:INFO:Starting cross validation
2024-09-08 02:22:54,977:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:25:00,662:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:25:01,629:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:25:01,648:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:25:02,138:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:25:02,658:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:25:04,098:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:25:04,515:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:25:05,172:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:26:04,812:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:26:05,505:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:26:05,543:INFO:Calculating mean and std
2024-09-08 02:26:05,544:INFO:Creating metrics dataframe
2024-09-08 02:26:05,548:INFO:Finalizing model
2024-09-08 02:26:07,502:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 02:27:10,337:INFO:Uploading results into container
2024-09-08 02:27:10,337:INFO:Uploading model into container now
2024-09-08 02:27:10,364:INFO:_master_model_container: 18
2024-09-08 02:27:10,364:INFO:_display_container: 5
2024-09-08 02:27:10,365:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 02:27:10,365:INFO:create_model() successfully completed......................................
2024-09-08 02:27:10,509:INFO:Initializing tune_model()
2024-09-08 02:27:10,510:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>)
2024-09-08 02:27:10,510:INFO:Checking exceptions
2024-09-08 02:27:10,510:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 02:27:10,608:INFO:Copying training dataset
2024-09-08 02:27:10,638:INFO:Checking base model
2024-09-08 02:27:10,638:INFO:Base model : Extreme Gradient Boosting
2024-09-08 02:27:10,638:INFO:Declaring metric variables
2024-09-08 02:27:10,638:INFO:Defining Hyperparameters
2024-09-08 02:27:10,764:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 02:27:10,764:INFO:Tuning with n_jobs=-1
2024-09-08 02:27:10,767:INFO:Initializing skopt.BayesSearchCV
2024-09-08 02:28:52,440:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 1.0), ('actual_estimator__gamma', 0.3), ('actual_estimator__learning_rate', 0.2), ('actual_estimator__max_depth', 3), ('actual_estimator__min_child_weight', 5), ('actual_estimator__n_estimators', 50), ('actual_estimator__subsample', 0.6)])
2024-09-08 02:28:52,440:INFO:Hyperparameter search completed
2024-09-08 02:28:52,445:INFO:SubProcess create_model() called ==================================
2024-09-08 02:28:52,446:INFO:Initializing create_model()
2024-09-08 02:28:52,446:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000244C975C610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 1.0, 'gamma': 0.3, 'learning_rate': 0.2, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 50, 'subsample': 0.6})
2024-09-08 02:28:52,446:INFO:Checking exceptions
2024-09-08 02:28:52,446:INFO:Importing libraries
2024-09-08 02:28:52,447:INFO:Copying training dataset
2024-09-08 02:28:52,491:INFO:Defining folds
2024-09-08 02:28:52,491:INFO:Declaring metric variables
2024-09-08 02:28:52,492:INFO:Importing untrained model
2024-09-08 02:28:52,492:INFO:Declaring custom model
2024-09-08 02:28:52,493:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 02:28:52,493:INFO:Starting cross validation
2024-09-08 02:28:52,499:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:28:59,512:INFO:Calculating mean and std
2024-09-08 02:28:59,513:INFO:Creating metrics dataframe
2024-09-08 02:28:59,515:INFO:Finalizing model
2024-09-08 02:29:00,875:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 02:29:02,496:INFO:Uploading results into container
2024-09-08 02:29:02,496:INFO:Uploading model into container now
2024-09-08 02:29:02,501:INFO:_master_model_container: 19
2024-09-08 02:29:02,501:INFO:_display_container: 6
2024-09-08 02:29:02,501:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=1.0, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0.3, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.2, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=3, max_leaves=None,
              min_child_weight=5, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=50, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 02:29:02,501:INFO:create_model() successfully completed......................................
2024-09-08 02:29:02,679:INFO:SubProcess create_model() end ==================================
2024-09-08 02:29:02,679:INFO:choose_better activated
2024-09-08 02:29:02,679:INFO:SubProcess create_model() called ==================================
2024-09-08 02:29:02,679:INFO:Initializing create_model()
2024-09-08 02:29:02,679:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:29:02,679:INFO:Checking exceptions
2024-09-08 02:29:02,679:INFO:Importing libraries
2024-09-08 02:29:02,679:INFO:Copying training dataset
2024-09-08 02:29:02,731:INFO:Defining folds
2024-09-08 02:29:02,731:INFO:Declaring metric variables
2024-09-08 02:29:02,731:INFO:Importing untrained model
2024-09-08 02:29:02,731:INFO:Declaring custom model
2024-09-08 02:29:02,731:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 02:29:02,731:INFO:Starting cross validation
2024-09-08 02:29:02,736:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:29:13,810:INFO:Calculating mean and std
2024-09-08 02:29:13,810:INFO:Creating metrics dataframe
2024-09-08 02:29:13,810:INFO:Finalizing model
2024-09-08 02:29:15,057:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 02:29:18,669:INFO:Uploading results into container
2024-09-08 02:29:18,669:INFO:Uploading model into container now
2024-09-08 02:29:18,669:INFO:_master_model_container: 20
2024-09-08 02:29:18,669:INFO:_display_container: 7
2024-09-08 02:29:18,669:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 02:29:18,669:INFO:create_model() successfully completed......................................
2024-09-08 02:29:18,862:INFO:SubProcess create_model() end ==================================
2024-09-08 02:29:18,862:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8279
2024-09-08 02:29:18,862:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=1.0, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0.3, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.2, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=3, max_leaves=None,
              min_child_weight=5, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=50, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.826
2024-09-08 02:29:18,867:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 02:29:18,867:INFO:choose_better completed
2024-09-08 02:29:18,867:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-09-08 02:29:18,886:INFO:_master_model_container: 20
2024-09-08 02:29:18,887:INFO:_display_container: 6
2024-09-08 02:29:18,888:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 02:29:18,888:INFO:tune_model() successfully completed......................................
2024-09-08 02:29:19,048:INFO:Initializing predict_model()
2024-09-08 02:29:19,048:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000024495171480>)
2024-09-08 02:29:19,048:INFO:Checking exceptions
2024-09-08 02:29:19,048:INFO:Preloading libraries
2024-09-08 02:29:19,987:INFO:Initializing get_config()
2024-09-08 02:29:19,987:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, variable=X_train)
2024-09-08 02:29:19,987:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 02:29:20,049:INFO:Variable:  returned as           id Marital status Application mode  ... Unemployment rate Inflation rate   GDP
63473  63473              1               39  ...              13.9           -0.3  0.79
58279  58279              1               39  ...              15.5            2.8 -4.06
68323  68323              1               18  ...              13.9           -0.3  0.79
27374  27374              1                1  ...              13.9           -0.3  0.79
46832  46832              1               17  ...              13.9           -0.3  0.79
...      ...            ...              ...  ...               ...            ...   ...
71758  71758              1                1  ...              12.7            3.7 -1.70
38253  38253              1               17  ...               9.4           -0.8 -3.12
71420  71420              1                1  ...              15.5            2.8 -4.06
74640  74640              1               17  ...               9.4           -0.8 -3.12
13710  13710              1               17  ...               8.9            1.4  3.51

[53562 rows x 37 columns]
2024-09-08 02:29:20,057:INFO:get_config() successfully completed......................................
2024-09-08 02:29:20,057:INFO:Initializing predict_model()
2024-09-08 02:29:20,057:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000024614621E10>)
2024-09-08 02:29:20,057:INFO:Checking exceptions
2024-09-08 02:29:20,057:INFO:Preloading libraries
2024-09-08 02:29:20,057:INFO:Set up data.
2024-09-08 02:29:20,077:INFO:Set up index.
2024-09-08 02:29:20,841:INFO:Initializing get_config()
2024-09-08 02:29:20,841:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, variable=y_train)
2024-09-08 02:29:20,841:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 02:29:20,854:INFO:Variable:  returned as 63473    1
58279    1
68323    0
27374    0
46832    0
        ..
71758    1
38253    1
71420    0
74640    2
13710    0
Name: Target, Length: 53562, dtype: int8
2024-09-08 02:29:20,854:INFO:get_config() successfully completed......................................
2024-09-08 02:29:20,854:INFO:Initializing get_config()
2024-09-08 02:29:20,854:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, variable=y_test)
2024-09-08 02:29:20,855:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 02:29:20,865:INFO:Variable:  returned as 36387    0
29621    1
22002    1
4939     0
66101    0
        ..
65620    0
60576    0
10486    2
62584    2
39904    2
Name: Target, Length: 22956, dtype: int8
2024-09-08 02:29:20,865:INFO:get_config() successfully completed......................................
2024-09-08 02:29:20,866:INFO:Initializing finalize_model()
2024-09-08 02:29:20,866:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 02:29:20,868:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 02:29:20,887:INFO:Initializing create_model()
2024-09-08 02:29:20,887:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:29:20,887:INFO:Checking exceptions
2024-09-08 02:29:20,887:INFO:Importing libraries
2024-09-08 02:29:20,887:INFO:Copying training dataset
2024-09-08 02:29:20,887:INFO:Defining folds
2024-09-08 02:29:20,887:INFO:Declaring metric variables
2024-09-08 02:29:20,892:INFO:Importing untrained model
2024-09-08 02:29:20,892:INFO:Declaring custom model
2024-09-08 02:29:20,892:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 02:29:20,897:INFO:Cross validation set to False
2024-09-08 02:29:20,897:INFO:Fitting Model
2024-09-08 02:29:22,617:INFO:Warning: No categorical columns found. Calling 'transform' will only return input data.
2024-09-08 02:29:27,555:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=None, max_leaves=None,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=None, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 02:29:27,556:INFO:create_model() successfully completed......................................
2024-09-08 02:29:27,685:INFO:_master_model_container: 20
2024-09-08 02:29:27,685:INFO:_display_container: 7
2024-09-08 02:29:27,863:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=None, max_leaves=None,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=None, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 02:29:27,863:INFO:finalize_model() successfully completed......................................
2024-09-08 02:29:28,857:INFO:Initializing predict_model()
2024-09-08 02:29:28,857:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000244CE877F40>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=None, max_leaves=None,
                               min_child_weight=None, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=None, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000024616128C10>)
2024-09-08 02:29:28,857:INFO:Checking exceptions
2024-09-08 02:29:28,857:INFO:Preloading libraries
2024-09-08 02:29:28,857:INFO:Set up data.
2024-09-08 02:29:28,978:INFO:Set up index.
2024-09-08 02:29:41,079:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:41,081:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:41,082:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:41,082:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:46,970:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:46,970:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:46,970:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:46,970:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:52,840:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:52,840:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:52,840:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:52,842:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:57,455:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:57,455:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:57,455:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:29:57,455:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:02,384:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:02,384:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:02,384:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:02,384:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:04,550:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:04,550:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:04,552:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:04,552:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:08,370:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:08,375:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:08,375:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:08,375:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:13,473:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:13,473:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:13,473:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:13,473:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:18,719:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:18,719:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:18,719:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:18,719:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:24,481:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:24,481:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:24,481:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:24,481:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:30,582:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:30,582:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:30,582:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:30,582:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:37,575:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:37,575:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:37,575:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:30:37,575:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:32:21,167:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:32:21,177:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:32:21,177:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:32:21,177:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:37:08,486:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:37:08,487:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:37:08,487:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:37:08,488:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:42:43,394:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:42:43,394:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:42:43,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:42:43,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:42:59,665:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:42:59,681:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:42:59,681:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:42:59,681:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:43:19,368:INFO:PyCaret ClassificationExperiment
2024-09-08 02:43:19,368:INFO:Logging name: clf-default-name
2024-09-08 02:43:19,368:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 02:43:19,375:INFO:version 3.3.2
2024-09-08 02:43:19,376:INFO:Initializing setup()
2024-09-08 02:43:19,376:INFO:self.USI: dd9b
2024-09-08 02:43:19,376:INFO:self._variable_keys: {'_ml_usecase', 'fix_imbalance', 'idx', 'gpu_n_jobs_param', 'y_train', 'X_train', 'memory', 'X', '_available_plots', 'exp_id', 'y_test', 'gpu_param', 'log_plots_param', 'exp_name_log', 'seed', 'fold_shuffle_param', 'data', 'X_test', 'is_multiclass', 'USI', 'pipeline', 'logging_param', 'fold_groups_param', 'y', 'fold_generator', 'n_jobs_param', 'html_param', 'target_param'}
2024-09-08 02:43:19,376:INFO:Checking environment
2024-09-08 02:43:19,376:INFO:python_version: 3.10.11
2024-09-08 02:43:19,376:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 02:43:19,376:INFO:machine: AMD64
2024-09-08 02:43:19,391:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 02:43:19,391:INFO:Memory: svmem(total=16407719936, available=7803736064, percent=52.4, used=8603983872, free=7803736064)
2024-09-08 02:43:19,391:INFO:Physical Core: 4
2024-09-08 02:43:19,391:INFO:Logical Core: 8
2024-09-08 02:43:19,391:INFO:Checking libraries
2024-09-08 02:43:19,391:INFO:System:
2024-09-08 02:43:19,391:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 02:43:19,391:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 02:43:19,391:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 02:43:19,396:INFO:PyCaret required dependencies:
2024-09-08 02:43:19,472:INFO:                 pip: 24.2
2024-09-08 02:43:19,472:INFO:          setuptools: 72.1.0
2024-09-08 02:43:19,472:INFO:             pycaret: 3.3.2
2024-09-08 02:43:19,472:INFO:             IPython: 8.25.0
2024-09-08 02:43:19,472:INFO:          ipywidgets: 8.1.5
2024-09-08 02:43:19,472:INFO:                tqdm: 4.66.5
2024-09-08 02:43:19,472:INFO:               numpy: 1.26.4
2024-09-08 02:43:19,472:INFO:              pandas: 2.1.4
2024-09-08 02:43:19,472:INFO:              jinja2: 3.1.4
2024-09-08 02:43:19,472:INFO:               scipy: 1.11.4
2024-09-08 02:43:19,472:INFO:              joblib: 1.3.2
2024-09-08 02:43:19,472:INFO:             sklearn: 1.4.2
2024-09-08 02:43:19,472:INFO:                pyod: 2.0.1
2024-09-08 02:43:19,472:INFO:            imblearn: 0.12.3
2024-09-08 02:43:19,472:INFO:   category_encoders: 2.6.3
2024-09-08 02:43:19,472:INFO:            lightgbm: 4.5.0
2024-09-08 02:43:19,472:INFO:               numba: 0.60.0
2024-09-08 02:43:19,472:INFO:            requests: 2.32.3
2024-09-08 02:43:19,472:INFO:          matplotlib: 3.7.5
2024-09-08 02:43:19,477:INFO:          scikitplot: 0.3.7
2024-09-08 02:43:19,477:INFO:         yellowbrick: 1.5
2024-09-08 02:43:19,477:INFO:              plotly: 5.24.0
2024-09-08 02:43:19,477:INFO:    plotly-resampler: Not installed
2024-09-08 02:43:19,477:INFO:             kaleido: 0.2.1
2024-09-08 02:43:19,477:INFO:           schemdraw: 0.15
2024-09-08 02:43:19,477:INFO:         statsmodels: 0.14.2
2024-09-08 02:43:19,477:INFO:              sktime: 0.26.0
2024-09-08 02:43:19,477:INFO:               tbats: 1.1.3
2024-09-08 02:43:19,477:INFO:            pmdarima: 2.0.4
2024-09-08 02:43:19,477:INFO:              psutil: 5.9.0
2024-09-08 02:43:19,477:INFO:          markupsafe: 2.1.3
2024-09-08 02:43:19,479:INFO:             pickle5: Not installed
2024-09-08 02:43:19,479:INFO:         cloudpickle: 3.0.0
2024-09-08 02:43:19,479:INFO:         deprecation: 2.1.0
2024-09-08 02:43:19,479:INFO:              xxhash: 3.5.0
2024-09-08 02:43:19,479:INFO:           wurlitzer: Not installed
2024-09-08 02:43:19,479:INFO:PyCaret optional dependencies:
2024-09-08 02:43:19,489:INFO:                shap: Not installed
2024-09-08 02:43:19,489:INFO:           interpret: Not installed
2024-09-08 02:43:19,489:INFO:                umap: Not installed
2024-09-08 02:43:19,489:INFO:     ydata_profiling: Not installed
2024-09-08 02:43:19,489:INFO:  explainerdashboard: Not installed
2024-09-08 02:43:19,489:INFO:             autoviz: Not installed
2024-09-08 02:43:19,489:INFO:           fairlearn: Not installed
2024-09-08 02:43:19,489:INFO:          deepchecks: Not installed
2024-09-08 02:43:19,489:INFO:             xgboost: 2.1.1
2024-09-08 02:43:19,489:INFO:            catboost: Not installed
2024-09-08 02:43:19,489:INFO:              kmodes: Not installed
2024-09-08 02:43:19,489:INFO:             mlxtend: Not installed
2024-09-08 02:43:19,489:INFO:       statsforecast: Not installed
2024-09-08 02:43:19,489:INFO:        tune_sklearn: Not installed
2024-09-08 02:43:19,489:INFO:                 ray: Not installed
2024-09-08 02:43:19,489:INFO:            hyperopt: 0.2.7
2024-09-08 02:43:19,489:INFO:              optuna: 4.0.0
2024-09-08 02:43:19,489:INFO:               skopt: 0.10.2
2024-09-08 02:43:19,489:INFO:              mlflow: Not installed
2024-09-08 02:43:19,489:INFO:              gradio: Not installed
2024-09-08 02:43:19,489:INFO:             fastapi: Not installed
2024-09-08 02:43:19,489:INFO:             uvicorn: Not installed
2024-09-08 02:43:19,489:INFO:              m2cgen: Not installed
2024-09-08 02:43:19,489:INFO:           evidently: Not installed
2024-09-08 02:43:19,489:INFO:               fugue: Not installed
2024-09-08 02:43:19,489:INFO:           streamlit: 1.38.0
2024-09-08 02:43:19,489:INFO:             prophet: Not installed
2024-09-08 02:43:19,495:INFO:None
2024-09-08 02:43:19,495:INFO:Set up data.
2024-09-08 02:43:19,511:INFO:Set up folding strategy.
2024-09-08 02:43:19,511:INFO:Set up train/test split.
2024-09-08 02:43:19,527:INFO:Set up index.
2024-09-08 02:43:19,527:INFO:Assigning column types.
2024-09-08 02:43:19,532:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 02:43:19,588:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 02:43:19,598:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 02:43:19,651:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:43:19,652:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:43:19,712:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 02:43:19,712:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 02:43:19,749:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:43:19,756:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:43:19,756:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 02:43:19,810:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 02:43:19,846:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:43:19,846:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:43:19,912:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 02:43:19,951:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:43:19,954:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:43:19,954:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 02:43:20,042:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:43:20,042:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:43:20,127:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:43:20,132:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:43:20,136:INFO:Preparing preprocessing pipeline...
2024-09-08 02:43:20,136:INFO:Set up simple imputation.
2024-09-08 02:43:20,143:INFO:Set up encoding of ordinal features.
2024-09-08 02:43:20,164:INFO:Set up encoding of categorical features.
2024-09-08 02:43:20,166:INFO:Set up column name cleaning.
2024-09-08 02:43:20,397:INFO:Finished creating preprocessing pipeline.
2024-09-08 02:43:20,607:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                                                    'qualification',
                                                                    'Fathers '
                                                                    'qualification',
                                                                    'Mothers '
                                                                    'occupation',
                                                                    'Fathers '
                                                                    'occupation'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 02:43:20,607:INFO:Creating final display dataframe.
2024-09-08 02:43:20,873:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (1000, 38)
4        Transformed data shape       (1000, 129)
5   Transformed train set shape        (700, 129)
6    Transformed test set shape        (300, 129)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              dd9b
2024-09-08 02:43:20,973:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:43:20,974:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:43:21,062:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:43:21,062:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:43:21,062:INFO:setup() successfully completed in 1.72s...............
2024-09-08 02:43:21,062:INFO:Initializing compare_models()
2024-09-08 02:43:21,062:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023D46913F40>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000023D46913F40>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 02:43:21,067:INFO:Checking exceptions
2024-09-08 02:43:21,067:INFO:Preparing display monitor
2024-09-08 02:43:21,076:INFO:Initializing Logistic Regression
2024-09-08 02:43:21,076:INFO:Total runtime is 0.0 minutes
2024-09-08 02:43:21,076:INFO:SubProcess create_model() called ==================================
2024-09-08 02:43:21,076:INFO:Initializing create_model()
2024-09-08 02:43:21,077:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023D46913F40>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023D5391ACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:43:21,077:INFO:Checking exceptions
2024-09-08 02:43:21,077:INFO:Importing libraries
2024-09-08 02:43:21,077:INFO:Copying training dataset
2024-09-08 02:43:21,084:INFO:Defining folds
2024-09-08 02:43:21,084:INFO:Declaring metric variables
2024-09-08 02:43:21,087:INFO:Importing untrained model
2024-09-08 02:43:21,087:INFO:Logistic Regression Imported successfully
2024-09-08 02:43:21,087:INFO:Starting cross validation
2024-09-08 02:43:21,092:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:43:37,938:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:43:37,948:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:43:37,948:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:43:37,948:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:43:46,945:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:43:46,945:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:43:46,946:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:43:46,946:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 02:44:00,940:INFO:PyCaret ClassificationExperiment
2024-09-08 02:44:00,940:INFO:Logging name: clf-default-name
2024-09-08 02:44:00,940:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 02:44:00,940:INFO:version 3.3.2
2024-09-08 02:44:00,940:INFO:Initializing setup()
2024-09-08 02:44:00,940:INFO:self.USI: ad90
2024-09-08 02:44:00,940:INFO:self._variable_keys: {'fold_generator', 'log_plots_param', 'logging_param', 'fold_shuffle_param', 'y_train', 'gpu_n_jobs_param', 'exp_id', 'is_multiclass', 'n_jobs_param', 'X', 'X_train', '_ml_usecase', 'X_test', 'y', 'fold_groups_param', 'USI', 'seed', 'y_test', 'target_param', '_available_plots', 'memory', 'exp_name_log', 'html_param', 'fix_imbalance', 'idx', 'pipeline', 'gpu_param', 'data'}
2024-09-08 02:44:00,940:INFO:Checking environment
2024-09-08 02:44:00,940:INFO:python_version: 3.10.11
2024-09-08 02:44:00,940:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 02:44:00,940:INFO:machine: AMD64
2024-09-08 02:44:00,960:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 02:44:00,960:INFO:Memory: svmem(total=16407719936, available=7816056832, percent=52.4, used=8591663104, free=7816056832)
2024-09-08 02:44:00,960:INFO:Physical Core: 4
2024-09-08 02:44:00,960:INFO:Logical Core: 8
2024-09-08 02:44:00,960:INFO:Checking libraries
2024-09-08 02:44:00,960:INFO:System:
2024-09-08 02:44:00,960:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 02:44:00,960:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 02:44:00,960:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 02:44:00,965:INFO:PyCaret required dependencies:
2024-09-08 02:44:01,039:INFO:                 pip: 24.2
2024-09-08 02:44:01,039:INFO:          setuptools: 72.1.0
2024-09-08 02:44:01,039:INFO:             pycaret: 3.3.2
2024-09-08 02:44:01,039:INFO:             IPython: 8.25.0
2024-09-08 02:44:01,039:INFO:          ipywidgets: 8.1.5
2024-09-08 02:44:01,039:INFO:                tqdm: 4.66.5
2024-09-08 02:44:01,039:INFO:               numpy: 1.26.4
2024-09-08 02:44:01,039:INFO:              pandas: 2.1.4
2024-09-08 02:44:01,039:INFO:              jinja2: 3.1.4
2024-09-08 02:44:01,039:INFO:               scipy: 1.11.4
2024-09-08 02:44:01,039:INFO:              joblib: 1.3.2
2024-09-08 02:44:01,045:INFO:             sklearn: 1.4.2
2024-09-08 02:44:01,045:INFO:                pyod: 2.0.1
2024-09-08 02:44:01,045:INFO:            imblearn: 0.12.3
2024-09-08 02:44:01,045:INFO:   category_encoders: 2.6.3
2024-09-08 02:44:01,045:INFO:            lightgbm: 4.5.0
2024-09-08 02:44:01,045:INFO:               numba: 0.60.0
2024-09-08 02:44:01,045:INFO:            requests: 2.32.3
2024-09-08 02:44:01,045:INFO:          matplotlib: 3.7.5
2024-09-08 02:44:01,045:INFO:          scikitplot: 0.3.7
2024-09-08 02:44:01,045:INFO:         yellowbrick: 1.5
2024-09-08 02:44:01,045:INFO:              plotly: 5.24.0
2024-09-08 02:44:01,045:INFO:    plotly-resampler: Not installed
2024-09-08 02:44:01,045:INFO:             kaleido: 0.2.1
2024-09-08 02:44:01,045:INFO:           schemdraw: 0.15
2024-09-08 02:44:01,045:INFO:         statsmodels: 0.14.2
2024-09-08 02:44:01,045:INFO:              sktime: 0.26.0
2024-09-08 02:44:01,045:INFO:               tbats: 1.1.3
2024-09-08 02:44:01,045:INFO:            pmdarima: 2.0.4
2024-09-08 02:44:01,045:INFO:              psutil: 5.9.0
2024-09-08 02:44:01,045:INFO:          markupsafe: 2.1.3
2024-09-08 02:44:01,045:INFO:             pickle5: Not installed
2024-09-08 02:44:01,045:INFO:         cloudpickle: 3.0.0
2024-09-08 02:44:01,045:INFO:         deprecation: 2.1.0
2024-09-08 02:44:01,045:INFO:              xxhash: 3.5.0
2024-09-08 02:44:01,045:INFO:           wurlitzer: Not installed
2024-09-08 02:44:01,045:INFO:PyCaret optional dependencies:
2024-09-08 02:44:01,060:INFO:                shap: Not installed
2024-09-08 02:44:01,060:INFO:           interpret: Not installed
2024-09-08 02:44:01,060:INFO:                umap: Not installed
2024-09-08 02:44:01,060:INFO:     ydata_profiling: Not installed
2024-09-08 02:44:01,060:INFO:  explainerdashboard: Not installed
2024-09-08 02:44:01,060:INFO:             autoviz: Not installed
2024-09-08 02:44:01,060:INFO:           fairlearn: Not installed
2024-09-08 02:44:01,060:INFO:          deepchecks: Not installed
2024-09-08 02:44:01,060:INFO:             xgboost: 2.1.1
2024-09-08 02:44:01,060:INFO:            catboost: Not installed
2024-09-08 02:44:01,060:INFO:              kmodes: Not installed
2024-09-08 02:44:01,060:INFO:             mlxtend: Not installed
2024-09-08 02:44:01,060:INFO:       statsforecast: Not installed
2024-09-08 02:44:01,060:INFO:        tune_sklearn: Not installed
2024-09-08 02:44:01,060:INFO:                 ray: Not installed
2024-09-08 02:44:01,060:INFO:            hyperopt: 0.2.7
2024-09-08 02:44:01,060:INFO:              optuna: 4.0.0
2024-09-08 02:44:01,060:INFO:               skopt: 0.10.2
2024-09-08 02:44:01,060:INFO:              mlflow: Not installed
2024-09-08 02:44:01,060:INFO:              gradio: Not installed
2024-09-08 02:44:01,060:INFO:             fastapi: Not installed
2024-09-08 02:44:01,060:INFO:             uvicorn: Not installed
2024-09-08 02:44:01,060:INFO:              m2cgen: Not installed
2024-09-08 02:44:01,060:INFO:           evidently: Not installed
2024-09-08 02:44:01,060:INFO:               fugue: Not installed
2024-09-08 02:44:01,060:INFO:           streamlit: 1.38.0
2024-09-08 02:44:01,060:INFO:             prophet: Not installed
2024-09-08 02:44:01,060:INFO:None
2024-09-08 02:44:01,060:INFO:Set up data.
2024-09-08 02:44:01,075:INFO:Set up folding strategy.
2024-09-08 02:44:01,075:INFO:Set up train/test split.
2024-09-08 02:44:01,085:INFO:Set up index.
2024-09-08 02:44:01,085:INFO:Assigning column types.
2024-09-08 02:44:01,090:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 02:44:01,143:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 02:44:01,149:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 02:44:01,280:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:44:01,295:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:44:01,510:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 02:44:01,513:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 02:44:01,652:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:44:01,664:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:44:01,664:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 02:44:01,880:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 02:44:02,010:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:44:02,021:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:44:02,234:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 02:44:02,360:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:44:02,370:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:44:02,370:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 02:44:02,705:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:44:02,719:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:44:03,065:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:44:03,074:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:44:03,080:INFO:Preparing preprocessing pipeline...
2024-09-08 02:44:03,080:INFO:Set up simple imputation.
2024-09-08 02:44:03,090:INFO:Set up encoding of ordinal features.
2024-09-08 02:44:03,110:INFO:Set up encoding of categorical features.
2024-09-08 02:44:03,115:INFO:Set up column name cleaning.
2024-09-08 02:44:03,352:INFO:Finished creating preprocessing pipeline.
2024-09-08 02:44:03,527:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                                                    'qualification',
                                                                    'Fathers '
                                                                    'qualification',
                                                                    'Mothers '
                                                                    'occupation',
                                                                    'Fathers '
                                                                    'occupation'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 02:44:03,527:INFO:Creating final display dataframe.
2024-09-08 02:44:03,810:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (1000, 38)
4        Transformed data shape       (1000, 129)
5   Transformed train set shape        (700, 129)
6    Transformed test set shape        (300, 129)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              ad90
2024-09-08 02:44:03,910:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:44:03,914:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:44:04,017:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 02:44:04,021:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 02:44:04,021:INFO:setup() successfully completed in 3.11s...............
2024-09-08 02:44:04,021:INFO:Initializing compare_models()
2024-09-08 02:44:04,021:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 02:44:04,021:INFO:Checking exceptions
2024-09-08 02:44:04,033:INFO:Preparing display monitor
2024-09-08 02:44:04,037:INFO:Initializing Logistic Regression
2024-09-08 02:44:04,037:INFO:Total runtime is 0.0 minutes
2024-09-08 02:44:04,037:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:04,037:INFO:Initializing create_model()
2024-09-08 02:44:04,037:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:04,037:INFO:Checking exceptions
2024-09-08 02:44:04,037:INFO:Importing libraries
2024-09-08 02:44:04,040:INFO:Copying training dataset
2024-09-08 02:44:04,052:INFO:Defining folds
2024-09-08 02:44:04,052:INFO:Declaring metric variables
2024-09-08 02:44:04,052:INFO:Importing untrained model
2024-09-08 02:44:04,054:INFO:Logistic Regression Imported successfully
2024-09-08 02:44:04,054:INFO:Starting cross validation
2024-09-08 02:44:04,060:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:13,342:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:44:13,343:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:44:13,370:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:44:13,485:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:44:13,722:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:13,825:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:13,882:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:13,896:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:44:13,900:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:44:13,906:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:14,068:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:44:14,114:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:44:14,164:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:14,167:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:14,341:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:14,409:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:15,373:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:44:15,464:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 02:44:15,501:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:15,576:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:15,613:INFO:Calculating mean and std
2024-09-08 02:44:15,615:INFO:Creating metrics dataframe
2024-09-08 02:44:15,618:INFO:Uploading results into container
2024-09-08 02:44:15,619:INFO:Uploading model into container now
2024-09-08 02:44:15,621:INFO:_master_model_container: 1
2024-09-08 02:44:15,621:INFO:_display_container: 2
2024-09-08 02:44:15,622:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 02:44:15,622:INFO:create_model() successfully completed......................................
2024-09-08 02:44:15,781:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:15,781:INFO:Creating metrics dataframe
2024-09-08 02:44:15,793:INFO:Initializing K Neighbors Classifier
2024-09-08 02:44:15,793:INFO:Total runtime is 0.19592657486597698 minutes
2024-09-08 02:44:15,793:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:15,793:INFO:Initializing create_model()
2024-09-08 02:44:15,793:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:15,793:INFO:Checking exceptions
2024-09-08 02:44:15,793:INFO:Importing libraries
2024-09-08 02:44:15,793:INFO:Copying training dataset
2024-09-08 02:44:15,818:INFO:Defining folds
2024-09-08 02:44:15,818:INFO:Declaring metric variables
2024-09-08 02:44:15,818:INFO:Importing untrained model
2024-09-08 02:44:15,821:INFO:K Neighbors Classifier Imported successfully
2024-09-08 02:44:15,822:INFO:Starting cross validation
2024-09-08 02:44:15,833:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:17,939:INFO:Calculating mean and std
2024-09-08 02:44:17,941:INFO:Creating metrics dataframe
2024-09-08 02:44:17,943:INFO:Uploading results into container
2024-09-08 02:44:17,944:INFO:Uploading model into container now
2024-09-08 02:44:17,944:INFO:_master_model_container: 2
2024-09-08 02:44:17,945:INFO:_display_container: 2
2024-09-08 02:44:17,945:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 02:44:17,945:INFO:create_model() successfully completed......................................
2024-09-08 02:44:18,065:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:18,065:INFO:Creating metrics dataframe
2024-09-08 02:44:18,070:INFO:Initializing Naive Bayes
2024-09-08 02:44:18,070:INFO:Total runtime is 0.23387646675109863 minutes
2024-09-08 02:44:18,070:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:18,070:INFO:Initializing create_model()
2024-09-08 02:44:18,070:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:18,070:INFO:Checking exceptions
2024-09-08 02:44:18,070:INFO:Importing libraries
2024-09-08 02:44:18,071:INFO:Copying training dataset
2024-09-08 02:44:18,080:INFO:Defining folds
2024-09-08 02:44:18,080:INFO:Declaring metric variables
2024-09-08 02:44:18,081:INFO:Importing untrained model
2024-09-08 02:44:18,081:INFO:Naive Bayes Imported successfully
2024-09-08 02:44:18,081:INFO:Starting cross validation
2024-09-08 02:44:18,086:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:19,298:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:19,713:INFO:Calculating mean and std
2024-09-08 02:44:19,714:INFO:Creating metrics dataframe
2024-09-08 02:44:19,717:INFO:Uploading results into container
2024-09-08 02:44:19,720:INFO:Uploading model into container now
2024-09-08 02:44:19,720:INFO:_master_model_container: 3
2024-09-08 02:44:19,720:INFO:_display_container: 2
2024-09-08 02:44:19,721:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 02:44:19,721:INFO:create_model() successfully completed......................................
2024-09-08 02:44:19,851:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:19,851:INFO:Creating metrics dataframe
2024-09-08 02:44:19,851:INFO:Initializing Decision Tree Classifier
2024-09-08 02:44:19,851:INFO:Total runtime is 0.26356503963470457 minutes
2024-09-08 02:44:19,851:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:19,851:INFO:Initializing create_model()
2024-09-08 02:44:19,851:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:19,851:INFO:Checking exceptions
2024-09-08 02:44:19,851:INFO:Importing libraries
2024-09-08 02:44:19,851:INFO:Copying training dataset
2024-09-08 02:44:19,863:INFO:Defining folds
2024-09-08 02:44:19,864:INFO:Declaring metric variables
2024-09-08 02:44:19,864:INFO:Importing untrained model
2024-09-08 02:44:19,864:INFO:Decision Tree Classifier Imported successfully
2024-09-08 02:44:19,864:INFO:Starting cross validation
2024-09-08 02:44:19,869:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:21,398:INFO:Calculating mean and std
2024-09-08 02:44:21,398:INFO:Creating metrics dataframe
2024-09-08 02:44:21,401:INFO:Uploading results into container
2024-09-08 02:44:21,401:INFO:Uploading model into container now
2024-09-08 02:44:21,401:INFO:_master_model_container: 4
2024-09-08 02:44:21,401:INFO:_display_container: 2
2024-09-08 02:44:21,401:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 02:44:21,401:INFO:create_model() successfully completed......................................
2024-09-08 02:44:21,526:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:21,526:INFO:Creating metrics dataframe
2024-09-08 02:44:21,534:INFO:Initializing SVM - Linear Kernel
2024-09-08 02:44:21,534:INFO:Total runtime is 0.29160425265630086 minutes
2024-09-08 02:44:21,534:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:21,534:INFO:Initializing create_model()
2024-09-08 02:44:21,534:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:21,534:INFO:Checking exceptions
2024-09-08 02:44:21,534:INFO:Importing libraries
2024-09-08 02:44:21,534:INFO:Copying training dataset
2024-09-08 02:44:21,541:INFO:Defining folds
2024-09-08 02:44:21,541:INFO:Declaring metric variables
2024-09-08 02:44:21,541:INFO:Importing untrained model
2024-09-08 02:44:21,541:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 02:44:21,541:INFO:Starting cross validation
2024-09-08 02:44:21,550:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:22,563:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:22,572:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:22,574:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:22,574:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:22,581:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:22,586:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:22,590:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:22,623:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:22,637:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:22,643:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:23,031:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:23,034:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:23,048:INFO:Calculating mean and std
2024-09-08 02:44:23,048:INFO:Creating metrics dataframe
2024-09-08 02:44:23,051:INFO:Uploading results into container
2024-09-08 02:44:23,051:INFO:Uploading model into container now
2024-09-08 02:44:23,051:INFO:_master_model_container: 5
2024-09-08 02:44:23,051:INFO:_display_container: 2
2024-09-08 02:44:23,051:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 02:44:23,051:INFO:create_model() successfully completed......................................
2024-09-08 02:44:23,181:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:23,181:INFO:Creating metrics dataframe
2024-09-08 02:44:23,184:INFO:Initializing Ridge Classifier
2024-09-08 02:44:23,184:INFO:Total runtime is 0.31910863320032756 minutes
2024-09-08 02:44:23,184:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:23,184:INFO:Initializing create_model()
2024-09-08 02:44:23,184:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:23,184:INFO:Checking exceptions
2024-09-08 02:44:23,184:INFO:Importing libraries
2024-09-08 02:44:23,184:INFO:Copying training dataset
2024-09-08 02:44:23,191:INFO:Defining folds
2024-09-08 02:44:23,191:INFO:Declaring metric variables
2024-09-08 02:44:23,191:INFO:Importing untrained model
2024-09-08 02:44:23,191:INFO:Ridge Classifier Imported successfully
2024-09-08 02:44:23,191:INFO:Starting cross validation
2024-09-08 02:44:23,200:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:24,136:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:24,147:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:24,151:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:24,157:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:24,157:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:24,177:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:24,192:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:24,204:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:24,566:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:24,574:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:24,605:INFO:Calculating mean and std
2024-09-08 02:44:24,607:INFO:Creating metrics dataframe
2024-09-08 02:44:24,609:INFO:Uploading results into container
2024-09-08 02:44:24,609:INFO:Uploading model into container now
2024-09-08 02:44:24,610:INFO:_master_model_container: 6
2024-09-08 02:44:24,610:INFO:_display_container: 2
2024-09-08 02:44:24,610:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 02:44:24,610:INFO:create_model() successfully completed......................................
2024-09-08 02:44:24,724:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:24,724:INFO:Creating metrics dataframe
2024-09-08 02:44:24,724:INFO:Initializing Random Forest Classifier
2024-09-08 02:44:24,724:INFO:Total runtime is 0.3447873791058858 minutes
2024-09-08 02:44:24,724:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:24,724:INFO:Initializing create_model()
2024-09-08 02:44:24,724:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:24,724:INFO:Checking exceptions
2024-09-08 02:44:24,724:INFO:Importing libraries
2024-09-08 02:44:24,724:INFO:Copying training dataset
2024-09-08 02:44:24,732:INFO:Defining folds
2024-09-08 02:44:24,732:INFO:Declaring metric variables
2024-09-08 02:44:24,732:INFO:Importing untrained model
2024-09-08 02:44:24,732:INFO:Random Forest Classifier Imported successfully
2024-09-08 02:44:24,732:INFO:Starting cross validation
2024-09-08 02:44:24,742:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:27,244:INFO:Calculating mean and std
2024-09-08 02:44:27,244:INFO:Creating metrics dataframe
2024-09-08 02:44:27,248:INFO:Uploading results into container
2024-09-08 02:44:27,250:INFO:Uploading model into container now
2024-09-08 02:44:27,250:INFO:_master_model_container: 7
2024-09-08 02:44:27,250:INFO:_display_container: 2
2024-09-08 02:44:27,251:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 02:44:27,251:INFO:create_model() successfully completed......................................
2024-09-08 02:44:27,364:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:27,364:INFO:Creating metrics dataframe
2024-09-08 02:44:27,365:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 02:44:27,365:INFO:Total runtime is 0.38880259593327837 minutes
2024-09-08 02:44:27,365:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:27,365:INFO:Initializing create_model()
2024-09-08 02:44:27,365:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:27,365:INFO:Checking exceptions
2024-09-08 02:44:27,365:INFO:Importing libraries
2024-09-08 02:44:27,365:INFO:Copying training dataset
2024-09-08 02:44:27,375:INFO:Defining folds
2024-09-08 02:44:27,375:INFO:Declaring metric variables
2024-09-08 02:44:27,375:INFO:Importing untrained model
2024-09-08 02:44:27,375:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 02:44:27,375:INFO:Starting cross validation
2024-09-08 02:44:27,377:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:28,088:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:44:28,088:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:44:28,097:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:44:28,117:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:44:28,130:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:44:28,156:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:44:28,162:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:44:28,180:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:44:28,352:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:28,358:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:28,372:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:28,393:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:28,407:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:28,434:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:28,434:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:28,452:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:28,782:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:44:28,796:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 02:44:28,952:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:28,962:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:28,986:INFO:Calculating mean and std
2024-09-08 02:44:28,987:INFO:Creating metrics dataframe
2024-09-08 02:44:28,990:INFO:Uploading results into container
2024-09-08 02:44:28,990:INFO:Uploading model into container now
2024-09-08 02:44:28,992:INFO:_master_model_container: 8
2024-09-08 02:44:28,992:INFO:_display_container: 2
2024-09-08 02:44:28,992:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 02:44:28,992:INFO:create_model() successfully completed......................................
2024-09-08 02:44:29,122:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:29,122:INFO:Creating metrics dataframe
2024-09-08 02:44:29,127:INFO:Initializing Ada Boost Classifier
2024-09-08 02:44:29,127:INFO:Total runtime is 0.41815592447916666 minutes
2024-09-08 02:44:29,127:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:29,127:INFO:Initializing create_model()
2024-09-08 02:44:29,127:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:29,127:INFO:Checking exceptions
2024-09-08 02:44:29,127:INFO:Importing libraries
2024-09-08 02:44:29,127:INFO:Copying training dataset
2024-09-08 02:44:29,133:INFO:Defining folds
2024-09-08 02:44:29,133:INFO:Declaring metric variables
2024-09-08 02:44:29,133:INFO:Importing untrained model
2024-09-08 02:44:29,133:INFO:Ada Boost Classifier Imported successfully
2024-09-08 02:44:29,133:INFO:Starting cross validation
2024-09-08 02:44:29,142:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:29,866:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:44:29,915:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:44:29,937:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:44:30,031:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:44:30,052:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:44:30,079:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:44:30,079:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:44:30,087:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:44:30,557:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:30,566:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:30,583:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:30,687:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:30,714:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:30,779:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:30,795:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:31,032:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:44:31,082:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 02:44:31,392:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:31,453:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:31,469:INFO:Calculating mean and std
2024-09-08 02:44:31,469:INFO:Creating metrics dataframe
2024-09-08 02:44:31,472:INFO:Uploading results into container
2024-09-08 02:44:31,472:INFO:Uploading model into container now
2024-09-08 02:44:31,472:INFO:_master_model_container: 9
2024-09-08 02:44:31,472:INFO:_display_container: 2
2024-09-08 02:44:31,472:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 02:44:31,472:INFO:create_model() successfully completed......................................
2024-09-08 02:44:31,608:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:31,608:INFO:Creating metrics dataframe
2024-09-08 02:44:31,611:INFO:Initializing Gradient Boosting Classifier
2024-09-08 02:44:31,611:INFO:Total runtime is 0.45956237316131593 minutes
2024-09-08 02:44:31,611:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:31,612:INFO:Initializing create_model()
2024-09-08 02:44:31,612:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:31,612:INFO:Checking exceptions
2024-09-08 02:44:31,612:INFO:Importing libraries
2024-09-08 02:44:31,612:INFO:Copying training dataset
2024-09-08 02:44:31,622:INFO:Defining folds
2024-09-08 02:44:31,622:INFO:Declaring metric variables
2024-09-08 02:44:31,622:INFO:Importing untrained model
2024-09-08 02:44:31,622:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 02:44:31,622:INFO:Starting cross validation
2024-09-08 02:44:31,628:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:34,555:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:34,607:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:34,627:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:34,802:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:34,958:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:34,982:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:34,997:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:35,033:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:36,271:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:36,305:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:36,332:INFO:Calculating mean and std
2024-09-08 02:44:36,332:INFO:Creating metrics dataframe
2024-09-08 02:44:36,332:INFO:Uploading results into container
2024-09-08 02:44:36,332:INFO:Uploading model into container now
2024-09-08 02:44:36,337:INFO:_master_model_container: 10
2024-09-08 02:44:36,337:INFO:_display_container: 2
2024-09-08 02:44:36,337:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 02:44:36,337:INFO:create_model() successfully completed......................................
2024-09-08 02:44:36,452:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:36,452:INFO:Creating metrics dataframe
2024-09-08 02:44:36,457:INFO:Initializing Linear Discriminant Analysis
2024-09-08 02:44:36,457:INFO:Total runtime is 0.5403340220451355 minutes
2024-09-08 02:44:36,457:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:36,457:INFO:Initializing create_model()
2024-09-08 02:44:36,459:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:36,459:INFO:Checking exceptions
2024-09-08 02:44:36,459:INFO:Importing libraries
2024-09-08 02:44:36,459:INFO:Copying training dataset
2024-09-08 02:44:36,467:INFO:Defining folds
2024-09-08 02:44:36,467:INFO:Declaring metric variables
2024-09-08 02:44:36,467:INFO:Importing untrained model
2024-09-08 02:44:36,467:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 02:44:36,467:INFO:Starting cross validation
2024-09-08 02:44:36,472:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:37,435:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:37,454:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:37,460:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:37,469:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:37,502:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:37,522:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:37,547:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:37,567:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:37,916:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:37,916:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:44:37,947:INFO:Calculating mean and std
2024-09-08 02:44:37,947:INFO:Creating metrics dataframe
2024-09-08 02:44:37,947:INFO:Uploading results into container
2024-09-08 02:44:37,947:INFO:Uploading model into container now
2024-09-08 02:44:37,952:INFO:_master_model_container: 11
2024-09-08 02:44:37,952:INFO:_display_container: 2
2024-09-08 02:44:37,952:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 02:44:37,952:INFO:create_model() successfully completed......................................
2024-09-08 02:44:38,064:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:38,064:INFO:Creating metrics dataframe
2024-09-08 02:44:38,064:INFO:Initializing Extra Trees Classifier
2024-09-08 02:44:38,064:INFO:Total runtime is 0.5671158353487651 minutes
2024-09-08 02:44:38,064:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:38,064:INFO:Initializing create_model()
2024-09-08 02:44:38,064:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:38,064:INFO:Checking exceptions
2024-09-08 02:44:38,072:INFO:Importing libraries
2024-09-08 02:44:38,072:INFO:Copying training dataset
2024-09-08 02:44:38,080:INFO:Defining folds
2024-09-08 02:44:38,080:INFO:Declaring metric variables
2024-09-08 02:44:38,080:INFO:Importing untrained model
2024-09-08 02:44:38,080:INFO:Extra Trees Classifier Imported successfully
2024-09-08 02:44:38,080:INFO:Starting cross validation
2024-09-08 02:44:38,085:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:40,483:INFO:Calculating mean and std
2024-09-08 02:44:40,484:INFO:Creating metrics dataframe
2024-09-08 02:44:40,487:INFO:Uploading results into container
2024-09-08 02:44:40,487:INFO:Uploading model into container now
2024-09-08 02:44:40,487:INFO:_master_model_container: 12
2024-09-08 02:44:40,488:INFO:_display_container: 2
2024-09-08 02:44:40,488:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 02:44:40,488:INFO:create_model() successfully completed......................................
2024-09-08 02:44:40,608:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:40,608:INFO:Creating metrics dataframe
2024-09-08 02:44:40,616:INFO:Initializing Extreme Gradient Boosting
2024-09-08 02:44:40,616:INFO:Total runtime is 0.6096378525098165 minutes
2024-09-08 02:44:40,616:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:40,616:INFO:Initializing create_model()
2024-09-08 02:44:40,616:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:40,616:INFO:Checking exceptions
2024-09-08 02:44:40,616:INFO:Importing libraries
2024-09-08 02:44:40,616:INFO:Copying training dataset
2024-09-08 02:44:40,623:INFO:Defining folds
2024-09-08 02:44:40,623:INFO:Declaring metric variables
2024-09-08 02:44:40,623:INFO:Importing untrained model
2024-09-08 02:44:40,623:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 02:44:40,623:INFO:Starting cross validation
2024-09-08 02:44:40,630:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:43,651:INFO:Calculating mean and std
2024-09-08 02:44:43,653:INFO:Creating metrics dataframe
2024-09-08 02:44:43,653:INFO:Uploading results into container
2024-09-08 02:44:43,653:INFO:Uploading model into container now
2024-09-08 02:44:43,653:INFO:_master_model_container: 13
2024-09-08 02:44:43,653:INFO:_display_container: 2
2024-09-08 02:44:43,653:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 02:44:43,653:INFO:create_model() successfully completed......................................
2024-09-08 02:44:43,780:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:43,780:INFO:Creating metrics dataframe
2024-09-08 02:44:43,783:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 02:44:43,783:INFO:Total runtime is 0.6624244650204977 minutes
2024-09-08 02:44:43,783:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:43,783:INFO:Initializing create_model()
2024-09-08 02:44:43,783:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:43,783:INFO:Checking exceptions
2024-09-08 02:44:43,783:INFO:Importing libraries
2024-09-08 02:44:43,783:INFO:Copying training dataset
2024-09-08 02:44:43,793:INFO:Defining folds
2024-09-08 02:44:43,793:INFO:Declaring metric variables
2024-09-08 02:44:43,793:INFO:Importing untrained model
2024-09-08 02:44:43,793:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 02:44:43,793:INFO:Starting cross validation
2024-09-08 02:44:43,796:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:50,908:INFO:Calculating mean and std
2024-09-08 02:44:50,908:INFO:Creating metrics dataframe
2024-09-08 02:44:50,913:INFO:Uploading results into container
2024-09-08 02:44:50,913:INFO:Uploading model into container now
2024-09-08 02:44:50,913:INFO:_master_model_container: 14
2024-09-08 02:44:50,913:INFO:_display_container: 2
2024-09-08 02:44:50,913:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 02:44:50,913:INFO:create_model() successfully completed......................................
2024-09-08 02:44:51,063:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:51,063:INFO:Creating metrics dataframe
2024-09-08 02:44:51,063:INFO:Initializing Dummy Classifier
2024-09-08 02:44:51,063:INFO:Total runtime is 0.7837684869766236 minutes
2024-09-08 02:44:51,063:INFO:SubProcess create_model() called ==================================
2024-09-08 02:44:51,063:INFO:Initializing create_model()
2024-09-08 02:44:51,063:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000256461BAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:51,063:INFO:Checking exceptions
2024-09-08 02:44:51,063:INFO:Importing libraries
2024-09-08 02:44:51,063:INFO:Copying training dataset
2024-09-08 02:44:51,073:INFO:Defining folds
2024-09-08 02:44:51,073:INFO:Declaring metric variables
2024-09-08 02:44:51,073:INFO:Importing untrained model
2024-09-08 02:44:51,073:INFO:Dummy Classifier Imported successfully
2024-09-08 02:44:51,073:INFO:Starting cross validation
2024-09-08 02:44:51,083:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:51,991:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:52,008:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:52,008:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:52,024:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:52,029:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:52,034:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:52,043:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:52,056:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:52,403:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:52,415:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 02:44:52,429:INFO:Calculating mean and std
2024-09-08 02:44:52,429:INFO:Creating metrics dataframe
2024-09-08 02:44:52,433:INFO:Uploading results into container
2024-09-08 02:44:52,433:INFO:Uploading model into container now
2024-09-08 02:44:52,433:INFO:_master_model_container: 15
2024-09-08 02:44:52,433:INFO:_display_container: 2
2024-09-08 02:44:52,433:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 02:44:52,433:INFO:create_model() successfully completed......................................
2024-09-08 02:44:52,548:INFO:SubProcess create_model() end ==================================
2024-09-08 02:44:52,548:INFO:Creating metrics dataframe
2024-09-08 02:44:52,554:INFO:Initializing create_model()
2024-09-08 02:44:52,554:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:52,554:INFO:Checking exceptions
2024-09-08 02:44:52,554:INFO:Importing libraries
2024-09-08 02:44:52,554:INFO:Copying training dataset
2024-09-08 02:44:52,554:INFO:Defining folds
2024-09-08 02:44:52,554:INFO:Declaring metric variables
2024-09-08 02:44:52,554:INFO:Importing untrained model
2024-09-08 02:44:52,554:INFO:Declaring custom model
2024-09-08 02:44:52,564:INFO:Random Forest Classifier Imported successfully
2024-09-08 02:44:52,566:INFO:Cross validation set to False
2024-09-08 02:44:52,566:INFO:Fitting Model
2024-09-08 02:44:52,990:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 02:44:52,990:INFO:create_model() successfully completed......................................
2024-09-08 02:44:53,106:INFO:Initializing create_model()
2024-09-08 02:44:53,106:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:53,106:INFO:Checking exceptions
2024-09-08 02:44:53,106:INFO:Importing libraries
2024-09-08 02:44:53,106:INFO:Copying training dataset
2024-09-08 02:44:53,116:INFO:Defining folds
2024-09-08 02:44:53,116:INFO:Declaring metric variables
2024-09-08 02:44:53,116:INFO:Importing untrained model
2024-09-08 02:44:53,116:INFO:Declaring custom model
2024-09-08 02:44:53,116:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 02:44:53,116:INFO:Cross validation set to False
2024-09-08 02:44:53,116:INFO:Fitting Model
2024-09-08 02:44:53,344:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 02:44:53,347:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000370 seconds.
2024-09-08 02:44:53,347:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 02:44:53,347:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 02:44:53,347:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 02:44:53,347:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 02:44:53,347:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 02:44:53,347:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 02:44:53,347:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 02:44:53,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:53,574:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 02:44:53,574:INFO:create_model() successfully completed......................................
2024-09-08 02:44:53,714:INFO:Initializing create_model()
2024-09-08 02:44:53,714:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:53,714:INFO:Checking exceptions
2024-09-08 02:44:53,715:INFO:Importing libraries
2024-09-08 02:44:53,715:INFO:Copying training dataset
2024-09-08 02:44:53,724:INFO:Defining folds
2024-09-08 02:44:53,724:INFO:Declaring metric variables
2024-09-08 02:44:53,724:INFO:Importing untrained model
2024-09-08 02:44:53,724:INFO:Declaring custom model
2024-09-08 02:44:53,724:INFO:Extra Trees Classifier Imported successfully
2024-09-08 02:44:53,729:INFO:Cross validation set to False
2024-09-08 02:44:53,729:INFO:Fitting Model
2024-09-08 02:44:54,103:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 02:44:54,103:INFO:create_model() successfully completed......................................
2024-09-08 02:44:54,235:INFO:_master_model_container: 15
2024-09-08 02:44:54,235:INFO:_display_container: 2
2024-09-08 02:44:54,236:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)]
2024-09-08 02:44:54,237:INFO:compare_models() successfully completed......................................
2024-09-08 02:44:54,237:INFO:Initializing create_model()
2024-09-08 02:44:54,237:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:44:54,237:INFO:Checking exceptions
2024-09-08 02:44:54,237:INFO:Importing libraries
2024-09-08 02:44:54,237:INFO:Copying training dataset
2024-09-08 02:44:54,244:INFO:Defining folds
2024-09-08 02:44:54,244:INFO:Declaring metric variables
2024-09-08 02:44:54,244:INFO:Importing untrained model
2024-09-08 02:44:54,244:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 02:44:54,244:INFO:Starting cross validation
2024-09-08 02:44:54,253:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:44:59,630:INFO:Calculating mean and std
2024-09-08 02:44:59,632:INFO:Creating metrics dataframe
2024-09-08 02:44:59,634:INFO:Finalizing model
2024-09-08 02:44:59,884:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 02:44:59,884:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000609 seconds.
2024-09-08 02:44:59,884:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-09-08 02:44:59,884:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 02:44:59,884:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 02:44:59,884:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 02:44:59,884:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 02:44:59,884:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 02:44:59,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:44:59,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 02:45:00,169:INFO:Uploading results into container
2024-09-08 02:45:00,169:INFO:Uploading model into container now
2024-09-08 02:45:00,184:INFO:_master_model_container: 16
2024-09-08 02:45:00,188:INFO:_display_container: 3
2024-09-08 02:45:00,189:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 02:45:00,189:INFO:create_model() successfully completed......................................
2024-09-08 02:45:00,330:INFO:Initializing create_model()
2024-09-08 02:45:00,330:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:45:00,330:INFO:Checking exceptions
2024-09-08 02:45:00,334:INFO:Importing libraries
2024-09-08 02:45:00,334:INFO:Copying training dataset
2024-09-08 02:45:00,344:INFO:Defining folds
2024-09-08 02:45:00,346:INFO:Declaring metric variables
2024-09-08 02:45:00,346:INFO:Importing untrained model
2024-09-08 02:45:00,347:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 02:45:00,347:INFO:Starting cross validation
2024-09-08 02:45:00,347:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:45:03,862:INFO:Calculating mean and std
2024-09-08 02:45:03,865:INFO:Creating metrics dataframe
2024-09-08 02:45:03,868:INFO:Finalizing model
2024-09-08 02:45:04,604:INFO:Uploading results into container
2024-09-08 02:45:04,606:INFO:Uploading model into container now
2024-09-08 02:45:04,624:INFO:_master_model_container: 17
2024-09-08 02:45:04,624:INFO:_display_container: 4
2024-09-08 02:45:04,629:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 02:45:04,630:INFO:create_model() successfully completed......................................
2024-09-08 02:45:04,784:INFO:Initializing create_model()
2024-09-08 02:45:04,784:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:45:04,784:INFO:Checking exceptions
2024-09-08 02:45:04,784:INFO:Importing libraries
2024-09-08 02:45:04,789:INFO:Copying training dataset
2024-09-08 02:45:04,796:INFO:Defining folds
2024-09-08 02:45:04,796:INFO:Declaring metric variables
2024-09-08 02:45:04,796:INFO:Importing untrained model
2024-09-08 02:45:04,796:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 02:45:04,796:INFO:Starting cross validation
2024-09-08 02:45:04,807:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:45:08,193:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:45:08,226:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:45:08,232:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:45:08,253:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:45:08,306:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:45:08,325:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:45:08,403:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:45:08,441:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:45:10,702:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:45:10,713:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 02:45:10,732:INFO:Calculating mean and std
2024-09-08 02:45:10,734:INFO:Creating metrics dataframe
2024-09-08 02:45:10,739:INFO:Finalizing model
2024-09-08 02:45:12,768:INFO:Uploading results into container
2024-09-08 02:45:12,769:INFO:Uploading model into container now
2024-09-08 02:45:12,792:INFO:_master_model_container: 18
2024-09-08 02:45:12,792:INFO:_display_container: 5
2024-09-08 02:45:12,793:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 02:45:12,793:INFO:create_model() successfully completed......................................
2024-09-08 02:45:12,930:INFO:Initializing tune_model()
2024-09-08 02:45:12,930:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>)
2024-09-08 02:45:12,930:INFO:Checking exceptions
2024-09-08 02:45:12,930:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 02:45:13,012:INFO:Copying training dataset
2024-09-08 02:45:13,021:INFO:Checking base model
2024-09-08 02:45:13,021:INFO:Base model : Extreme Gradient Boosting
2024-09-08 02:45:13,023:INFO:Declaring metric variables
2024-09-08 02:45:13,023:INFO:Defining Hyperparameters
2024-09-08 02:45:13,164:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 02:45:13,165:INFO:Tuning with n_jobs=-1
2024-09-08 02:45:13,171:INFO:Initializing skopt.BayesSearchCV
2024-09-08 02:45:28,581:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.6), ('actual_estimator__gamma', 0), ('actual_estimator__learning_rate', 0.01), ('actual_estimator__max_depth', 7), ('actual_estimator__min_child_weight', 1), ('actual_estimator__n_estimators', 100), ('actual_estimator__subsample', 0.6)])
2024-09-08 02:45:28,581:INFO:Hyperparameter search completed
2024-09-08 02:45:28,581:INFO:SubProcess create_model() called ==================================
2024-09-08 02:45:28,581:INFO:Initializing create_model()
2024-09-08 02:45:28,581:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002564619A980>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.6})
2024-09-08 02:45:28,581:INFO:Checking exceptions
2024-09-08 02:45:28,581:INFO:Importing libraries
2024-09-08 02:45:28,581:INFO:Copying training dataset
2024-09-08 02:45:28,596:INFO:Defining folds
2024-09-08 02:45:28,596:INFO:Declaring metric variables
2024-09-08 02:45:28,596:INFO:Importing untrained model
2024-09-08 02:45:28,596:INFO:Declaring custom model
2024-09-08 02:45:28,596:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 02:45:28,596:INFO:Starting cross validation
2024-09-08 02:45:28,604:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:45:30,039:INFO:Calculating mean and std
2024-09-08 02:45:30,040:INFO:Creating metrics dataframe
2024-09-08 02:45:30,040:INFO:Finalizing model
2024-09-08 02:45:30,668:INFO:Uploading results into container
2024-09-08 02:45:30,668:INFO:Uploading model into container now
2024-09-08 02:45:30,668:INFO:_master_model_container: 19
2024-09-08 02:45:30,668:INFO:_display_container: 6
2024-09-08 02:45:30,676:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 02:45:30,676:INFO:create_model() successfully completed......................................
2024-09-08 02:45:30,819:INFO:SubProcess create_model() end ==================================
2024-09-08 02:45:30,819:INFO:choose_better activated
2024-09-08 02:45:30,819:INFO:SubProcess create_model() called ==================================
2024-09-08 02:45:30,819:INFO:Initializing create_model()
2024-09-08 02:45:30,819:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:45:30,819:INFO:Checking exceptions
2024-09-08 02:45:30,819:INFO:Importing libraries
2024-09-08 02:45:30,819:INFO:Copying training dataset
2024-09-08 02:45:30,831:INFO:Defining folds
2024-09-08 02:45:30,831:INFO:Declaring metric variables
2024-09-08 02:45:30,831:INFO:Importing untrained model
2024-09-08 02:45:30,831:INFO:Declaring custom model
2024-09-08 02:45:30,831:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 02:45:30,831:INFO:Starting cross validation
2024-09-08 02:45:30,836:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 02:45:32,072:INFO:Calculating mean and std
2024-09-08 02:45:32,072:INFO:Creating metrics dataframe
2024-09-08 02:45:32,072:INFO:Finalizing model
2024-09-08 02:45:32,715:INFO:Uploading results into container
2024-09-08 02:45:32,717:INFO:Uploading model into container now
2024-09-08 02:45:32,717:INFO:_master_model_container: 20
2024-09-08 02:45:32,717:INFO:_display_container: 7
2024-09-08 02:45:32,717:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 02:45:32,717:INFO:create_model() successfully completed......................................
2024-09-08 02:45:32,866:INFO:SubProcess create_model() end ==================================
2024-09-08 02:45:32,871:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8186
2024-09-08 02:45:32,871:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8386
2024-09-08 02:45:32,871:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 02:45:32,871:INFO:choose_better completed
2024-09-08 02:45:32,876:INFO:_master_model_container: 20
2024-09-08 02:45:32,876:INFO:_display_container: 6
2024-09-08 02:45:32,888:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 02:45:32,888:INFO:tune_model() successfully completed......................................
2024-09-08 02:45:33,029:INFO:Initializing predict_model()
2024-09-08 02:45:33,029:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000025624581480>)
2024-09-08 02:45:33,029:INFO:Checking exceptions
2024-09-08 02:45:33,029:INFO:Preloading libraries
2024-09-08 02:45:33,437:INFO:Initializing get_config()
2024-09-08 02:45:33,445:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, variable=X_train)
2024-09-08 02:45:33,445:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 02:45:33,497:INFO:Variable:  returned as       id Marital status Application mode Application order  ... Curricular units 2nd sem (without evaluations) Unemployment rate Inflation rate   GDP
765  765              1                1                 6  ...                                              0         10.800000            1.4  1.74
323  323              1               17                 1  ...                                              0          7.600000            2.6  0.32
909  909              1                1                 1  ...                                              0         12.400000            0.5  1.79
774  774              1               17                 1  ...                                              0          8.900000            1.4  3.51
931  931              1               39                 1  ...                                              0         16.200001            0.3 -0.92
..   ...            ...              ...               ...  ...                                            ...               ...            ...   ...
806  806              4               39                 1  ...                                              0         10.800000            1.4  1.74
492  492              1                1                 3  ...                                              0          7.600000            2.6  0.32
698  698              1               39                 1  ...                                              1         10.800000            1.4  1.74
44    44              1                1                 1  ...                                              0          9.400000           -0.8 -3.12
604  604              1                1                 1  ...                                              0         10.800000            1.4  1.74

[700 rows x 37 columns]
2024-09-08 02:45:33,497:INFO:get_config() successfully completed......................................
2024-09-08 02:45:33,498:INFO:Initializing predict_model()
2024-09-08 02:45:33,498:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002564618DC60>)
2024-09-08 02:45:33,498:INFO:Checking exceptions
2024-09-08 02:45:33,498:INFO:Preloading libraries
2024-09-08 02:45:33,499:INFO:Set up data.
2024-09-08 02:45:33,509:INFO:Set up index.
2024-09-08 02:45:33,858:INFO:Initializing get_config()
2024-09-08 02:45:33,866:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, variable=y_train)
2024-09-08 02:45:33,866:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 02:45:33,866:INFO:Variable:  returned as 765    0
323    1
909    0
774    0
931    1
      ..
806    1
492    0
698    1
44     0
604    1
Name: Target, Length: 700, dtype: int8
2024-09-08 02:45:33,866:INFO:get_config() successfully completed......................................
2024-09-08 02:45:33,866:INFO:Initializing get_config()
2024-09-08 02:45:33,866:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, variable=y_test)
2024-09-08 02:45:33,871:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 02:45:33,877:INFO:Variable:  returned as 47     1
233    1
132    2
34     0
162    0
      ..
772    0
632    0
365    2
136    0
299    1
Name: Target, Length: 300, dtype: int8
2024-09-08 02:45:33,877:INFO:get_config() successfully completed......................................
2024-09-08 02:45:33,878:INFO:Initializing finalize_model()
2024-09-08 02:45:33,878:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 02:45:33,879:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 02:45:33,879:INFO:Initializing create_model()
2024-09-08 02:45:33,879:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 02:45:33,879:INFO:Checking exceptions
2024-09-08 02:45:33,887:INFO:Importing libraries
2024-09-08 02:45:33,887:INFO:Copying training dataset
2024-09-08 02:45:33,887:INFO:Defining folds
2024-09-08 02:45:33,887:INFO:Declaring metric variables
2024-09-08 02:45:33,887:INFO:Importing untrained model
2024-09-08 02:45:33,887:INFO:Declaring custom model
2024-09-08 02:45:33,890:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 02:45:33,896:INFO:Cross validation set to False
2024-09-08 02:45:33,896:INFO:Fitting Model
2024-09-08 02:45:35,177:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 02:45:35,177:INFO:create_model() successfully completed......................................
2024-09-08 02:45:35,322:INFO:_master_model_container: 20
2024-09-08 02:45:35,322:INFO:_display_container: 7
2024-09-08 02:45:35,514:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 02:45:35,514:INFO:finalize_model() successfully completed......................................
2024-09-08 02:45:35,959:INFO:Initializing predict_model()
2024-09-08 02:45:35,959:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000256390D7A90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000256464C0E50>)
2024-09-08 02:45:35,959:INFO:Checking exceptions
2024-09-08 02:45:35,959:INFO:Preloading libraries
2024-09-08 02:45:35,959:INFO:Set up data.
2024-09-08 02:45:35,987:INFO:Set up index.
2024-09-08 03:09:04,187:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:09:04,189:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:09:04,189:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:09:04,189:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:09:12,520:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:09:12,520:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:09:12,520:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:09:12,520:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:09:33,811:INFO:PyCaret ClassificationExperiment
2024-09-08 03:09:33,820:INFO:Logging name: clf-default-name
2024-09-08 03:09:33,822:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 03:09:33,822:INFO:version 3.3.2
2024-09-08 03:09:33,823:INFO:Initializing setup()
2024-09-08 03:09:33,823:INFO:self.USI: d04d
2024-09-08 03:09:33,824:INFO:self._variable_keys: {'seed', 'memory', 'html_param', 'target_param', 'fold_shuffle_param', 'pipeline', 'idx', 'n_jobs_param', '_available_plots', 'exp_name_log', 'fold_groups_param', 'logging_param', 'gpu_n_jobs_param', '_ml_usecase', 'X_train', 'exp_id', 'fold_generator', 'USI', 'y_train', 'is_multiclass', 'y', 'y_test', 'log_plots_param', 'fix_imbalance', 'X_test', 'gpu_param', 'X', 'data'}
2024-09-08 03:09:33,824:INFO:Checking environment
2024-09-08 03:09:33,824:INFO:python_version: 3.10.11
2024-09-08 03:09:33,825:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 03:09:33,825:INFO:machine: AMD64
2024-09-08 03:09:33,836:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 03:09:33,839:INFO:Memory: svmem(total=16407719936, available=6924599296, percent=57.8, used=9483120640, free=6924599296)
2024-09-08 03:09:33,839:INFO:Physical Core: 4
2024-09-08 03:09:33,839:INFO:Logical Core: 8
2024-09-08 03:09:33,839:INFO:Checking libraries
2024-09-08 03:09:33,839:INFO:System:
2024-09-08 03:09:33,839:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 03:09:33,841:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 03:09:33,841:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 03:09:33,841:INFO:PyCaret required dependencies:
2024-09-08 03:09:33,920:INFO:                 pip: 24.2
2024-09-08 03:09:33,920:INFO:          setuptools: 72.1.0
2024-09-08 03:09:33,920:INFO:             pycaret: 3.3.2
2024-09-08 03:09:33,920:INFO:             IPython: 8.25.0
2024-09-08 03:09:33,920:INFO:          ipywidgets: 8.1.5
2024-09-08 03:09:33,920:INFO:                tqdm: 4.66.5
2024-09-08 03:09:33,920:INFO:               numpy: 1.26.4
2024-09-08 03:09:33,920:INFO:              pandas: 2.1.4
2024-09-08 03:09:33,920:INFO:              jinja2: 3.1.4
2024-09-08 03:09:33,920:INFO:               scipy: 1.11.4
2024-09-08 03:09:33,925:INFO:              joblib: 1.3.2
2024-09-08 03:09:33,925:INFO:             sklearn: 1.4.2
2024-09-08 03:09:33,926:INFO:                pyod: 2.0.1
2024-09-08 03:09:33,926:INFO:            imblearn: 0.12.3
2024-09-08 03:09:33,926:INFO:   category_encoders: 2.6.3
2024-09-08 03:09:33,926:INFO:            lightgbm: 4.5.0
2024-09-08 03:09:33,926:INFO:               numba: 0.60.0
2024-09-08 03:09:33,927:INFO:            requests: 2.32.3
2024-09-08 03:09:33,927:INFO:          matplotlib: 3.7.5
2024-09-08 03:09:33,927:INFO:          scikitplot: 0.3.7
2024-09-08 03:09:33,927:INFO:         yellowbrick: 1.5
2024-09-08 03:09:33,928:INFO:              plotly: 5.24.0
2024-09-08 03:09:33,928:INFO:    plotly-resampler: Not installed
2024-09-08 03:09:33,928:INFO:             kaleido: 0.2.1
2024-09-08 03:09:33,928:INFO:           schemdraw: 0.15
2024-09-08 03:09:33,929:INFO:         statsmodels: 0.14.2
2024-09-08 03:09:33,929:INFO:              sktime: 0.26.0
2024-09-08 03:09:33,929:INFO:               tbats: 1.1.3
2024-09-08 03:09:33,929:INFO:            pmdarima: 2.0.4
2024-09-08 03:09:33,930:INFO:              psutil: 5.9.0
2024-09-08 03:09:33,930:INFO:          markupsafe: 2.1.3
2024-09-08 03:09:33,930:INFO:             pickle5: Not installed
2024-09-08 03:09:33,930:INFO:         cloudpickle: 3.0.0
2024-09-08 03:09:33,931:INFO:         deprecation: 2.1.0
2024-09-08 03:09:33,931:INFO:              xxhash: 3.5.0
2024-09-08 03:09:33,931:INFO:           wurlitzer: Not installed
2024-09-08 03:09:33,931:INFO:PyCaret optional dependencies:
2024-09-08 03:09:33,947:INFO:                shap: Not installed
2024-09-08 03:09:33,947:INFO:           interpret: Not installed
2024-09-08 03:09:33,948:INFO:                umap: Not installed
2024-09-08 03:09:33,948:INFO:     ydata_profiling: Not installed
2024-09-08 03:09:33,948:INFO:  explainerdashboard: Not installed
2024-09-08 03:09:33,948:INFO:             autoviz: Not installed
2024-09-08 03:09:33,948:INFO:           fairlearn: Not installed
2024-09-08 03:09:33,948:INFO:          deepchecks: Not installed
2024-09-08 03:09:33,948:INFO:             xgboost: 2.1.1
2024-09-08 03:09:33,949:INFO:            catboost: Not installed
2024-09-08 03:09:33,949:INFO:              kmodes: Not installed
2024-09-08 03:09:33,949:INFO:             mlxtend: Not installed
2024-09-08 03:09:33,949:INFO:       statsforecast: Not installed
2024-09-08 03:09:33,949:INFO:        tune_sklearn: Not installed
2024-09-08 03:09:33,949:INFO:                 ray: Not installed
2024-09-08 03:09:33,949:INFO:            hyperopt: 0.2.7
2024-09-08 03:09:33,949:INFO:              optuna: 4.0.0
2024-09-08 03:09:33,949:INFO:               skopt: 0.10.2
2024-09-08 03:09:33,949:INFO:              mlflow: Not installed
2024-09-08 03:09:33,949:INFO:              gradio: Not installed
2024-09-08 03:09:33,949:INFO:             fastapi: Not installed
2024-09-08 03:09:33,949:INFO:             uvicorn: Not installed
2024-09-08 03:09:33,949:INFO:              m2cgen: Not installed
2024-09-08 03:09:33,949:INFO:           evidently: Not installed
2024-09-08 03:09:33,949:INFO:               fugue: Not installed
2024-09-08 03:09:33,949:INFO:           streamlit: 1.38.0
2024-09-08 03:09:33,949:INFO:             prophet: Not installed
2024-09-08 03:09:33,950:INFO:None
2024-09-08 03:09:33,950:INFO:Set up data.
2024-09-08 03:09:33,967:INFO:Set up folding strategy.
2024-09-08 03:09:33,967:INFO:Set up train/test split.
2024-09-08 03:09:33,984:INFO:Set up index.
2024-09-08 03:09:33,984:INFO:Assigning column types.
2024-09-08 03:09:33,990:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 03:09:34,039:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 03:09:34,054:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 03:09:34,100:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:09:34,101:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:09:34,161:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 03:09:34,161:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 03:09:34,213:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:09:34,217:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:09:34,217:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 03:09:34,356:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 03:09:34,420:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:09:34,425:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:09:34,504:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 03:09:34,553:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:09:34,561:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:09:34,562:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 03:09:34,717:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:09:34,722:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:09:34,847:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:09:34,851:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:09:34,854:INFO:Preparing preprocessing pipeline...
2024-09-08 03:09:34,856:INFO:Set up simple imputation.
2024-09-08 03:09:34,868:INFO:Set up encoding of ordinal features.
2024-09-08 03:09:34,892:INFO:Set up encoding of categorical features.
2024-09-08 03:09:34,893:INFO:Set up column name cleaning.
2024-09-08 03:09:35,232:INFO:Finished creating preprocessing pipeline.
2024-09-08 03:09:35,510:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                                                    'qualification',
                                                                    'Fathers '
                                                                    'qualification',
                                                                    'Mothers '
                                                                    'occupation',
                                                                    'Fathers '
                                                                    'occupation'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 03:09:35,513:INFO:Creating final display dataframe.
2024-09-08 03:09:35,824:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (1000, 38)
4        Transformed data shape       (1000, 129)
5   Transformed train set shape        (700, 129)
6    Transformed test set shape        (300, 129)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              d04d
2024-09-08 03:09:35,928:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:09:35,941:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:09:36,036:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:09:36,039:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:09:36,042:INFO:setup() successfully completed in 2.26s...............
2024-09-08 03:09:36,042:INFO:Initializing compare_models()
2024-09-08 03:09:36,042:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D88AEF7A90>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002D88AEF7A90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 03:09:36,042:INFO:Checking exceptions
2024-09-08 03:09:36,044:INFO:Preparing display monitor
2024-09-08 03:09:36,044:INFO:Initializing Logistic Regression
2024-09-08 03:09:36,044:INFO:Total runtime is 0.0 minutes
2024-09-08 03:09:36,044:INFO:SubProcess create_model() called ==================================
2024-09-08 03:09:36,044:INFO:Initializing create_model()
2024-09-08 03:09:36,044:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D88AEF7A90>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D897FFAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:09:36,044:INFO:Checking exceptions
2024-09-08 03:09:36,044:INFO:Importing libraries
2024-09-08 03:09:36,044:INFO:Copying training dataset
2024-09-08 03:09:36,064:INFO:Defining folds
2024-09-08 03:09:36,064:INFO:Declaring metric variables
2024-09-08 03:09:36,064:INFO:Importing untrained model
2024-09-08 03:09:36,064:INFO:Logistic Regression Imported successfully
2024-09-08 03:09:36,064:INFO:Starting cross validation
2024-09-08 03:09:36,064:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:09:45,016:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:09:45,048:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:09:45,243:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:09:45,244:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:09:45,310:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:45,310:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:45,341:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:09:45,372:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:09:45,467:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:45,483:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:09:45,483:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:45,483:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:09:45,546:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:45,609:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:45,672:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:45,698:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:46,373:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:09:46,440:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:09:46,481:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:46,538:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:46,561:INFO:Calculating mean and std
2024-09-08 03:09:46,561:INFO:Creating metrics dataframe
2024-09-08 03:09:46,563:INFO:Uploading results into container
2024-09-08 03:09:46,564:INFO:Uploading model into container now
2024-09-08 03:09:46,564:INFO:_master_model_container: 1
2024-09-08 03:09:46,565:INFO:_display_container: 2
2024-09-08 03:09:46,565:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 03:09:46,565:INFO:create_model() successfully completed......................................
2024-09-08 03:09:46,685:INFO:SubProcess create_model() end ==================================
2024-09-08 03:09:46,685:INFO:Creating metrics dataframe
2024-09-08 03:09:46,688:INFO:Initializing K Neighbors Classifier
2024-09-08 03:09:46,688:INFO:Total runtime is 0.17741133371988932 minutes
2024-09-08 03:09:46,688:INFO:SubProcess create_model() called ==================================
2024-09-08 03:09:46,688:INFO:Initializing create_model()
2024-09-08 03:09:46,688:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D88AEF7A90>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D897FFAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:09:46,688:INFO:Checking exceptions
2024-09-08 03:09:46,688:INFO:Importing libraries
2024-09-08 03:09:46,688:INFO:Copying training dataset
2024-09-08 03:09:46,699:INFO:Defining folds
2024-09-08 03:09:46,699:INFO:Declaring metric variables
2024-09-08 03:09:46,700:INFO:Importing untrained model
2024-09-08 03:09:46,700:INFO:K Neighbors Classifier Imported successfully
2024-09-08 03:09:46,700:INFO:Starting cross validation
2024-09-08 03:09:46,704:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:09:48,511:INFO:Calculating mean and std
2024-09-08 03:09:48,513:INFO:Creating metrics dataframe
2024-09-08 03:09:48,513:INFO:Uploading results into container
2024-09-08 03:09:48,517:INFO:Uploading model into container now
2024-09-08 03:09:48,517:INFO:_master_model_container: 2
2024-09-08 03:09:48,517:INFO:_display_container: 2
2024-09-08 03:09:48,517:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 03:09:48,517:INFO:create_model() successfully completed......................................
2024-09-08 03:09:48,653:INFO:SubProcess create_model() end ==================================
2024-09-08 03:09:48,653:INFO:Creating metrics dataframe
2024-09-08 03:09:48,662:INFO:Initializing Naive Bayes
2024-09-08 03:09:48,662:INFO:Total runtime is 0.2102974812189738 minutes
2024-09-08 03:09:48,662:INFO:SubProcess create_model() called ==================================
2024-09-08 03:09:48,662:INFO:Initializing create_model()
2024-09-08 03:09:48,662:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D88AEF7A90>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D897FFAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:09:48,662:INFO:Checking exceptions
2024-09-08 03:09:48,662:INFO:Importing libraries
2024-09-08 03:09:48,662:INFO:Copying training dataset
2024-09-08 03:09:48,671:INFO:Defining folds
2024-09-08 03:09:48,671:INFO:Declaring metric variables
2024-09-08 03:09:48,671:INFO:Importing untrained model
2024-09-08 03:09:48,671:INFO:Naive Bayes Imported successfully
2024-09-08 03:09:48,671:INFO:Starting cross validation
2024-09-08 03:09:48,679:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:09:49,658:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:09:50,418:INFO:Calculating mean and std
2024-09-08 03:09:50,418:INFO:Creating metrics dataframe
2024-09-08 03:09:50,422:INFO:Uploading results into container
2024-09-08 03:09:50,423:INFO:Uploading model into container now
2024-09-08 03:09:50,423:INFO:_master_model_container: 3
2024-09-08 03:09:50,424:INFO:_display_container: 2
2024-09-08 03:09:50,424:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 03:09:50,424:INFO:create_model() successfully completed......................................
2024-09-08 03:09:50,568:INFO:SubProcess create_model() end ==================================
2024-09-08 03:09:50,568:INFO:Creating metrics dataframe
2024-09-08 03:09:50,576:INFO:Initializing Decision Tree Classifier
2024-09-08 03:09:50,576:INFO:Total runtime is 0.2422035257021586 minutes
2024-09-08 03:09:50,576:INFO:SubProcess create_model() called ==================================
2024-09-08 03:09:50,576:INFO:Initializing create_model()
2024-09-08 03:09:50,576:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D88AEF7A90>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D897FFAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:09:50,576:INFO:Checking exceptions
2024-09-08 03:09:50,576:INFO:Importing libraries
2024-09-08 03:09:50,576:INFO:Copying training dataset
2024-09-08 03:09:50,594:INFO:Defining folds
2024-09-08 03:09:50,594:INFO:Declaring metric variables
2024-09-08 03:09:50,594:INFO:Importing untrained model
2024-09-08 03:09:50,594:INFO:Decision Tree Classifier Imported successfully
2024-09-08 03:09:50,594:INFO:Starting cross validation
2024-09-08 03:09:50,599:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:09:52,160:INFO:Calculating mean and std
2024-09-08 03:09:52,160:INFO:Creating metrics dataframe
2024-09-08 03:09:52,160:INFO:Uploading results into container
2024-09-08 03:09:52,160:INFO:Uploading model into container now
2024-09-08 03:09:52,160:INFO:_master_model_container: 4
2024-09-08 03:09:52,160:INFO:_display_container: 2
2024-09-08 03:09:52,160:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 03:09:52,160:INFO:create_model() successfully completed......................................
2024-09-08 03:09:52,300:INFO:SubProcess create_model() end ==================================
2024-09-08 03:09:52,300:INFO:Creating metrics dataframe
2024-09-08 03:09:52,303:INFO:Initializing SVM - Linear Kernel
2024-09-08 03:09:52,303:INFO:Total runtime is 0.2709904472033183 minutes
2024-09-08 03:09:52,303:INFO:SubProcess create_model() called ==================================
2024-09-08 03:09:52,304:INFO:Initializing create_model()
2024-09-08 03:09:52,304:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D88AEF7A90>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D897FFAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:09:52,304:INFO:Checking exceptions
2024-09-08 03:09:52,304:INFO:Importing libraries
2024-09-08 03:09:52,304:INFO:Copying training dataset
2024-09-08 03:09:52,313:INFO:Defining folds
2024-09-08 03:09:52,313:INFO:Declaring metric variables
2024-09-08 03:09:52,315:INFO:Importing untrained model
2024-09-08 03:09:52,315:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 03:09:52,315:INFO:Starting cross validation
2024-09-08 03:09:52,315:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:09:53,272:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:53,278:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:09:53,287:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:09:53,287:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:53,287:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:53,303:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:53,303:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:53,319:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:09:53,319:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:53,714:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:53,714:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:53,745:INFO:Calculating mean and std
2024-09-08 03:09:53,745:INFO:Creating metrics dataframe
2024-09-08 03:09:53,745:INFO:Uploading results into container
2024-09-08 03:09:53,745:INFO:Uploading model into container now
2024-09-08 03:09:53,745:INFO:_master_model_container: 5
2024-09-08 03:09:53,745:INFO:_display_container: 2
2024-09-08 03:09:53,745:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 03:09:53,745:INFO:create_model() successfully completed......................................
2024-09-08 03:09:53,862:INFO:SubProcess create_model() end ==================================
2024-09-08 03:09:53,862:INFO:Creating metrics dataframe
2024-09-08 03:09:53,862:INFO:Initializing Ridge Classifier
2024-09-08 03:09:53,862:INFO:Total runtime is 0.2969660719235738 minutes
2024-09-08 03:09:53,862:INFO:SubProcess create_model() called ==================================
2024-09-08 03:09:53,862:INFO:Initializing create_model()
2024-09-08 03:09:53,862:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D88AEF7A90>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D897FFAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:09:53,862:INFO:Checking exceptions
2024-09-08 03:09:53,862:INFO:Importing libraries
2024-09-08 03:09:53,862:INFO:Copying training dataset
2024-09-08 03:09:53,862:INFO:Defining folds
2024-09-08 03:09:53,862:INFO:Declaring metric variables
2024-09-08 03:09:53,877:INFO:Importing untrained model
2024-09-08 03:09:53,877:INFO:Ridge Classifier Imported successfully
2024-09-08 03:09:53,877:INFO:Starting cross validation
2024-09-08 03:09:53,877:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:09:54,740:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:54,753:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:54,762:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:54,768:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:54,768:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:54,800:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:54,800:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:54,800:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:55,147:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:55,147:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:55,178:INFO:Calculating mean and std
2024-09-08 03:09:55,178:INFO:Creating metrics dataframe
2024-09-08 03:09:55,178:INFO:Uploading results into container
2024-09-08 03:09:55,178:INFO:Uploading model into container now
2024-09-08 03:09:55,178:INFO:_master_model_container: 6
2024-09-08 03:09:55,178:INFO:_display_container: 2
2024-09-08 03:09:55,178:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 03:09:55,178:INFO:create_model() successfully completed......................................
2024-09-08 03:09:55,299:INFO:SubProcess create_model() end ==================================
2024-09-08 03:09:55,300:INFO:Creating metrics dataframe
2024-09-08 03:09:55,303:INFO:Initializing Random Forest Classifier
2024-09-08 03:09:55,303:INFO:Total runtime is 0.32099328041076663 minutes
2024-09-08 03:09:55,303:INFO:SubProcess create_model() called ==================================
2024-09-08 03:09:55,303:INFO:Initializing create_model()
2024-09-08 03:09:55,303:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D88AEF7A90>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D897FFAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:09:55,303:INFO:Checking exceptions
2024-09-08 03:09:55,304:INFO:Importing libraries
2024-09-08 03:09:55,304:INFO:Copying training dataset
2024-09-08 03:09:55,313:INFO:Defining folds
2024-09-08 03:09:55,314:INFO:Declaring metric variables
2024-09-08 03:09:55,314:INFO:Importing untrained model
2024-09-08 03:09:55,314:INFO:Random Forest Classifier Imported successfully
2024-09-08 03:09:55,314:INFO:Starting cross validation
2024-09-08 03:09:55,314:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:09:57,768:INFO:Calculating mean and std
2024-09-08 03:09:57,770:INFO:Creating metrics dataframe
2024-09-08 03:09:57,770:INFO:Uploading results into container
2024-09-08 03:09:57,770:INFO:Uploading model into container now
2024-09-08 03:09:57,770:INFO:_master_model_container: 7
2024-09-08 03:09:57,770:INFO:_display_container: 2
2024-09-08 03:09:57,770:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 03:09:57,770:INFO:create_model() successfully completed......................................
2024-09-08 03:09:57,888:INFO:SubProcess create_model() end ==================================
2024-09-08 03:09:57,888:INFO:Creating metrics dataframe
2024-09-08 03:09:57,890:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 03:09:57,891:INFO:Total runtime is 0.36411807139714564 minutes
2024-09-08 03:09:57,891:INFO:SubProcess create_model() called ==================================
2024-09-08 03:09:57,891:INFO:Initializing create_model()
2024-09-08 03:09:57,891:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D88AEF7A90>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D897FFAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:09:57,891:INFO:Checking exceptions
2024-09-08 03:09:57,891:INFO:Importing libraries
2024-09-08 03:09:57,891:INFO:Copying training dataset
2024-09-08 03:09:57,896:INFO:Defining folds
2024-09-08 03:09:57,896:INFO:Declaring metric variables
2024-09-08 03:09:57,896:INFO:Importing untrained model
2024-09-08 03:09:57,896:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 03:09:57,896:INFO:Starting cross validation
2024-09-08 03:09:57,896:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:09:58,555:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:09:58,555:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:09:58,586:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:09:58,586:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:09:58,602:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:09:58,602:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:09:58,633:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:09:58,633:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:09:58,775:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:58,791:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:58,811:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:58,822:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:58,838:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:58,838:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:58,858:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:58,874:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:59,123:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:09:59,123:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:09:59,217:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:59,217:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:09:59,249:INFO:Calculating mean and std
2024-09-08 03:09:59,249:INFO:Creating metrics dataframe
2024-09-08 03:09:59,249:INFO:Uploading results into container
2024-09-08 03:09:59,249:INFO:Uploading model into container now
2024-09-08 03:09:59,249:INFO:_master_model_container: 8
2024-09-08 03:09:59,249:INFO:_display_container: 2
2024-09-08 03:09:59,249:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 03:09:59,249:INFO:create_model() successfully completed......................................
2024-09-08 03:09:59,369:INFO:SubProcess create_model() end ==================================
2024-09-08 03:09:59,369:INFO:Creating metrics dataframe
2024-09-08 03:09:59,372:INFO:Initializing Ada Boost Classifier
2024-09-08 03:09:59,372:INFO:Total runtime is 0.38881086508433027 minutes
2024-09-08 03:09:59,372:INFO:SubProcess create_model() called ==================================
2024-09-08 03:09:59,373:INFO:Initializing create_model()
2024-09-08 03:09:59,373:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D88AEF7A90>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D897FFAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:09:59,373:INFO:Checking exceptions
2024-09-08 03:09:59,373:INFO:Importing libraries
2024-09-08 03:09:59,373:INFO:Copying training dataset
2024-09-08 03:09:59,379:INFO:Defining folds
2024-09-08 03:09:59,379:INFO:Declaring metric variables
2024-09-08 03:09:59,379:INFO:Importing untrained model
2024-09-08 03:09:59,379:INFO:Ada Boost Classifier Imported successfully
2024-09-08 03:09:59,379:INFO:Starting cross validation
2024-09-08 03:09:59,379:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:10:00,030:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:10:00,049:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:10:00,093:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:10:00,093:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:10:00,124:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:10:00,140:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:10:00,140:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:10:00,639:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:00,667:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:00,707:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:00,740:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:00,766:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:00,766:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:00,782:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:00,782:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:01,047:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:10:01,052:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:10:01,445:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:01,452:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:01,479:INFO:Calculating mean and std
2024-09-08 03:10:01,488:INFO:Creating metrics dataframe
2024-09-08 03:10:01,488:INFO:Uploading results into container
2024-09-08 03:10:01,488:INFO:Uploading model into container now
2024-09-08 03:10:01,488:INFO:_master_model_container: 9
2024-09-08 03:10:01,488:INFO:_display_container: 2
2024-09-08 03:10:01,488:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 03:10:01,488:INFO:create_model() successfully completed......................................
2024-09-08 03:10:01,610:INFO:SubProcess create_model() end ==================================
2024-09-08 03:10:01,610:INFO:Creating metrics dataframe
2024-09-08 03:10:01,610:INFO:Initializing Gradient Boosting Classifier
2024-09-08 03:10:01,610:INFO:Total runtime is 0.42609966595967613 minutes
2024-09-08 03:10:01,610:INFO:SubProcess create_model() called ==================================
2024-09-08 03:10:01,625:INFO:Initializing create_model()
2024-09-08 03:10:01,625:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D88AEF7A90>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D897FFAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:10:01,625:INFO:Checking exceptions
2024-09-08 03:10:01,625:INFO:Importing libraries
2024-09-08 03:10:01,625:INFO:Copying training dataset
2024-09-08 03:10:01,625:INFO:Defining folds
2024-09-08 03:10:01,625:INFO:Declaring metric variables
2024-09-08 03:10:01,625:INFO:Importing untrained model
2024-09-08 03:10:01,625:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 03:10:01,625:INFO:Starting cross validation
2024-09-08 03:10:01,625:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:10:04,517:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:04,564:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:04,585:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:04,605:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:04,762:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:04,777:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:04,825:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:10:04,858:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:12:18,128:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:12:18,141:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:12:18,141:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:12:18,141:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:12:28,853:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:12:28,854:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:12:28,854:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:12:28,855:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:13:19,128:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:13:19,128:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:13:19,128:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:13:19,128:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:13:28,354:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:13:28,354:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:13:28,354:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:13:28,354:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 03:13:46,481:INFO:PyCaret ClassificationExperiment
2024-09-08 03:13:46,481:INFO:Logging name: clf-default-name
2024-09-08 03:13:46,481:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 03:13:46,481:INFO:version 3.3.2
2024-09-08 03:13:46,481:INFO:Initializing setup()
2024-09-08 03:13:46,481:INFO:self.USI: 6b5b
2024-09-08 03:13:46,481:INFO:self._variable_keys: {'exp_id', 'y_train', 'gpu_param', 'log_plots_param', 'logging_param', 'y_test', 'y', 'target_param', '_ml_usecase', 'fold_generator', '_available_plots', 'fix_imbalance', 'fold_groups_param', 'is_multiclass', 'idx', 'USI', 'fold_shuffle_param', 'pipeline', 'exp_name_log', 'seed', 'data', 'X', 'n_jobs_param', 'X_train', 'gpu_n_jobs_param', 'html_param', 'memory', 'X_test'}
2024-09-08 03:13:46,481:INFO:Checking environment
2024-09-08 03:13:46,481:INFO:python_version: 3.10.11
2024-09-08 03:13:46,481:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 03:13:46,481:INFO:machine: AMD64
2024-09-08 03:13:46,498:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 03:13:46,498:INFO:Memory: svmem(total=16407719936, available=7087333376, percent=56.8, used=9320386560, free=7087333376)
2024-09-08 03:13:46,498:INFO:Physical Core: 4
2024-09-08 03:13:46,498:INFO:Logical Core: 8
2024-09-08 03:13:46,498:INFO:Checking libraries
2024-09-08 03:13:46,498:INFO:System:
2024-09-08 03:13:46,498:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 03:13:46,498:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 03:13:46,498:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 03:13:46,498:INFO:PyCaret required dependencies:
2024-09-08 03:13:46,579:INFO:                 pip: 24.2
2024-09-08 03:13:46,579:INFO:          setuptools: 72.1.0
2024-09-08 03:13:46,579:INFO:             pycaret: 3.3.2
2024-09-08 03:13:46,579:INFO:             IPython: 8.25.0
2024-09-08 03:13:46,579:INFO:          ipywidgets: 8.1.5
2024-09-08 03:13:46,579:INFO:                tqdm: 4.66.5
2024-09-08 03:13:46,579:INFO:               numpy: 1.26.4
2024-09-08 03:13:46,579:INFO:              pandas: 2.1.4
2024-09-08 03:13:46,579:INFO:              jinja2: 3.1.4
2024-09-08 03:13:46,579:INFO:               scipy: 1.11.4
2024-09-08 03:13:46,579:INFO:              joblib: 1.3.2
2024-09-08 03:13:46,579:INFO:             sklearn: 1.4.2
2024-09-08 03:13:46,579:INFO:                pyod: 2.0.1
2024-09-08 03:13:46,579:INFO:            imblearn: 0.12.3
2024-09-08 03:13:46,579:INFO:   category_encoders: 2.6.3
2024-09-08 03:13:46,579:INFO:            lightgbm: 4.5.0
2024-09-08 03:13:46,579:INFO:               numba: 0.60.0
2024-09-08 03:13:46,579:INFO:            requests: 2.32.3
2024-09-08 03:13:46,579:INFO:          matplotlib: 3.7.5
2024-09-08 03:13:46,579:INFO:          scikitplot: 0.3.7
2024-09-08 03:13:46,579:INFO:         yellowbrick: 1.5
2024-09-08 03:13:46,579:INFO:              plotly: 5.24.0
2024-09-08 03:13:46,579:INFO:    plotly-resampler: Not installed
2024-09-08 03:13:46,579:INFO:             kaleido: 0.2.1
2024-09-08 03:13:46,579:INFO:           schemdraw: 0.15
2024-09-08 03:13:46,579:INFO:         statsmodels: 0.14.2
2024-09-08 03:13:46,579:INFO:              sktime: 0.26.0
2024-09-08 03:13:46,579:INFO:               tbats: 1.1.3
2024-09-08 03:13:46,579:INFO:            pmdarima: 2.0.4
2024-09-08 03:13:46,579:INFO:              psutil: 5.9.0
2024-09-08 03:13:46,579:INFO:          markupsafe: 2.1.3
2024-09-08 03:13:46,579:INFO:             pickle5: Not installed
2024-09-08 03:13:46,579:INFO:         cloudpickle: 3.0.0
2024-09-08 03:13:46,579:INFO:         deprecation: 2.1.0
2024-09-08 03:13:46,579:INFO:              xxhash: 3.5.0
2024-09-08 03:13:46,579:INFO:           wurlitzer: Not installed
2024-09-08 03:13:46,579:INFO:PyCaret optional dependencies:
2024-09-08 03:13:46,594:INFO:                shap: Not installed
2024-09-08 03:13:46,594:INFO:           interpret: Not installed
2024-09-08 03:13:46,594:INFO:                umap: Not installed
2024-09-08 03:13:46,594:INFO:     ydata_profiling: Not installed
2024-09-08 03:13:46,594:INFO:  explainerdashboard: Not installed
2024-09-08 03:13:46,594:INFO:             autoviz: Not installed
2024-09-08 03:13:46,594:INFO:           fairlearn: Not installed
2024-09-08 03:13:46,594:INFO:          deepchecks: Not installed
2024-09-08 03:13:46,594:INFO:             xgboost: 2.1.1
2024-09-08 03:13:46,594:INFO:            catboost: Not installed
2024-09-08 03:13:46,594:INFO:              kmodes: Not installed
2024-09-08 03:13:46,594:INFO:             mlxtend: Not installed
2024-09-08 03:13:46,594:INFO:       statsforecast: Not installed
2024-09-08 03:13:46,594:INFO:        tune_sklearn: Not installed
2024-09-08 03:13:46,594:INFO:                 ray: Not installed
2024-09-08 03:13:46,594:INFO:            hyperopt: 0.2.7
2024-09-08 03:13:46,594:INFO:              optuna: 4.0.0
2024-09-08 03:13:46,594:INFO:               skopt: 0.10.2
2024-09-08 03:13:46,594:INFO:              mlflow: Not installed
2024-09-08 03:13:46,594:INFO:              gradio: Not installed
2024-09-08 03:13:46,594:INFO:             fastapi: Not installed
2024-09-08 03:13:46,594:INFO:             uvicorn: Not installed
2024-09-08 03:13:46,594:INFO:              m2cgen: Not installed
2024-09-08 03:13:46,594:INFO:           evidently: Not installed
2024-09-08 03:13:46,594:INFO:               fugue: Not installed
2024-09-08 03:13:46,594:INFO:           streamlit: 1.38.0
2024-09-08 03:13:46,594:INFO:             prophet: Not installed
2024-09-08 03:13:46,594:INFO:None
2024-09-08 03:13:46,594:INFO:Set up data.
2024-09-08 03:13:46,609:INFO:Set up folding strategy.
2024-09-08 03:13:46,609:INFO:Set up train/test split.
2024-09-08 03:13:46,609:INFO:Set up index.
2024-09-08 03:13:46,609:INFO:Assigning column types.
2024-09-08 03:13:46,629:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 03:13:46,672:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 03:13:46,672:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 03:13:46,706:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:13:46,720:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:13:46,774:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 03:13:46,774:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 03:13:46,805:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:13:46,805:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:13:46,805:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 03:13:46,858:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 03:13:46,884:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:13:46,884:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:13:46,946:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 03:13:46,980:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:13:46,980:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:13:46,980:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 03:13:47,060:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:13:47,060:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:13:47,158:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:13:47,160:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:13:47,160:INFO:Preparing preprocessing pipeline...
2024-09-08 03:13:47,160:INFO:Set up simple imputation.
2024-09-08 03:13:47,160:INFO:Set up encoding of ordinal features.
2024-09-08 03:13:47,173:INFO:Set up encoding of categorical features.
2024-09-08 03:13:47,189:INFO:Set up column name cleaning.
2024-09-08 03:13:47,439:INFO:Finished creating preprocessing pipeline.
2024-09-08 03:13:47,658:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                                                    'qualification',
                                                                    'Fathers '
                                                                    'qualification',
                                                                    'Mothers '
                                                                    'occupation',
                                                                    'Fathers '
                                                                    'occupation'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 03:13:47,658:INFO:Creating final display dataframe.
2024-09-08 03:13:47,902:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (1000, 38)
4        Transformed data shape       (1000, 129)
5   Transformed train set shape        (700, 129)
6    Transformed test set shape        (300, 129)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              6b5b
2024-09-08 03:13:48,016:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:13:48,016:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:13:48,112:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 03:13:48,116:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 03:13:48,117:INFO:setup() successfully completed in 1.65s...............
2024-09-08 03:13:48,117:INFO:Initializing compare_models()
2024-09-08 03:13:48,117:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 03:13:48,117:INFO:Checking exceptions
2024-09-08 03:13:48,123:INFO:Preparing display monitor
2024-09-08 03:13:48,126:INFO:Initializing Logistic Regression
2024-09-08 03:13:48,126:INFO:Total runtime is 0.0 minutes
2024-09-08 03:13:48,126:INFO:SubProcess create_model() called ==================================
2024-09-08 03:13:48,129:INFO:Initializing create_model()
2024-09-08 03:13:48,129:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:13:48,129:INFO:Checking exceptions
2024-09-08 03:13:48,129:INFO:Importing libraries
2024-09-08 03:13:48,129:INFO:Copying training dataset
2024-09-08 03:13:48,129:INFO:Defining folds
2024-09-08 03:13:48,129:INFO:Declaring metric variables
2024-09-08 03:13:48,129:INFO:Importing untrained model
2024-09-08 03:13:48,129:INFO:Logistic Regression Imported successfully
2024-09-08 03:13:48,129:INFO:Starting cross validation
2024-09-08 03:13:48,143:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:13:56,935:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:13:56,998:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:13:56,998:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:13:57,014:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:13:57,188:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:13:57,188:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:13:57,226:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:13:57,226:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:13:57,247:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:13:57,259:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:13:57,272:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:13:57,285:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:13:57,421:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:13:57,452:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:13:57,452:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:13:57,466:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:13:58,189:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:13:58,203:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 03:13:58,276:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:13:58,286:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:13:58,309:INFO:Calculating mean and std
2024-09-08 03:13:58,311:INFO:Creating metrics dataframe
2024-09-08 03:13:58,311:INFO:Uploading results into container
2024-09-08 03:13:58,311:INFO:Uploading model into container now
2024-09-08 03:13:58,311:INFO:_master_model_container: 1
2024-09-08 03:13:58,311:INFO:_display_container: 2
2024-09-08 03:13:58,311:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 03:13:58,311:INFO:create_model() successfully completed......................................
2024-09-08 03:13:58,426:INFO:SubProcess create_model() end ==================================
2024-09-08 03:13:58,426:INFO:Creating metrics dataframe
2024-09-08 03:13:58,426:INFO:Initializing K Neighbors Classifier
2024-09-08 03:13:58,426:INFO:Total runtime is 0.17165238062540691 minutes
2024-09-08 03:13:58,426:INFO:SubProcess create_model() called ==================================
2024-09-08 03:13:58,426:INFO:Initializing create_model()
2024-09-08 03:13:58,426:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:13:58,426:INFO:Checking exceptions
2024-09-08 03:13:58,426:INFO:Importing libraries
2024-09-08 03:13:58,426:INFO:Copying training dataset
2024-09-08 03:13:58,441:INFO:Defining folds
2024-09-08 03:13:58,441:INFO:Declaring metric variables
2024-09-08 03:13:58,441:INFO:Importing untrained model
2024-09-08 03:13:58,441:INFO:K Neighbors Classifier Imported successfully
2024-09-08 03:13:58,441:INFO:Starting cross validation
2024-09-08 03:13:58,441:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:13:59,687:INFO:Calculating mean and std
2024-09-08 03:13:59,687:INFO:Creating metrics dataframe
2024-09-08 03:13:59,687:INFO:Uploading results into container
2024-09-08 03:13:59,687:INFO:Uploading model into container now
2024-09-08 03:13:59,687:INFO:_master_model_container: 2
2024-09-08 03:13:59,687:INFO:_display_container: 2
2024-09-08 03:13:59,687:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 03:13:59,687:INFO:create_model() successfully completed......................................
2024-09-08 03:13:59,805:INFO:SubProcess create_model() end ==================================
2024-09-08 03:13:59,805:INFO:Creating metrics dataframe
2024-09-08 03:13:59,807:INFO:Initializing Naive Bayes
2024-09-08 03:13:59,808:INFO:Total runtime is 0.1947014411290487 minutes
2024-09-08 03:13:59,808:INFO:SubProcess create_model() called ==================================
2024-09-08 03:13:59,808:INFO:Initializing create_model()
2024-09-08 03:13:59,808:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:13:59,809:INFO:Checking exceptions
2024-09-08 03:13:59,809:INFO:Importing libraries
2024-09-08 03:13:59,809:INFO:Copying training dataset
2024-09-08 03:13:59,811:INFO:Defining folds
2024-09-08 03:13:59,811:INFO:Declaring metric variables
2024-09-08 03:13:59,811:INFO:Importing untrained model
2024-09-08 03:13:59,811:INFO:Naive Bayes Imported successfully
2024-09-08 03:13:59,811:INFO:Starting cross validation
2024-09-08 03:13:59,811:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:00,536:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:00,957:INFO:Calculating mean and std
2024-09-08 03:14:00,957:INFO:Creating metrics dataframe
2024-09-08 03:14:00,957:INFO:Uploading results into container
2024-09-08 03:14:00,957:INFO:Uploading model into container now
2024-09-08 03:14:00,957:INFO:_master_model_container: 3
2024-09-08 03:14:00,957:INFO:_display_container: 2
2024-09-08 03:14:00,957:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 03:14:00,957:INFO:create_model() successfully completed......................................
2024-09-08 03:14:01,075:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:01,075:INFO:Creating metrics dataframe
2024-09-08 03:14:01,075:INFO:Initializing Decision Tree Classifier
2024-09-08 03:14:01,075:INFO:Total runtime is 0.2158182541529338 minutes
2024-09-08 03:14:01,075:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:01,075:INFO:Initializing create_model()
2024-09-08 03:14:01,075:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:01,075:INFO:Checking exceptions
2024-09-08 03:14:01,075:INFO:Importing libraries
2024-09-08 03:14:01,075:INFO:Copying training dataset
2024-09-08 03:14:01,082:INFO:Defining folds
2024-09-08 03:14:01,082:INFO:Declaring metric variables
2024-09-08 03:14:01,082:INFO:Importing untrained model
2024-09-08 03:14:01,082:INFO:Decision Tree Classifier Imported successfully
2024-09-08 03:14:01,082:INFO:Starting cross validation
2024-09-08 03:14:01,092:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:02,198:INFO:Calculating mean and std
2024-09-08 03:14:02,198:INFO:Creating metrics dataframe
2024-09-08 03:14:02,201:INFO:Uploading results into container
2024-09-08 03:14:02,201:INFO:Uploading model into container now
2024-09-08 03:14:02,201:INFO:_master_model_container: 4
2024-09-08 03:14:02,201:INFO:_display_container: 2
2024-09-08 03:14:02,201:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 03:14:02,201:INFO:create_model() successfully completed......................................
2024-09-08 03:14:02,317:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:02,317:INFO:Creating metrics dataframe
2024-09-08 03:14:02,320:INFO:Initializing SVM - Linear Kernel
2024-09-08 03:14:02,320:INFO:Total runtime is 0.23655339876810713 minutes
2024-09-08 03:14:02,320:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:02,320:INFO:Initializing create_model()
2024-09-08 03:14:02,321:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:02,321:INFO:Checking exceptions
2024-09-08 03:14:02,321:INFO:Importing libraries
2024-09-08 03:14:02,321:INFO:Copying training dataset
2024-09-08 03:14:02,329:INFO:Defining folds
2024-09-08 03:14:02,329:INFO:Declaring metric variables
2024-09-08 03:14:02,329:INFO:Importing untrained model
2024-09-08 03:14:02,330:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 03:14:02,331:INFO:Starting cross validation
2024-09-08 03:14:02,331:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:03,060:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:03,076:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:03,092:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:03,108:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:03,108:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:03,108:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:03,123:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:03,124:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:03,129:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:03,155:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:03,455:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:03,455:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:03,487:INFO:Calculating mean and std
2024-09-08 03:14:03,487:INFO:Creating metrics dataframe
2024-09-08 03:14:03,487:INFO:Uploading results into container
2024-09-08 03:14:03,487:INFO:Uploading model into container now
2024-09-08 03:14:03,487:INFO:_master_model_container: 5
2024-09-08 03:14:03,487:INFO:_display_container: 2
2024-09-08 03:14:03,487:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 03:14:03,487:INFO:create_model() successfully completed......................................
2024-09-08 03:14:03,642:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:03,642:INFO:Creating metrics dataframe
2024-09-08 03:14:03,642:INFO:Initializing Ridge Classifier
2024-09-08 03:14:03,642:INFO:Total runtime is 0.2585893869400025 minutes
2024-09-08 03:14:03,642:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:03,642:INFO:Initializing create_model()
2024-09-08 03:14:03,642:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:03,642:INFO:Checking exceptions
2024-09-08 03:14:03,642:INFO:Importing libraries
2024-09-08 03:14:03,642:INFO:Copying training dataset
2024-09-08 03:14:03,642:INFO:Defining folds
2024-09-08 03:14:03,642:INFO:Declaring metric variables
2024-09-08 03:14:03,642:INFO:Importing untrained model
2024-09-08 03:14:03,642:INFO:Ridge Classifier Imported successfully
2024-09-08 03:14:03,642:INFO:Starting cross validation
2024-09-08 03:14:03,657:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:04,375:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:04,375:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:04,391:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:04,391:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:04,391:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:04,422:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:04,454:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:04,454:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:04,824:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:04,826:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:04,840:INFO:Calculating mean and std
2024-09-08 03:14:04,840:INFO:Creating metrics dataframe
2024-09-08 03:14:04,840:INFO:Uploading results into container
2024-09-08 03:14:04,840:INFO:Uploading model into container now
2024-09-08 03:14:04,840:INFO:_master_model_container: 6
2024-09-08 03:14:04,840:INFO:_display_container: 2
2024-09-08 03:14:04,840:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 03:14:04,840:INFO:create_model() successfully completed......................................
2024-09-08 03:14:04,975:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:04,975:INFO:Creating metrics dataframe
2024-09-08 03:14:04,975:INFO:Initializing Random Forest Classifier
2024-09-08 03:14:04,975:INFO:Total runtime is 0.280803406238556 minutes
2024-09-08 03:14:04,975:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:04,975:INFO:Initializing create_model()
2024-09-08 03:14:04,975:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:04,975:INFO:Checking exceptions
2024-09-08 03:14:04,975:INFO:Importing libraries
2024-09-08 03:14:04,975:INFO:Copying training dataset
2024-09-08 03:14:04,987:INFO:Defining folds
2024-09-08 03:14:04,987:INFO:Declaring metric variables
2024-09-08 03:14:04,987:INFO:Importing untrained model
2024-09-08 03:14:04,987:INFO:Random Forest Classifier Imported successfully
2024-09-08 03:14:04,987:INFO:Starting cross validation
2024-09-08 03:14:04,991:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:07,221:INFO:Calculating mean and std
2024-09-08 03:14:07,221:INFO:Creating metrics dataframe
2024-09-08 03:14:07,221:INFO:Uploading results into container
2024-09-08 03:14:07,221:INFO:Uploading model into container now
2024-09-08 03:14:07,221:INFO:_master_model_container: 7
2024-09-08 03:14:07,221:INFO:_display_container: 2
2024-09-08 03:14:07,221:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 03:14:07,221:INFO:create_model() successfully completed......................................
2024-09-08 03:14:07,344:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:07,344:INFO:Creating metrics dataframe
2024-09-08 03:14:07,344:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 03:14:07,344:INFO:Total runtime is 0.3203003366788229 minutes
2024-09-08 03:14:07,344:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:07,344:INFO:Initializing create_model()
2024-09-08 03:14:07,344:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:07,344:INFO:Checking exceptions
2024-09-08 03:14:07,344:INFO:Importing libraries
2024-09-08 03:14:07,344:INFO:Copying training dataset
2024-09-08 03:14:07,360:INFO:Defining folds
2024-09-08 03:14:07,360:INFO:Declaring metric variables
2024-09-08 03:14:07,361:INFO:Importing untrained model
2024-09-08 03:14:07,361:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 03:14:07,362:INFO:Starting cross validation
2024-09-08 03:14:07,362:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:07,892:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:14:07,920:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:14:07,920:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:14:07,936:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:14:07,953:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:14:07,967:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:14:07,992:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:14:07,992:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:14:08,077:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:08,093:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:08,110:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:08,110:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:08,144:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:08,166:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:08,172:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:08,173:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:08,455:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:14:08,455:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 03:14:08,534:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:08,534:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:08,565:INFO:Calculating mean and std
2024-09-08 03:14:08,565:INFO:Creating metrics dataframe
2024-09-08 03:14:08,565:INFO:Uploading results into container
2024-09-08 03:14:08,565:INFO:Uploading model into container now
2024-09-08 03:14:08,565:INFO:_master_model_container: 8
2024-09-08 03:14:08,565:INFO:_display_container: 2
2024-09-08 03:14:08,565:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 03:14:08,565:INFO:create_model() successfully completed......................................
2024-09-08 03:14:08,686:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:08,686:INFO:Creating metrics dataframe
2024-09-08 03:14:08,689:INFO:Initializing Ada Boost Classifier
2024-09-08 03:14:08,689:INFO:Total runtime is 0.3427077492078146 minutes
2024-09-08 03:14:08,689:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:08,689:INFO:Initializing create_model()
2024-09-08 03:14:08,689:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:08,689:INFO:Checking exceptions
2024-09-08 03:14:08,689:INFO:Importing libraries
2024-09-08 03:14:08,689:INFO:Copying training dataset
2024-09-08 03:14:08,698:INFO:Defining folds
2024-09-08 03:14:08,698:INFO:Declaring metric variables
2024-09-08 03:14:08,698:INFO:Importing untrained model
2024-09-08 03:14:08,698:INFO:Ada Boost Classifier Imported successfully
2024-09-08 03:14:08,699:INFO:Starting cross validation
2024-09-08 03:14:08,700:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:09,242:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:14:09,243:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:14:09,244:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:14:09,260:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:14:09,260:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:14:09,260:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:14:09,275:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:14:09,275:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:14:09,695:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:09,709:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:09,725:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:09,725:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:09,742:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:09,744:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:09,744:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:09,760:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:09,990:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:14:09,990:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 03:14:10,211:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:10,211:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:10,243:INFO:Calculating mean and std
2024-09-08 03:14:10,244:INFO:Creating metrics dataframe
2024-09-08 03:14:10,244:INFO:Uploading results into container
2024-09-08 03:14:10,244:INFO:Uploading model into container now
2024-09-08 03:14:10,244:INFO:_master_model_container: 9
2024-09-08 03:14:10,244:INFO:_display_container: 2
2024-09-08 03:14:10,244:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 03:14:10,244:INFO:create_model() successfully completed......................................
2024-09-08 03:14:10,365:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:10,365:INFO:Creating metrics dataframe
2024-09-08 03:14:10,368:INFO:Initializing Gradient Boosting Classifier
2024-09-08 03:14:10,368:INFO:Total runtime is 0.3706858634948731 minutes
2024-09-08 03:14:10,368:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:10,368:INFO:Initializing create_model()
2024-09-08 03:14:10,369:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:10,369:INFO:Checking exceptions
2024-09-08 03:14:10,369:INFO:Importing libraries
2024-09-08 03:14:10,369:INFO:Copying training dataset
2024-09-08 03:14:10,377:INFO:Defining folds
2024-09-08 03:14:10,378:INFO:Declaring metric variables
2024-09-08 03:14:10,378:INFO:Importing untrained model
2024-09-08 03:14:10,378:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 03:14:10,379:INFO:Starting cross validation
2024-09-08 03:14:10,382:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:12,375:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:12,408:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:12,422:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:12,422:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:12,469:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:12,469:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:12,502:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:12,517:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:13,615:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:13,633:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:13,661:INFO:Calculating mean and std
2024-09-08 03:14:13,663:INFO:Creating metrics dataframe
2024-09-08 03:14:13,665:INFO:Uploading results into container
2024-09-08 03:14:13,665:INFO:Uploading model into container now
2024-09-08 03:14:13,665:INFO:_master_model_container: 10
2024-09-08 03:14:13,665:INFO:_display_container: 2
2024-09-08 03:14:13,665:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 03:14:13,665:INFO:create_model() successfully completed......................................
2024-09-08 03:14:13,783:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:13,783:INFO:Creating metrics dataframe
2024-09-08 03:14:13,786:INFO:Initializing Linear Discriminant Analysis
2024-09-08 03:14:13,786:INFO:Total runtime is 0.42765787045160936 minutes
2024-09-08 03:14:13,786:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:13,786:INFO:Initializing create_model()
2024-09-08 03:14:13,787:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:13,787:INFO:Checking exceptions
2024-09-08 03:14:13,787:INFO:Importing libraries
2024-09-08 03:14:13,787:INFO:Copying training dataset
2024-09-08 03:14:13,794:INFO:Defining folds
2024-09-08 03:14:13,795:INFO:Declaring metric variables
2024-09-08 03:14:13,795:INFO:Importing untrained model
2024-09-08 03:14:13,795:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 03:14:13,795:INFO:Starting cross validation
2024-09-08 03:14:13,796:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:14,502:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:14,516:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:14,532:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:14,532:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:14,532:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:14,547:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:14,563:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:14,596:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:14,875:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:14,890:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:14,906:INFO:Calculating mean and std
2024-09-08 03:14:14,906:INFO:Creating metrics dataframe
2024-09-08 03:14:14,909:INFO:Uploading results into container
2024-09-08 03:14:14,909:INFO:Uploading model into container now
2024-09-08 03:14:14,909:INFO:_master_model_container: 11
2024-09-08 03:14:14,909:INFO:_display_container: 2
2024-09-08 03:14:14,909:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 03:14:14,909:INFO:create_model() successfully completed......................................
2024-09-08 03:14:15,024:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:15,024:INFO:Creating metrics dataframe
2024-09-08 03:14:15,027:INFO:Initializing Extra Trees Classifier
2024-09-08 03:14:15,027:INFO:Total runtime is 0.4483493566513062 minutes
2024-09-08 03:14:15,027:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:15,027:INFO:Initializing create_model()
2024-09-08 03:14:15,027:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:15,027:INFO:Checking exceptions
2024-09-08 03:14:15,027:INFO:Importing libraries
2024-09-08 03:14:15,028:INFO:Copying training dataset
2024-09-08 03:14:15,029:INFO:Defining folds
2024-09-08 03:14:15,029:INFO:Declaring metric variables
2024-09-08 03:14:15,029:INFO:Importing untrained model
2024-09-08 03:14:15,029:INFO:Extra Trees Classifier Imported successfully
2024-09-08 03:14:15,029:INFO:Starting cross validation
2024-09-08 03:14:15,029:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:16,843:INFO:Calculating mean and std
2024-09-08 03:14:16,843:INFO:Creating metrics dataframe
2024-09-08 03:14:16,847:INFO:Uploading results into container
2024-09-08 03:14:16,847:INFO:Uploading model into container now
2024-09-08 03:14:16,847:INFO:_master_model_container: 12
2024-09-08 03:14:16,847:INFO:_display_container: 2
2024-09-08 03:14:16,847:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 03:14:16,847:INFO:create_model() successfully completed......................................
2024-09-08 03:14:16,961:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:16,961:INFO:Creating metrics dataframe
2024-09-08 03:14:16,961:INFO:Initializing Extreme Gradient Boosting
2024-09-08 03:14:16,961:INFO:Total runtime is 0.4805818915367127 minutes
2024-09-08 03:14:16,961:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:16,961:INFO:Initializing create_model()
2024-09-08 03:14:16,961:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:16,961:INFO:Checking exceptions
2024-09-08 03:14:16,961:INFO:Importing libraries
2024-09-08 03:14:16,961:INFO:Copying training dataset
2024-09-08 03:14:16,961:INFO:Defining folds
2024-09-08 03:14:16,961:INFO:Declaring metric variables
2024-09-08 03:14:16,961:INFO:Importing untrained model
2024-09-08 03:14:16,977:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 03:14:16,977:INFO:Starting cross validation
2024-09-08 03:14:16,977:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:19,006:INFO:Calculating mean and std
2024-09-08 03:14:19,007:INFO:Creating metrics dataframe
2024-09-08 03:14:19,007:INFO:Uploading results into container
2024-09-08 03:14:19,007:INFO:Uploading model into container now
2024-09-08 03:14:19,007:INFO:_master_model_container: 13
2024-09-08 03:14:19,007:INFO:_display_container: 2
2024-09-08 03:14:19,007:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 03:14:19,007:INFO:create_model() successfully completed......................................
2024-09-08 03:14:19,126:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:19,126:INFO:Creating metrics dataframe
2024-09-08 03:14:19,129:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 03:14:19,129:INFO:Total runtime is 0.5167179544766745 minutes
2024-09-08 03:14:19,129:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:19,129:INFO:Initializing create_model()
2024-09-08 03:14:19,129:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:19,129:INFO:Checking exceptions
2024-09-08 03:14:19,129:INFO:Importing libraries
2024-09-08 03:14:19,129:INFO:Copying training dataset
2024-09-08 03:14:19,129:INFO:Defining folds
2024-09-08 03:14:19,129:INFO:Declaring metric variables
2024-09-08 03:14:19,129:INFO:Importing untrained model
2024-09-08 03:14:19,129:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 03:14:19,129:INFO:Starting cross validation
2024-09-08 03:14:19,129:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:24,560:INFO:Calculating mean and std
2024-09-08 03:14:24,560:INFO:Creating metrics dataframe
2024-09-08 03:14:24,560:INFO:Uploading results into container
2024-09-08 03:14:24,560:INFO:Uploading model into container now
2024-09-08 03:14:24,560:INFO:_master_model_container: 14
2024-09-08 03:14:24,560:INFO:_display_container: 2
2024-09-08 03:14:24,560:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 03:14:24,560:INFO:create_model() successfully completed......................................
2024-09-08 03:14:24,698:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:24,698:INFO:Creating metrics dataframe
2024-09-08 03:14:24,698:INFO:Initializing Dummy Classifier
2024-09-08 03:14:24,698:INFO:Total runtime is 0.6095335046450298 minutes
2024-09-08 03:14:24,698:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:24,698:INFO:Initializing create_model()
2024-09-08 03:14:24,698:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEBDACB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:24,698:INFO:Checking exceptions
2024-09-08 03:14:24,698:INFO:Importing libraries
2024-09-08 03:14:24,698:INFO:Copying training dataset
2024-09-08 03:14:24,713:INFO:Defining folds
2024-09-08 03:14:24,713:INFO:Declaring metric variables
2024-09-08 03:14:24,713:INFO:Importing untrained model
2024-09-08 03:14:24,713:INFO:Dummy Classifier Imported successfully
2024-09-08 03:14:24,713:INFO:Starting cross validation
2024-09-08 03:14:24,713:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:25,439:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:25,455:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:25,471:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:25,485:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:25,485:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:25,485:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:25,485:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:25,517:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:25,807:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:25,815:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 03:14:25,828:INFO:Calculating mean and std
2024-09-08 03:14:25,828:INFO:Creating metrics dataframe
2024-09-08 03:14:25,828:INFO:Uploading results into container
2024-09-08 03:14:25,828:INFO:Uploading model into container now
2024-09-08 03:14:25,828:INFO:_master_model_container: 15
2024-09-08 03:14:25,828:INFO:_display_container: 2
2024-09-08 03:14:25,828:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 03:14:25,828:INFO:create_model() successfully completed......................................
2024-09-08 03:14:25,945:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:25,945:INFO:Creating metrics dataframe
2024-09-08 03:14:25,945:INFO:Initializing create_model()
2024-09-08 03:14:25,945:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:25,945:INFO:Checking exceptions
2024-09-08 03:14:25,945:INFO:Importing libraries
2024-09-08 03:14:25,945:INFO:Copying training dataset
2024-09-08 03:14:25,960:INFO:Defining folds
2024-09-08 03:14:25,960:INFO:Declaring metric variables
2024-09-08 03:14:25,960:INFO:Importing untrained model
2024-09-08 03:14:25,960:INFO:Declaring custom model
2024-09-08 03:14:25,960:INFO:Random Forest Classifier Imported successfully
2024-09-08 03:14:25,960:INFO:Cross validation set to False
2024-09-08 03:14:25,960:INFO:Fitting Model
2024-09-08 03:14:26,364:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 03:14:26,364:INFO:create_model() successfully completed......................................
2024-09-08 03:14:26,471:INFO:Initializing create_model()
2024-09-08 03:14:26,471:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:26,471:INFO:Checking exceptions
2024-09-08 03:14:26,471:INFO:Importing libraries
2024-09-08 03:14:26,471:INFO:Copying training dataset
2024-09-08 03:14:26,471:INFO:Defining folds
2024-09-08 03:14:26,487:INFO:Declaring metric variables
2024-09-08 03:14:26,487:INFO:Importing untrained model
2024-09-08 03:14:26,487:INFO:Declaring custom model
2024-09-08 03:14:26,487:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 03:14:26,489:INFO:Cross validation set to False
2024-09-08 03:14:26,489:INFO:Fitting Model
2024-09-08 03:14:26,676:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 03:14:26,676:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000543 seconds.
2024-09-08 03:14:26,676:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-09-08 03:14:26,676:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 03:14:26,692:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 03:14:26,692:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 03:14:26,692:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 03:14:26,692:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:26,926:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 03:14:26,926:INFO:create_model() successfully completed......................................
2024-09-08 03:14:27,052:INFO:Initializing create_model()
2024-09-08 03:14:27,052:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:27,052:INFO:Checking exceptions
2024-09-08 03:14:27,052:INFO:Importing libraries
2024-09-08 03:14:27,052:INFO:Copying training dataset
2024-09-08 03:14:27,052:INFO:Defining folds
2024-09-08 03:14:27,052:INFO:Declaring metric variables
2024-09-08 03:14:27,052:INFO:Importing untrained model
2024-09-08 03:14:27,052:INFO:Declaring custom model
2024-09-08 03:14:27,068:INFO:Extra Trees Classifier Imported successfully
2024-09-08 03:14:27,070:INFO:Cross validation set to False
2024-09-08 03:14:27,070:INFO:Fitting Model
2024-09-08 03:14:27,415:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 03:14:27,420:INFO:create_model() successfully completed......................................
2024-09-08 03:14:27,538:INFO:_master_model_container: 15
2024-09-08 03:14:27,538:INFO:_display_container: 2
2024-09-08 03:14:27,547:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)]
2024-09-08 03:14:27,547:INFO:compare_models() successfully completed......................................
2024-09-08 03:14:27,547:INFO:Initializing create_model()
2024-09-08 03:14:27,547:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:27,547:INFO:Checking exceptions
2024-09-08 03:14:27,547:INFO:Importing libraries
2024-09-08 03:14:27,547:INFO:Copying training dataset
2024-09-08 03:14:27,560:INFO:Defining folds
2024-09-08 03:14:27,561:INFO:Declaring metric variables
2024-09-08 03:14:27,561:INFO:Importing untrained model
2024-09-08 03:14:27,562:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 03:14:27,562:INFO:Starting cross validation
2024-09-08 03:14:27,565:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:32,144:INFO:Calculating mean and std
2024-09-08 03:14:32,144:INFO:Creating metrics dataframe
2024-09-08 03:14:32,144:INFO:Finalizing model
2024-09-08 03:14:32,360:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 03:14:32,360:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000561 seconds.
2024-09-08 03:14:32,360:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-09-08 03:14:32,360:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 03:14:32,360:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 03:14:32,360:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 03:14:32,360:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 03:14:32,360:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 03:14:32,532:INFO:Uploading results into container
2024-09-08 03:14:32,532:INFO:Uploading model into container now
2024-09-08 03:14:32,548:INFO:_master_model_container: 16
2024-09-08 03:14:32,548:INFO:_display_container: 3
2024-09-08 03:14:32,548:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 03:14:32,548:INFO:create_model() successfully completed......................................
2024-09-08 03:14:32,681:INFO:Initializing create_model()
2024-09-08 03:14:32,681:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:32,681:INFO:Checking exceptions
2024-09-08 03:14:32,681:INFO:Importing libraries
2024-09-08 03:14:32,681:INFO:Copying training dataset
2024-09-08 03:14:32,702:INFO:Defining folds
2024-09-08 03:14:32,702:INFO:Declaring metric variables
2024-09-08 03:14:32,702:INFO:Importing untrained model
2024-09-08 03:14:32,703:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 03:14:32,703:INFO:Starting cross validation
2024-09-08 03:14:32,707:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:34,894:INFO:Calculating mean and std
2024-09-08 03:14:34,894:INFO:Creating metrics dataframe
2024-09-08 03:14:34,894:INFO:Finalizing model
2024-09-08 03:14:35,409:INFO:Uploading results into container
2024-09-08 03:14:35,409:INFO:Uploading model into container now
2024-09-08 03:14:35,431:INFO:_master_model_container: 17
2024-09-08 03:14:35,431:INFO:_display_container: 4
2024-09-08 03:14:35,433:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 03:14:35,433:INFO:create_model() successfully completed......................................
2024-09-08 03:14:35,562:INFO:Initializing create_model()
2024-09-08 03:14:35,562:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:35,562:INFO:Checking exceptions
2024-09-08 03:14:35,562:INFO:Importing libraries
2024-09-08 03:14:35,562:INFO:Copying training dataset
2024-09-08 03:14:35,578:INFO:Defining folds
2024-09-08 03:14:35,578:INFO:Declaring metric variables
2024-09-08 03:14:35,578:INFO:Importing untrained model
2024-09-08 03:14:35,578:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 03:14:35,578:INFO:Starting cross validation
2024-09-08 03:14:35,578:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:37,631:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:37,673:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:37,673:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:37,673:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:37,878:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:37,957:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:37,957:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:37,957:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:39,099:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:39,118:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 03:14:39,145:INFO:Calculating mean and std
2024-09-08 03:14:39,146:INFO:Creating metrics dataframe
2024-09-08 03:14:39,146:INFO:Finalizing model
2024-09-08 03:14:40,347:INFO:Uploading results into container
2024-09-08 03:14:40,347:INFO:Uploading model into container now
2024-09-08 03:14:40,347:INFO:_master_model_container: 18
2024-09-08 03:14:40,347:INFO:_display_container: 5
2024-09-08 03:14:40,347:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 03:14:40,347:INFO:create_model() successfully completed......................................
2024-09-08 03:14:40,475:INFO:Initializing tune_model()
2024-09-08 03:14:40,476:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>)
2024-09-08 03:14:40,476:INFO:Checking exceptions
2024-09-08 03:14:40,476:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 03:14:40,515:INFO:Copying training dataset
2024-09-08 03:14:40,523:INFO:Checking base model
2024-09-08 03:14:40,523:INFO:Base model : Extreme Gradient Boosting
2024-09-08 03:14:40,523:INFO:Declaring metric variables
2024-09-08 03:14:40,523:INFO:Defining Hyperparameters
2024-09-08 03:14:40,650:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 03:14:40,650:INFO:Tuning with n_jobs=-1
2024-09-08 03:14:40,654:INFO:Initializing skopt.BayesSearchCV
2024-09-08 03:14:49,938:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.6), ('actual_estimator__gamma', 0), ('actual_estimator__learning_rate', 0.01), ('actual_estimator__max_depth', 7), ('actual_estimator__min_child_weight', 1), ('actual_estimator__n_estimators', 100), ('actual_estimator__subsample', 0.6)])
2024-09-08 03:14:49,941:INFO:Hyperparameter search completed
2024-09-08 03:14:49,941:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:49,942:INFO:Initializing create_model()
2024-09-08 03:14:49,942:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024BCEF34D60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.6})
2024-09-08 03:14:49,943:INFO:Checking exceptions
2024-09-08 03:14:49,943:INFO:Importing libraries
2024-09-08 03:14:49,943:INFO:Copying training dataset
2024-09-08 03:14:49,948:INFO:Defining folds
2024-09-08 03:14:49,948:INFO:Declaring metric variables
2024-09-08 03:14:49,948:INFO:Importing untrained model
2024-09-08 03:14:49,948:INFO:Declaring custom model
2024-09-08 03:14:49,948:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 03:14:49,948:INFO:Starting cross validation
2024-09-08 03:14:49,957:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:51,083:INFO:Calculating mean and std
2024-09-08 03:14:51,083:INFO:Creating metrics dataframe
2024-09-08 03:14:51,083:INFO:Finalizing model
2024-09-08 03:14:51,583:INFO:Uploading results into container
2024-09-08 03:14:51,583:INFO:Uploading model into container now
2024-09-08 03:14:51,583:INFO:_master_model_container: 19
2024-09-08 03:14:51,583:INFO:_display_container: 6
2024-09-08 03:14:51,583:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 03:14:51,583:INFO:create_model() successfully completed......................................
2024-09-08 03:14:51,731:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:51,731:INFO:choose_better activated
2024-09-08 03:14:51,731:INFO:SubProcess create_model() called ==================================
2024-09-08 03:14:51,733:INFO:Initializing create_model()
2024-09-08 03:14:51,734:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:51,734:INFO:Checking exceptions
2024-09-08 03:14:51,734:INFO:Importing libraries
2024-09-08 03:14:51,734:INFO:Copying training dataset
2024-09-08 03:14:51,734:INFO:Defining folds
2024-09-08 03:14:51,734:INFO:Declaring metric variables
2024-09-08 03:14:51,734:INFO:Importing untrained model
2024-09-08 03:14:51,734:INFO:Declaring custom model
2024-09-08 03:14:51,734:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 03:14:51,734:INFO:Starting cross validation
2024-09-08 03:14:51,749:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 03:14:52,777:INFO:Calculating mean and std
2024-09-08 03:14:52,777:INFO:Creating metrics dataframe
2024-09-08 03:14:52,777:INFO:Finalizing model
2024-09-08 03:14:53,302:INFO:Uploading results into container
2024-09-08 03:14:53,303:INFO:Uploading model into container now
2024-09-08 03:14:53,304:INFO:_master_model_container: 20
2024-09-08 03:14:53,304:INFO:_display_container: 7
2024-09-08 03:14:53,305:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 03:14:53,305:INFO:create_model() successfully completed......................................
2024-09-08 03:14:53,447:INFO:SubProcess create_model() end ==================================
2024-09-08 03:14:53,447:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8186
2024-09-08 03:14:53,447:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8386
2024-09-08 03:14:53,451:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 03:14:53,451:INFO:choose_better completed
2024-09-08 03:14:53,463:INFO:_master_model_container: 20
2024-09-08 03:14:53,463:INFO:_display_container: 6
2024-09-08 03:14:53,464:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 03:14:53,464:INFO:tune_model() successfully completed......................................
2024-09-08 03:14:53,583:INFO:Initializing predict_model()
2024-09-08 03:14:53,583:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000024BB81013F0>)
2024-09-08 03:14:53,583:INFO:Checking exceptions
2024-09-08 03:14:53,583:INFO:Preloading libraries
2024-09-08 03:14:53,967:INFO:Initializing get_config()
2024-09-08 03:14:53,967:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, variable=X_train)
2024-09-08 03:14:53,967:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 03:14:54,022:INFO:Variable:  returned as       id Marital status Application mode Application order  ... Curricular units 2nd sem (without evaluations) Unemployment rate Inflation rate   GDP
765  765              1                1                 6  ...                                              0         10.800000            1.4  1.74
323  323              1               17                 1  ...                                              0          7.600000            2.6  0.32
909  909              1                1                 1  ...                                              0         12.400000            0.5  1.79
774  774              1               17                 1  ...                                              0          8.900000            1.4  3.51
931  931              1               39                 1  ...                                              0         16.200001            0.3 -0.92
..   ...            ...              ...               ...  ...                                            ...               ...            ...   ...
806  806              4               39                 1  ...                                              0         10.800000            1.4  1.74
492  492              1                1                 3  ...                                              0          7.600000            2.6  0.32
698  698              1               39                 1  ...                                              1         10.800000            1.4  1.74
44    44              1                1                 1  ...                                              0          9.400000           -0.8 -3.12
604  604              1                1                 1  ...                                              0         10.800000            1.4  1.74

[700 rows x 37 columns]
2024-09-08 03:14:54,022:INFO:get_config() successfully completed......................................
2024-09-08 03:14:54,022:INFO:Initializing predict_model()
2024-09-08 03:14:54,022:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000024BB81013F0>)
2024-09-08 03:14:54,022:INFO:Checking exceptions
2024-09-08 03:14:54,022:INFO:Preloading libraries
2024-09-08 03:14:54,022:INFO:Set up data.
2024-09-08 03:14:54,032:INFO:Set up index.
2024-09-08 03:14:54,297:INFO:Initializing get_config()
2024-09-08 03:14:54,297:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, variable=y_train)
2024-09-08 03:14:54,298:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 03:14:54,301:INFO:Variable:  returned as 765    0
323    1
909    0
774    0
931    1
      ..
806    1
492    0
698    1
44     0
604    1
Name: Target, Length: 700, dtype: int8
2024-09-08 03:14:54,301:INFO:get_config() successfully completed......................................
2024-09-08 03:14:54,301:INFO:Initializing get_config()
2024-09-08 03:14:54,301:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, variable=y_test)
2024-09-08 03:14:54,302:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 03:14:54,305:INFO:Variable:  returned as 47     1
233    1
132    2
34     0
162    0
      ..
772    0
632    0
365    2
136    0
299    1
Name: Target, Length: 300, dtype: int8
2024-09-08 03:14:54,305:INFO:get_config() successfully completed......................................
2024-09-08 03:14:54,306:INFO:Initializing finalize_model()
2024-09-08 03:14:54,306:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 03:14:54,307:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 03:14:54,312:INFO:Initializing create_model()
2024-09-08 03:14:54,312:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 03:14:54,312:INFO:Checking exceptions
2024-09-08 03:14:54,313:INFO:Importing libraries
2024-09-08 03:14:54,313:INFO:Copying training dataset
2024-09-08 03:14:54,314:INFO:Defining folds
2024-09-08 03:14:54,314:INFO:Declaring metric variables
2024-09-08 03:14:54,314:INFO:Importing untrained model
2024-09-08 03:14:54,314:INFO:Declaring custom model
2024-09-08 03:14:54,316:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 03:14:54,317:INFO:Cross validation set to False
2024-09-08 03:14:54,317:INFO:Fitting Model
2024-09-08 03:14:55,195:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 03:14:55,195:INFO:create_model() successfully completed......................................
2024-09-08 03:14:55,313:INFO:_master_model_container: 20
2024-09-08 03:14:55,313:INFO:_display_container: 7
2024-09-08 03:14:55,457:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 03:14:55,457:INFO:finalize_model() successfully completed......................................
2024-09-08 03:14:55,746:INFO:Initializing predict_model()
2024-09-08 03:14:55,746:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024BCCC33F40>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000024BCEFE5480>)
2024-09-08 03:14:55,746:INFO:Checking exceptions
2024-09-08 03:14:55,746:INFO:Preloading libraries
2024-09-08 03:14:55,746:INFO:Set up data.
2024-09-08 03:14:55,760:INFO:Set up index.
2024-09-08 09:18:04,368:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:18:04,383:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:18:04,383:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:18:04,383:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:23:52,589:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:23:52,592:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:23:52,592:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:23:52,592:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:24:20,598:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:24:20,598:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:24:20,614:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:24:20,614:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:44:48,805:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:44:48,805:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:44:48,805:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:44:48,805:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:47:05,134:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:47:05,150:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:47:05,150:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:47:05,150:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:48:21,472:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:48:21,495:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:48:21,495:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:48:21,495:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:53:18,320:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:53:18,320:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:53:18,320:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:53:18,320:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:59:10,412:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:59:10,427:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:59:10,427:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 09:59:10,427:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:03:57,377:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:03:57,377:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:03:57,377:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:03:57,377:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:05:07,659:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:05:07,677:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:05:07,677:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:05:07,677:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:29:42,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:29:42,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:29:42,397:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:29:42,406:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:30:27,892:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:30:27,892:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:30:27,892:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:30:27,892:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:30:56,021:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:30:56,026:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:30:56,026:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 10:30:56,026:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:23:53,153:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:23:53,167:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:23:53,167:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:23:53,169:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:24:05,262:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:24:05,275:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:24:05,275:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:24:05,275:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:24:38,680:INFO:PyCaret ClassificationExperiment
2024-09-08 11:24:38,680:INFO:Logging name: clf-default-name
2024-09-08 11:24:38,685:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 11:24:38,685:INFO:version 3.3.2
2024-09-08 11:24:38,685:INFO:Initializing setup()
2024-09-08 11:24:38,685:INFO:self.USI: 13db
2024-09-08 11:24:38,685:INFO:self._variable_keys: {'gpu_n_jobs_param', '_available_plots', 'is_multiclass', 'y', 'idx', 'X_train', 'fold_shuffle_param', 'fix_imbalance', 'n_jobs_param', 'html_param', 'y_test', 'X_test', 'seed', 'y_train', 'X', '_ml_usecase', 'USI', 'memory', 'log_plots_param', 'data', 'fold_generator', 'exp_id', 'target_param', 'pipeline', 'fold_groups_param', 'gpu_param', 'exp_name_log', 'logging_param'}
2024-09-08 11:24:38,685:INFO:Checking environment
2024-09-08 11:24:38,685:INFO:python_version: 3.10.11
2024-09-08 11:24:38,688:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 11:24:38,688:INFO:machine: AMD64
2024-09-08 11:24:38,705:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 11:24:38,705:INFO:Memory: svmem(total=16407719936, available=6541680640, percent=60.1, used=9866039296, free=6541680640)
2024-09-08 11:24:38,705:INFO:Physical Core: 4
2024-09-08 11:24:38,705:INFO:Logical Core: 8
2024-09-08 11:24:38,705:INFO:Checking libraries
2024-09-08 11:24:38,705:INFO:System:
2024-09-08 11:24:38,705:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 11:24:38,705:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 11:24:38,705:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 11:24:38,705:INFO:PyCaret required dependencies:
2024-09-08 11:24:38,820:INFO:                 pip: 24.2
2024-09-08 11:24:38,820:INFO:          setuptools: 72.1.0
2024-09-08 11:24:38,825:INFO:             pycaret: 3.3.2
2024-09-08 11:24:38,825:INFO:             IPython: 8.25.0
2024-09-08 11:24:38,825:INFO:          ipywidgets: 8.1.5
2024-09-08 11:24:38,825:INFO:                tqdm: 4.66.5
2024-09-08 11:24:38,825:INFO:               numpy: 1.26.4
2024-09-08 11:24:38,825:INFO:              pandas: 2.1.4
2024-09-08 11:24:38,828:INFO:              jinja2: 3.1.4
2024-09-08 11:24:38,828:INFO:               scipy: 1.11.4
2024-09-08 11:24:38,828:INFO:              joblib: 1.3.2
2024-09-08 11:24:38,828:INFO:             sklearn: 1.4.2
2024-09-08 11:24:38,828:INFO:                pyod: 2.0.1
2024-09-08 11:24:38,828:INFO:            imblearn: 0.12.3
2024-09-08 11:24:38,828:INFO:   category_encoders: 2.6.3
2024-09-08 11:24:38,828:INFO:            lightgbm: 4.5.0
2024-09-08 11:24:38,828:INFO:               numba: 0.60.0
2024-09-08 11:24:38,828:INFO:            requests: 2.32.3
2024-09-08 11:24:38,828:INFO:          matplotlib: 3.7.5
2024-09-08 11:24:38,828:INFO:          scikitplot: 0.3.7
2024-09-08 11:24:38,828:INFO:         yellowbrick: 1.5
2024-09-08 11:24:38,828:INFO:              plotly: 5.24.0
2024-09-08 11:24:38,828:INFO:    plotly-resampler: Not installed
2024-09-08 11:24:38,828:INFO:             kaleido: 0.2.1
2024-09-08 11:24:38,828:INFO:           schemdraw: 0.15
2024-09-08 11:24:38,828:INFO:         statsmodels: 0.14.2
2024-09-08 11:24:38,828:INFO:              sktime: 0.26.0
2024-09-08 11:24:38,835:INFO:               tbats: 1.1.3
2024-09-08 11:24:38,835:INFO:            pmdarima: 2.0.4
2024-09-08 11:24:38,835:INFO:              psutil: 5.9.0
2024-09-08 11:24:38,835:INFO:          markupsafe: 2.1.3
2024-09-08 11:24:38,836:INFO:             pickle5: Not installed
2024-09-08 11:24:38,836:INFO:         cloudpickle: 3.0.0
2024-09-08 11:24:38,836:INFO:         deprecation: 2.1.0
2024-09-08 11:24:38,836:INFO:              xxhash: 3.5.0
2024-09-08 11:24:38,836:INFO:           wurlitzer: Not installed
2024-09-08 11:24:38,836:INFO:PyCaret optional dependencies:
2024-09-08 11:24:38,862:INFO:                shap: Not installed
2024-09-08 11:24:38,862:INFO:           interpret: Not installed
2024-09-08 11:24:38,862:INFO:                umap: Not installed
2024-09-08 11:24:38,862:INFO:     ydata_profiling: Not installed
2024-09-08 11:24:38,862:INFO:  explainerdashboard: Not installed
2024-09-08 11:24:38,862:INFO:             autoviz: Not installed
2024-09-08 11:24:38,865:INFO:           fairlearn: Not installed
2024-09-08 11:24:38,865:INFO:          deepchecks: Not installed
2024-09-08 11:24:38,865:INFO:             xgboost: 2.1.1
2024-09-08 11:24:38,865:INFO:            catboost: Not installed
2024-09-08 11:24:38,865:INFO:              kmodes: Not installed
2024-09-08 11:24:38,865:INFO:             mlxtend: Not installed
2024-09-08 11:24:38,865:INFO:       statsforecast: Not installed
2024-09-08 11:24:38,865:INFO:        tune_sklearn: Not installed
2024-09-08 11:24:38,865:INFO:                 ray: Not installed
2024-09-08 11:24:38,865:INFO:            hyperopt: 0.2.7
2024-09-08 11:24:38,865:INFO:              optuna: 4.0.0
2024-09-08 11:24:38,865:INFO:               skopt: 0.10.2
2024-09-08 11:24:38,865:INFO:              mlflow: Not installed
2024-09-08 11:24:38,865:INFO:              gradio: Not installed
2024-09-08 11:24:38,865:INFO:             fastapi: Not installed
2024-09-08 11:24:38,865:INFO:             uvicorn: Not installed
2024-09-08 11:24:38,865:INFO:              m2cgen: Not installed
2024-09-08 11:24:38,865:INFO:           evidently: Not installed
2024-09-08 11:24:38,865:INFO:               fugue: Not installed
2024-09-08 11:24:38,865:INFO:           streamlit: 1.38.0
2024-09-08 11:24:38,865:INFO:             prophet: Not installed
2024-09-08 11:24:38,865:INFO:None
2024-09-08 11:24:38,865:INFO:Set up data.
2024-09-08 11:24:38,886:INFO:Set up folding strategy.
2024-09-08 11:24:38,886:INFO:Set up train/test split.
2024-09-08 11:24:38,935:INFO:Set up index.
2024-09-08 11:24:38,935:INFO:Assigning column types.
2024-09-08 11:24:38,950:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 11:24:39,035:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 11:24:39,045:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:24:39,125:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:24:39,132:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:24:39,223:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 11:24:39,225:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:24:39,285:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:24:39,289:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:24:39,289:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 11:24:39,387:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:24:39,445:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:24:39,451:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:24:39,545:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:24:39,600:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:24:39,608:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:24:39,608:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 11:24:39,765:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:24:39,775:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:24:39,928:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:24:39,935:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:24:39,950:INFO:Preparing preprocessing pipeline...
2024-09-08 11:24:39,955:INFO:Set up simple imputation.
2024-09-08 11:24:39,968:INFO:Set up encoding of ordinal features.
2024-09-08 11:24:40,010:INFO:Set up encoding of categorical features.
2024-09-08 11:24:40,010:INFO:Set up column name cleaning.
2024-09-08 11:24:40,498:INFO:Finished creating preprocessing pipeline.
2024-09-08 11:24:40,875:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                                                    'qualification',
                                                                    'Fathers '
                                                                    'qualification',
                                                                    'Mothers '
                                                                    'occupation',
                                                                    'Fathers '
                                                                    'occupation'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 11:24:40,875:INFO:Creating final display dataframe.
2024-09-08 11:24:41,385:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (1000, 38)
4        Transformed data shape       (1000, 129)
5   Transformed train set shape        (700, 129)
6    Transformed test set shape        (300, 129)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              13db
2024-09-08 11:24:41,558:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:24:41,565:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:24:41,705:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:24:41,711:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:24:41,715:INFO:setup() successfully completed in 3.12s...............
2024-09-08 11:24:41,715:INFO:Initializing compare_models()
2024-09-08 11:24:41,715:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 11:24:41,715:INFO:Checking exceptions
2024-09-08 11:24:41,736:INFO:Preparing display monitor
2024-09-08 11:24:41,745:INFO:Initializing Logistic Regression
2024-09-08 11:24:41,745:INFO:Total runtime is 0.0 minutes
2024-09-08 11:24:41,745:INFO:SubProcess create_model() called ==================================
2024-09-08 11:24:41,745:INFO:Initializing create_model()
2024-09-08 11:24:41,745:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:24:41,745:INFO:Checking exceptions
2024-09-08 11:24:41,745:INFO:Importing libraries
2024-09-08 11:24:41,745:INFO:Copying training dataset
2024-09-08 11:24:41,762:INFO:Defining folds
2024-09-08 11:24:41,762:INFO:Declaring metric variables
2024-09-08 11:24:41,762:INFO:Importing untrained model
2024-09-08 11:24:41,762:INFO:Logistic Regression Imported successfully
2024-09-08 11:24:41,762:INFO:Starting cross validation
2024-09-08 11:24:41,768:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:24:56,905:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:24:56,939:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:24:57,496:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:24:57,581:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:24:57,581:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:24:57,821:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:24:57,891:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:24:58,247:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:24:58,412:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:24:58,739:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:24:58,767:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:24:58,971:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:24:59,037:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:24:59,078:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:24:59,207:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:24:59,270:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:00,087:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:25:00,118:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:25:00,277:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:00,298:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:00,341:INFO:Calculating mean and std
2024-09-08 11:25:00,352:INFO:Creating metrics dataframe
2024-09-08 11:25:00,352:INFO:Uploading results into container
2024-09-08 11:25:00,357:INFO:Uploading model into container now
2024-09-08 11:25:00,357:INFO:_master_model_container: 1
2024-09-08 11:25:00,357:INFO:_display_container: 2
2024-09-08 11:25:00,357:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 11:25:00,357:INFO:create_model() successfully completed......................................
2024-09-08 11:25:00,522:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:00,522:INFO:Creating metrics dataframe
2024-09-08 11:25:00,530:INFO:Initializing K Neighbors Classifier
2024-09-08 11:25:00,530:INFO:Total runtime is 0.31307616233825686 minutes
2024-09-08 11:25:00,530:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:00,530:INFO:Initializing create_model()
2024-09-08 11:25:00,530:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:00,530:INFO:Checking exceptions
2024-09-08 11:25:00,530:INFO:Importing libraries
2024-09-08 11:25:00,530:INFO:Copying training dataset
2024-09-08 11:25:00,546:INFO:Defining folds
2024-09-08 11:25:00,546:INFO:Declaring metric variables
2024-09-08 11:25:00,546:INFO:Importing untrained model
2024-09-08 11:25:00,546:INFO:K Neighbors Classifier Imported successfully
2024-09-08 11:25:00,546:INFO:Starting cross validation
2024-09-08 11:25:00,556:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:03,207:INFO:Calculating mean and std
2024-09-08 11:25:03,207:INFO:Creating metrics dataframe
2024-09-08 11:25:03,210:INFO:Uploading results into container
2024-09-08 11:25:03,210:INFO:Uploading model into container now
2024-09-08 11:25:03,210:INFO:_master_model_container: 2
2024-09-08 11:25:03,210:INFO:_display_container: 2
2024-09-08 11:25:03,210:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 11:25:03,210:INFO:create_model() successfully completed......................................
2024-09-08 11:25:03,393:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:03,393:INFO:Creating metrics dataframe
2024-09-08 11:25:03,397:INFO:Initializing Naive Bayes
2024-09-08 11:25:03,397:INFO:Total runtime is 0.3608575463294983 minutes
2024-09-08 11:25:03,401:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:03,401:INFO:Initializing create_model()
2024-09-08 11:25:03,401:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:03,401:INFO:Checking exceptions
2024-09-08 11:25:03,401:INFO:Importing libraries
2024-09-08 11:25:03,401:INFO:Copying training dataset
2024-09-08 11:25:03,422:INFO:Defining folds
2024-09-08 11:25:03,422:INFO:Declaring metric variables
2024-09-08 11:25:03,422:INFO:Importing untrained model
2024-09-08 11:25:03,425:INFO:Naive Bayes Imported successfully
2024-09-08 11:25:03,425:INFO:Starting cross validation
2024-09-08 11:25:03,434:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:04,887:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:05,736:INFO:Calculating mean and std
2024-09-08 11:25:05,737:INFO:Creating metrics dataframe
2024-09-08 11:25:05,743:INFO:Uploading results into container
2024-09-08 11:25:05,743:INFO:Uploading model into container now
2024-09-08 11:25:05,743:INFO:_master_model_container: 3
2024-09-08 11:25:05,743:INFO:_display_container: 2
2024-09-08 11:25:05,743:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 11:25:05,743:INFO:create_model() successfully completed......................................
2024-09-08 11:25:05,932:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:05,932:INFO:Creating metrics dataframe
2024-09-08 11:25:05,937:INFO:Initializing Decision Tree Classifier
2024-09-08 11:25:05,937:INFO:Total runtime is 0.40319398244222004 minutes
2024-09-08 11:25:05,937:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:05,937:INFO:Initializing create_model()
2024-09-08 11:25:05,937:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:05,937:INFO:Checking exceptions
2024-09-08 11:25:05,937:INFO:Importing libraries
2024-09-08 11:25:05,937:INFO:Copying training dataset
2024-09-08 11:25:05,957:INFO:Defining folds
2024-09-08 11:25:05,957:INFO:Declaring metric variables
2024-09-08 11:25:05,957:INFO:Importing untrained model
2024-09-08 11:25:05,959:INFO:Decision Tree Classifier Imported successfully
2024-09-08 11:25:05,960:INFO:Starting cross validation
2024-09-08 11:25:05,967:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:08,281:INFO:Calculating mean and std
2024-09-08 11:25:08,281:INFO:Creating metrics dataframe
2024-09-08 11:25:08,281:INFO:Uploading results into container
2024-09-08 11:25:08,287:INFO:Uploading model into container now
2024-09-08 11:25:08,287:INFO:_master_model_container: 4
2024-09-08 11:25:08,287:INFO:_display_container: 2
2024-09-08 11:25:08,287:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 11:25:08,287:INFO:create_model() successfully completed......................................
2024-09-08 11:25:08,437:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:08,438:INFO:Creating metrics dataframe
2024-09-08 11:25:08,439:INFO:Initializing SVM - Linear Kernel
2024-09-08 11:25:08,439:INFO:Total runtime is 0.44489100376764934 minutes
2024-09-08 11:25:08,439:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:08,439:INFO:Initializing create_model()
2024-09-08 11:25:08,439:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:08,439:INFO:Checking exceptions
2024-09-08 11:25:08,439:INFO:Importing libraries
2024-09-08 11:25:08,439:INFO:Copying training dataset
2024-09-08 11:25:08,461:INFO:Defining folds
2024-09-08 11:25:08,461:INFO:Declaring metric variables
2024-09-08 11:25:08,461:INFO:Importing untrained model
2024-09-08 11:25:08,464:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 11:25:08,464:INFO:Starting cross validation
2024-09-08 11:25:08,471:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:09,917:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:09,967:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:09,987:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:10,002:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:10,007:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:10,030:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:10,039:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:10,057:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:10,079:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:10,079:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:10,147:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:10,727:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:10,738:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:10,763:INFO:Calculating mean and std
2024-09-08 11:25:10,764:INFO:Creating metrics dataframe
2024-09-08 11:25:10,767:INFO:Uploading results into container
2024-09-08 11:25:10,767:INFO:Uploading model into container now
2024-09-08 11:25:10,770:INFO:_master_model_container: 5
2024-09-08 11:25:10,770:INFO:_display_container: 2
2024-09-08 11:25:10,770:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 11:25:10,770:INFO:create_model() successfully completed......................................
2024-09-08 11:25:10,927:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:10,934:INFO:Creating metrics dataframe
2024-09-08 11:25:10,937:INFO:Initializing Ridge Classifier
2024-09-08 11:25:10,937:INFO:Total runtime is 0.4865306297938029 minutes
2024-09-08 11:25:10,937:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:10,937:INFO:Initializing create_model()
2024-09-08 11:25:10,937:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:10,937:INFO:Checking exceptions
2024-09-08 11:25:10,937:INFO:Importing libraries
2024-09-08 11:25:10,937:INFO:Copying training dataset
2024-09-08 11:25:10,950:INFO:Defining folds
2024-09-08 11:25:10,950:INFO:Declaring metric variables
2024-09-08 11:25:10,950:INFO:Importing untrained model
2024-09-08 11:25:10,950:INFO:Ridge Classifier Imported successfully
2024-09-08 11:25:10,958:INFO:Starting cross validation
2024-09-08 11:25:10,967:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:12,432:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:12,432:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:12,461:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:12,465:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:12,478:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:12,478:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:12,494:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:12,537:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:13,207:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:13,217:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:13,257:INFO:Calculating mean and std
2024-09-08 11:25:13,257:INFO:Creating metrics dataframe
2024-09-08 11:25:13,257:INFO:Uploading results into container
2024-09-08 11:25:13,257:INFO:Uploading model into container now
2024-09-08 11:25:13,265:INFO:_master_model_container: 6
2024-09-08 11:25:13,265:INFO:_display_container: 2
2024-09-08 11:25:13,265:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 11:25:13,266:INFO:create_model() successfully completed......................................
2024-09-08 11:25:13,427:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:13,427:INFO:Creating metrics dataframe
2024-09-08 11:25:13,431:INFO:Initializing Random Forest Classifier
2024-09-08 11:25:13,431:INFO:Total runtime is 0.5280948479970297 minutes
2024-09-08 11:25:13,431:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:13,431:INFO:Initializing create_model()
2024-09-08 11:25:13,431:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:13,431:INFO:Checking exceptions
2024-09-08 11:25:13,431:INFO:Importing libraries
2024-09-08 11:25:13,431:INFO:Copying training dataset
2024-09-08 11:25:13,448:INFO:Defining folds
2024-09-08 11:25:13,448:INFO:Declaring metric variables
2024-09-08 11:25:13,448:INFO:Importing untrained model
2024-09-08 11:25:13,448:INFO:Random Forest Classifier Imported successfully
2024-09-08 11:25:13,448:INFO:Starting cross validation
2024-09-08 11:25:13,457:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:17,693:INFO:Calculating mean and std
2024-09-08 11:25:17,693:INFO:Creating metrics dataframe
2024-09-08 11:25:17,701:INFO:Uploading results into container
2024-09-08 11:25:17,701:INFO:Uploading model into container now
2024-09-08 11:25:17,701:INFO:_master_model_container: 7
2024-09-08 11:25:17,701:INFO:_display_container: 2
2024-09-08 11:25:17,708:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 11:25:17,709:INFO:create_model() successfully completed......................................
2024-09-08 11:25:17,878:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:17,881:INFO:Creating metrics dataframe
2024-09-08 11:25:17,882:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 11:25:17,888:INFO:Total runtime is 0.6022701462109884 minutes
2024-09-08 11:25:17,888:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:17,889:INFO:Initializing create_model()
2024-09-08 11:25:17,889:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:17,889:INFO:Checking exceptions
2024-09-08 11:25:17,889:INFO:Importing libraries
2024-09-08 11:25:17,889:INFO:Copying training dataset
2024-09-08 11:25:17,908:INFO:Defining folds
2024-09-08 11:25:17,908:INFO:Declaring metric variables
2024-09-08 11:25:17,908:INFO:Importing untrained model
2024-09-08 11:25:17,908:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 11:25:17,908:INFO:Starting cross validation
2024-09-08 11:25:17,918:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:19,020:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:25:19,020:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:25:19,033:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:25:19,045:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:25:19,138:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:25:19,163:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:25:19,169:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:25:19,178:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:25:19,383:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:19,433:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:19,453:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:19,464:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:19,470:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:19,503:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:19,520:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:19,529:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:20,078:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:25:20,132:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:25:20,278:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:20,338:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:20,364:INFO:Calculating mean and std
2024-09-08 11:25:20,372:INFO:Creating metrics dataframe
2024-09-08 11:25:20,378:INFO:Uploading results into container
2024-09-08 11:25:20,379:INFO:Uploading model into container now
2024-09-08 11:25:20,379:INFO:_master_model_container: 8
2024-09-08 11:25:20,379:INFO:_display_container: 2
2024-09-08 11:25:20,379:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 11:25:20,379:INFO:create_model() successfully completed......................................
2024-09-08 11:25:20,558:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:20,558:INFO:Creating metrics dataframe
2024-09-08 11:25:20,564:INFO:Initializing Ada Boost Classifier
2024-09-08 11:25:20,564:INFO:Total runtime is 0.6469798882802328 minutes
2024-09-08 11:25:20,568:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:20,568:INFO:Initializing create_model()
2024-09-08 11:25:20,568:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:20,568:INFO:Checking exceptions
2024-09-08 11:25:20,568:INFO:Importing libraries
2024-09-08 11:25:20,568:INFO:Copying training dataset
2024-09-08 11:25:20,588:INFO:Defining folds
2024-09-08 11:25:20,588:INFO:Declaring metric variables
2024-09-08 11:25:20,588:INFO:Importing untrained model
2024-09-08 11:25:20,588:INFO:Ada Boost Classifier Imported successfully
2024-09-08 11:25:20,588:INFO:Starting cross validation
2024-09-08 11:25:20,604:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:21,581:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:25:21,581:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:25:21,678:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:25:21,720:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:25:21,788:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:25:21,868:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:25:21,884:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:25:21,973:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:25:22,526:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:22,672:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:22,681:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:22,681:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:22,721:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:22,843:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:22,848:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:22,888:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:23,248:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:25:23,351:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:25:23,783:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:23,853:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:23,878:INFO:Calculating mean and std
2024-09-08 11:25:23,878:INFO:Creating metrics dataframe
2024-09-08 11:25:23,878:INFO:Uploading results into container
2024-09-08 11:25:23,878:INFO:Uploading model into container now
2024-09-08 11:25:23,878:INFO:_master_model_container: 9
2024-09-08 11:25:23,878:INFO:_display_container: 2
2024-09-08 11:25:23,878:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 11:25:23,878:INFO:create_model() successfully completed......................................
2024-09-08 11:25:24,048:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:24,048:INFO:Creating metrics dataframe
2024-09-08 11:25:24,053:INFO:Initializing Gradient Boosting Classifier
2024-09-08 11:25:24,053:INFO:Total runtime is 0.7051316142082215 minutes
2024-09-08 11:25:24,053:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:24,053:INFO:Initializing create_model()
2024-09-08 11:25:24,053:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:24,053:INFO:Checking exceptions
2024-09-08 11:25:24,053:INFO:Importing libraries
2024-09-08 11:25:24,053:INFO:Copying training dataset
2024-09-08 11:25:24,070:INFO:Defining folds
2024-09-08 11:25:24,070:INFO:Declaring metric variables
2024-09-08 11:25:24,070:INFO:Importing untrained model
2024-09-08 11:25:24,070:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 11:25:24,070:INFO:Starting cross validation
2024-09-08 11:25:24,078:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:27,823:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:27,908:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:27,923:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:27,934:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:28,013:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:28,054:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:28,070:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:28,189:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:30,342:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:30,364:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:30,392:INFO:Calculating mean and std
2024-09-08 11:25:30,392:INFO:Creating metrics dataframe
2024-09-08 11:25:30,392:INFO:Uploading results into container
2024-09-08 11:25:30,399:INFO:Uploading model into container now
2024-09-08 11:25:30,399:INFO:_master_model_container: 10
2024-09-08 11:25:30,400:INFO:_display_container: 2
2024-09-08 11:25:30,400:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 11:25:30,400:INFO:create_model() successfully completed......................................
2024-09-08 11:25:30,647:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:30,647:INFO:Creating metrics dataframe
2024-09-08 11:25:30,655:INFO:Initializing Linear Discriminant Analysis
2024-09-08 11:25:30,656:INFO:Total runtime is 0.8151700218518575 minutes
2024-09-08 11:25:30,656:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:30,656:INFO:Initializing create_model()
2024-09-08 11:25:30,656:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:30,656:INFO:Checking exceptions
2024-09-08 11:25:30,656:INFO:Importing libraries
2024-09-08 11:25:30,656:INFO:Copying training dataset
2024-09-08 11:25:30,672:INFO:Defining folds
2024-09-08 11:25:30,672:INFO:Declaring metric variables
2024-09-08 11:25:30,672:INFO:Importing untrained model
2024-09-08 11:25:30,679:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 11:25:30,680:INFO:Starting cross validation
2024-09-08 11:25:30,688:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:32,249:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:32,274:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:32,293:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:32,293:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:32,319:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:32,329:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:32,352:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:32,360:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:33,004:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:33,037:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:25:33,062:INFO:Calculating mean and std
2024-09-08 11:25:33,062:INFO:Creating metrics dataframe
2024-09-08 11:25:33,062:INFO:Uploading results into container
2024-09-08 11:25:33,062:INFO:Uploading model into container now
2024-09-08 11:25:33,069:INFO:_master_model_container: 11
2024-09-08 11:25:33,069:INFO:_display_container: 2
2024-09-08 11:25:33,069:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 11:25:33,069:INFO:create_model() successfully completed......................................
2024-09-08 11:25:33,229:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:33,229:INFO:Creating metrics dataframe
2024-09-08 11:25:33,239:INFO:Initializing Extra Trees Classifier
2024-09-08 11:25:33,239:INFO:Total runtime is 0.8582280953725179 minutes
2024-09-08 11:25:33,239:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:33,239:INFO:Initializing create_model()
2024-09-08 11:25:33,239:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:33,239:INFO:Checking exceptions
2024-09-08 11:25:33,239:INFO:Importing libraries
2024-09-08 11:25:33,239:INFO:Copying training dataset
2024-09-08 11:25:33,259:INFO:Defining folds
2024-09-08 11:25:33,259:INFO:Declaring metric variables
2024-09-08 11:25:33,259:INFO:Importing untrained model
2024-09-08 11:25:33,259:INFO:Extra Trees Classifier Imported successfully
2024-09-08 11:25:33,259:INFO:Starting cross validation
2024-09-08 11:25:33,275:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:37,114:INFO:Calculating mean and std
2024-09-08 11:25:37,114:INFO:Creating metrics dataframe
2024-09-08 11:25:37,119:INFO:Uploading results into container
2024-09-08 11:25:37,119:INFO:Uploading model into container now
2024-09-08 11:25:37,122:INFO:_master_model_container: 12
2024-09-08 11:25:37,122:INFO:_display_container: 2
2024-09-08 11:25:37,122:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 11:25:37,122:INFO:create_model() successfully completed......................................
2024-09-08 11:25:37,296:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:37,296:INFO:Creating metrics dataframe
2024-09-08 11:25:37,305:INFO:Initializing Extreme Gradient Boosting
2024-09-08 11:25:37,305:INFO:Total runtime is 0.9259896794954936 minutes
2024-09-08 11:25:37,305:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:37,305:INFO:Initializing create_model()
2024-09-08 11:25:37,305:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:37,305:INFO:Checking exceptions
2024-09-08 11:25:37,305:INFO:Importing libraries
2024-09-08 11:25:37,305:INFO:Copying training dataset
2024-09-08 11:25:37,321:INFO:Defining folds
2024-09-08 11:25:37,321:INFO:Declaring metric variables
2024-09-08 11:25:37,321:INFO:Importing untrained model
2024-09-08 11:25:37,321:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:25:37,321:INFO:Starting cross validation
2024-09-08 11:25:37,330:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:41,713:INFO:Calculating mean and std
2024-09-08 11:25:41,713:INFO:Creating metrics dataframe
2024-09-08 11:25:41,719:INFO:Uploading results into container
2024-09-08 11:25:41,721:INFO:Uploading model into container now
2024-09-08 11:25:41,721:INFO:_master_model_container: 13
2024-09-08 11:25:41,721:INFO:_display_container: 2
2024-09-08 11:25:41,721:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 11:25:41,721:INFO:create_model() successfully completed......................................
2024-09-08 11:25:41,894:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:41,895:INFO:Creating metrics dataframe
2024-09-08 11:25:41,900:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 11:25:41,900:INFO:Total runtime is 1.0025697469711303 minutes
2024-09-08 11:25:41,900:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:41,900:INFO:Initializing create_model()
2024-09-08 11:25:41,900:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:41,900:INFO:Checking exceptions
2024-09-08 11:25:41,900:INFO:Importing libraries
2024-09-08 11:25:41,900:INFO:Copying training dataset
2024-09-08 11:25:41,925:INFO:Defining folds
2024-09-08 11:25:41,925:INFO:Declaring metric variables
2024-09-08 11:25:41,925:INFO:Importing untrained model
2024-09-08 11:25:41,926:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:25:41,926:INFO:Starting cross validation
2024-09-08 11:25:41,930:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:50,743:INFO:Calculating mean and std
2024-09-08 11:25:50,743:INFO:Creating metrics dataframe
2024-09-08 11:25:50,752:INFO:Uploading results into container
2024-09-08 11:25:50,752:INFO:Uploading model into container now
2024-09-08 11:25:50,755:INFO:_master_model_container: 14
2024-09-08 11:25:50,755:INFO:_display_container: 2
2024-09-08 11:25:50,755:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:25:50,755:INFO:create_model() successfully completed......................................
2024-09-08 11:25:50,940:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:50,940:INFO:Creating metrics dataframe
2024-09-08 11:25:50,943:INFO:Initializing Dummy Classifier
2024-09-08 11:25:50,943:INFO:Total runtime is 1.1532980759938558 minutes
2024-09-08 11:25:50,943:INFO:SubProcess create_model() called ==================================
2024-09-08 11:25:50,943:INFO:Initializing create_model()
2024-09-08 11:25:50,943:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B110AAF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:50,943:INFO:Checking exceptions
2024-09-08 11:25:50,943:INFO:Importing libraries
2024-09-08 11:25:50,943:INFO:Copying training dataset
2024-09-08 11:25:50,965:INFO:Defining folds
2024-09-08 11:25:50,965:INFO:Declaring metric variables
2024-09-08 11:25:50,968:INFO:Importing untrained model
2024-09-08 11:25:50,968:INFO:Dummy Classifier Imported successfully
2024-09-08 11:25:50,970:INFO:Starting cross validation
2024-09-08 11:25:50,977:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:25:52,420:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:52,431:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:52,431:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:52,495:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:52,501:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:52,516:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:52,551:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:52,655:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:53,255:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:53,260:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:25:53,280:INFO:Calculating mean and std
2024-09-08 11:25:53,280:INFO:Creating metrics dataframe
2024-09-08 11:25:53,280:INFO:Uploading results into container
2024-09-08 11:25:53,288:INFO:Uploading model into container now
2024-09-08 11:25:53,288:INFO:_master_model_container: 15
2024-09-08 11:25:53,288:INFO:_display_container: 2
2024-09-08 11:25:53,288:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 11:25:53,288:INFO:create_model() successfully completed......................................
2024-09-08 11:25:53,483:INFO:SubProcess create_model() end ==================================
2024-09-08 11:25:53,483:INFO:Creating metrics dataframe
2024-09-08 11:25:53,492:INFO:Initializing create_model()
2024-09-08 11:25:53,492:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:53,492:INFO:Checking exceptions
2024-09-08 11:25:53,492:INFO:Importing libraries
2024-09-08 11:25:53,492:INFO:Copying training dataset
2024-09-08 11:25:53,510:INFO:Defining folds
2024-09-08 11:25:53,510:INFO:Declaring metric variables
2024-09-08 11:25:53,510:INFO:Importing untrained model
2024-09-08 11:25:53,510:INFO:Declaring custom model
2024-09-08 11:25:53,517:INFO:Random Forest Classifier Imported successfully
2024-09-08 11:25:53,525:INFO:Cross validation set to False
2024-09-08 11:25:53,525:INFO:Fitting Model
2024-09-08 11:25:54,465:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 11:25:54,465:INFO:create_model() successfully completed......................................
2024-09-08 11:25:54,633:INFO:Initializing create_model()
2024-09-08 11:25:54,638:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:54,638:INFO:Checking exceptions
2024-09-08 11:25:54,640:INFO:Importing libraries
2024-09-08 11:25:54,640:INFO:Copying training dataset
2024-09-08 11:25:54,653:INFO:Defining folds
2024-09-08 11:25:54,653:INFO:Declaring metric variables
2024-09-08 11:25:54,653:INFO:Importing untrained model
2024-09-08 11:25:54,653:INFO:Declaring custom model
2024-09-08 11:25:54,653:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:25:54,669:INFO:Cross validation set to False
2024-09-08 11:25:54,671:INFO:Fitting Model
2024-09-08 11:25:55,179:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 11:25:55,180:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000689 seconds.
2024-09-08 11:25:55,180:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 11:25:55,180:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 11:25:55,185:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 11:25:55,185:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 11:25:55,187:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 11:25:55,187:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 11:25:55,187:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 11:25:55,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:25:55,650:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:25:55,650:INFO:create_model() successfully completed......................................
2024-09-08 11:25:55,822:INFO:Initializing create_model()
2024-09-08 11:25:55,822:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:55,822:INFO:Checking exceptions
2024-09-08 11:25:55,831:INFO:Importing libraries
2024-09-08 11:25:55,831:INFO:Copying training dataset
2024-09-08 11:25:55,851:INFO:Defining folds
2024-09-08 11:25:55,851:INFO:Declaring metric variables
2024-09-08 11:25:55,851:INFO:Importing untrained model
2024-09-08 11:25:55,851:INFO:Declaring custom model
2024-09-08 11:25:55,856:INFO:Extra Trees Classifier Imported successfully
2024-09-08 11:25:55,862:INFO:Cross validation set to False
2024-09-08 11:25:55,864:INFO:Fitting Model
2024-09-08 11:25:56,584:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 11:25:56,584:INFO:create_model() successfully completed......................................
2024-09-08 11:25:56,773:INFO:_master_model_container: 15
2024-09-08 11:25:56,773:INFO:_display_container: 2
2024-09-08 11:25:56,773:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)]
2024-09-08 11:25:56,773:INFO:compare_models() successfully completed......................................
2024-09-08 11:25:56,773:INFO:Initializing create_model()
2024-09-08 11:25:56,773:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:25:56,773:INFO:Checking exceptions
2024-09-08 11:25:56,773:INFO:Importing libraries
2024-09-08 11:25:56,773:INFO:Copying training dataset
2024-09-08 11:25:56,805:INFO:Defining folds
2024-09-08 11:25:56,805:INFO:Declaring metric variables
2024-09-08 11:25:56,805:INFO:Importing untrained model
2024-09-08 11:25:56,805:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:25:56,805:INFO:Starting cross validation
2024-09-08 11:25:56,816:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:26:06,029:INFO:Calculating mean and std
2024-09-08 11:26:06,031:INFO:Creating metrics dataframe
2024-09-08 11:26:06,036:INFO:Finalizing model
2024-09-08 11:26:06,531:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 11:26:06,537:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000528 seconds.
2024-09-08 11:26:06,537:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 11:26:06,537:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 11:26:06,537:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 11:26:06,537:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 11:26:06,537:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 11:26:06,537:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 11:26:06,542:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 11:26:06,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:06,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:07,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:07,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:07,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:07,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:07,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:26:07,171:INFO:Uploading results into container
2024-09-08 11:26:07,171:INFO:Uploading model into container now
2024-09-08 11:26:07,203:INFO:_master_model_container: 16
2024-09-08 11:26:07,203:INFO:_display_container: 3
2024-09-08 11:26:07,203:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:26:07,212:INFO:create_model() successfully completed......................................
2024-09-08 11:26:07,402:INFO:Initializing create_model()
2024-09-08 11:26:07,402:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:26:07,402:INFO:Checking exceptions
2024-09-08 11:26:07,412:INFO:Importing libraries
2024-09-08 11:26:07,412:INFO:Copying training dataset
2024-09-08 11:26:07,431:INFO:Defining folds
2024-09-08 11:26:07,431:INFO:Declaring metric variables
2024-09-08 11:26:07,431:INFO:Importing untrained model
2024-09-08 11:26:07,434:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:26:07,434:INFO:Starting cross validation
2024-09-08 11:26:07,443:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:26:11,780:INFO:Calculating mean and std
2024-09-08 11:26:11,782:INFO:Creating metrics dataframe
2024-09-08 11:26:11,787:INFO:Finalizing model
2024-09-08 11:26:12,748:INFO:Uploading results into container
2024-09-08 11:26:12,752:INFO:Uploading model into container now
2024-09-08 11:26:12,782:INFO:_master_model_container: 17
2024-09-08 11:26:12,782:INFO:_display_container: 4
2024-09-08 11:26:12,787:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:26:12,787:INFO:create_model() successfully completed......................................
2024-09-08 11:26:12,953:INFO:Initializing create_model()
2024-09-08 11:26:12,953:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:26:12,953:INFO:Checking exceptions
2024-09-08 11:26:12,953:INFO:Importing libraries
2024-09-08 11:26:12,953:INFO:Copying training dataset
2024-09-08 11:26:12,972:INFO:Defining folds
2024-09-08 11:26:12,972:INFO:Declaring metric variables
2024-09-08 11:26:12,972:INFO:Importing untrained model
2024-09-08 11:26:12,972:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 11:26:12,978:INFO:Starting cross validation
2024-09-08 11:26:12,982:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:26:16,800:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:26:16,834:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:26:16,952:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:26:16,987:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:26:17,092:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:26:17,253:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:26:17,297:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:26:17,383:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:26:19,524:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:26:19,592:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:26:19,616:INFO:Calculating mean and std
2024-09-08 11:26:19,616:INFO:Creating metrics dataframe
2024-09-08 11:26:19,616:INFO:Finalizing model
2024-09-08 11:26:21,912:INFO:Uploading results into container
2024-09-08 11:26:21,914:INFO:Uploading model into container now
2024-09-08 11:26:21,944:INFO:_master_model_container: 18
2024-09-08 11:26:21,944:INFO:_display_container: 5
2024-09-08 11:26:21,944:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 11:26:21,944:INFO:create_model() successfully completed......................................
2024-09-08 11:26:22,118:INFO:Initializing tune_model()
2024-09-08 11:26:22,118:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>)
2024-09-08 11:26:22,118:INFO:Checking exceptions
2024-09-08 11:26:22,118:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 11:26:22,276:INFO:Copying training dataset
2024-09-08 11:26:22,284:INFO:Checking base model
2024-09-08 11:26:22,284:INFO:Base model : Extreme Gradient Boosting
2024-09-08 11:26:22,284:INFO:Declaring metric variables
2024-09-08 11:26:22,284:INFO:Defining Hyperparameters
2024-09-08 11:26:22,473:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 11:26:22,473:INFO:Tuning with n_jobs=-1
2024-09-08 11:26:22,482:INFO:Initializing skopt.BayesSearchCV
2024-09-08 11:26:40,366:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.6), ('actual_estimator__gamma', 0), ('actual_estimator__learning_rate', 0.01), ('actual_estimator__max_depth', 7), ('actual_estimator__min_child_weight', 1), ('actual_estimator__n_estimators', 100), ('actual_estimator__subsample', 0.6)])
2024-09-08 11:26:40,366:INFO:Hyperparameter search completed
2024-09-08 11:26:40,366:INFO:SubProcess create_model() called ==================================
2024-09-08 11:26:40,366:INFO:Initializing create_model()
2024-09-08 11:26:40,366:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021B10927CA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.6})
2024-09-08 11:26:40,366:INFO:Checking exceptions
2024-09-08 11:26:40,366:INFO:Importing libraries
2024-09-08 11:26:40,366:INFO:Copying training dataset
2024-09-08 11:26:40,384:INFO:Defining folds
2024-09-08 11:26:40,384:INFO:Declaring metric variables
2024-09-08 11:26:40,384:INFO:Importing untrained model
2024-09-08 11:26:40,384:INFO:Declaring custom model
2024-09-08 11:26:40,391:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:26:40,391:INFO:Starting cross validation
2024-09-08 11:26:40,399:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:26:42,554:INFO:Calculating mean and std
2024-09-08 11:26:42,554:INFO:Creating metrics dataframe
2024-09-08 11:26:42,559:INFO:Finalizing model
2024-09-08 11:26:43,714:INFO:Uploading results into container
2024-09-08 11:26:43,719:INFO:Uploading model into container now
2024-09-08 11:26:43,719:INFO:_master_model_container: 19
2024-09-08 11:26:43,719:INFO:_display_container: 6
2024-09-08 11:26:43,719:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:26:43,724:INFO:create_model() successfully completed......................................
2024-09-08 11:26:43,909:INFO:SubProcess create_model() end ==================================
2024-09-08 11:26:43,909:INFO:choose_better activated
2024-09-08 11:26:43,909:INFO:SubProcess create_model() called ==================================
2024-09-08 11:26:43,914:INFO:Initializing create_model()
2024-09-08 11:26:43,914:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:26:43,914:INFO:Checking exceptions
2024-09-08 11:26:43,914:INFO:Importing libraries
2024-09-08 11:26:43,914:INFO:Copying training dataset
2024-09-08 11:26:43,926:INFO:Defining folds
2024-09-08 11:26:43,926:INFO:Declaring metric variables
2024-09-08 11:26:43,934:INFO:Importing untrained model
2024-09-08 11:26:43,934:INFO:Declaring custom model
2024-09-08 11:26:43,934:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:26:43,934:INFO:Starting cross validation
2024-09-08 11:26:43,944:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:26:45,739:INFO:Calculating mean and std
2024-09-08 11:26:45,739:INFO:Creating metrics dataframe
2024-09-08 11:26:45,744:INFO:Finalizing model
2024-09-08 11:26:46,764:INFO:Uploading results into container
2024-09-08 11:26:46,764:INFO:Uploading model into container now
2024-09-08 11:26:46,764:INFO:_master_model_container: 20
2024-09-08 11:26:46,769:INFO:_display_container: 7
2024-09-08 11:26:46,769:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:26:46,769:INFO:create_model() successfully completed......................................
2024-09-08 11:26:46,948:INFO:SubProcess create_model() end ==================================
2024-09-08 11:26:46,956:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8186
2024-09-08 11:26:46,957:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8386
2024-09-08 11:26:46,957:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 11:26:46,957:INFO:choose_better completed
2024-09-08 11:26:46,981:INFO:_master_model_container: 20
2024-09-08 11:26:46,981:INFO:_display_container: 6
2024-09-08 11:26:46,981:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:26:46,984:INFO:tune_model() successfully completed......................................
2024-09-08 11:26:47,154:INFO:Initializing predict_model()
2024-09-08 11:26:47,158:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021B0C3E1480>)
2024-09-08 11:26:47,158:INFO:Checking exceptions
2024-09-08 11:26:47,158:INFO:Preloading libraries
2024-09-08 11:26:47,775:INFO:Initializing get_config()
2024-09-08 11:26:47,775:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, variable=X_train)
2024-09-08 11:26:47,775:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 11:26:47,859:INFO:Variable:  returned as       id Marital status Application mode Application order  ... Curricular units 2nd sem (without evaluations) Unemployment rate Inflation rate   GDP
765  765              1                1                 6  ...                                              0         10.800000            1.4  1.74
323  323              1               17                 1  ...                                              0          7.600000            2.6  0.32
909  909              1                1                 1  ...                                              0         12.400000            0.5  1.79
774  774              1               17                 1  ...                                              0          8.900000            1.4  3.51
931  931              1               39                 1  ...                                              0         16.200001            0.3 -0.92
..   ...            ...              ...               ...  ...                                            ...               ...            ...   ...
806  806              4               39                 1  ...                                              0         10.800000            1.4  1.74
492  492              1                1                 3  ...                                              0          7.600000            2.6  0.32
698  698              1               39                 1  ...                                              1         10.800000            1.4  1.74
44    44              1                1                 1  ...                                              0          9.400000           -0.8 -3.12
604  604              1                1                 1  ...                                              0         10.800000            1.4  1.74

[700 rows x 37 columns]
2024-09-08 11:26:47,859:INFO:get_config() successfully completed......................................
2024-09-08 11:26:47,863:INFO:Initializing predict_model()
2024-09-08 11:26:47,863:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021B1107DC60>)
2024-09-08 11:26:47,863:INFO:Checking exceptions
2024-09-08 11:26:47,863:INFO:Preloading libraries
2024-09-08 11:26:47,863:INFO:Set up data.
2024-09-08 11:26:47,879:INFO:Set up index.
2024-09-08 11:26:48,294:INFO:Initializing get_config()
2024-09-08 11:26:48,294:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, variable=y_train)
2024-09-08 11:26:48,294:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 11:26:48,301:INFO:Variable:  returned as 765    0
323    1
909    0
774    0
931    1
      ..
806    1
492    0
698    1
44     0
604    1
Name: Target, Length: 700, dtype: int8
2024-09-08 11:26:48,301:INFO:get_config() successfully completed......................................
2024-09-08 11:26:48,301:INFO:Initializing get_config()
2024-09-08 11:26:48,301:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, variable=y_test)
2024-09-08 11:26:48,301:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 11:26:48,305:INFO:Variable:  returned as 47     1
233    1
132    2
34     0
162    0
      ..
772    0
632    0
365    2
136    0
299    1
Name: Target, Length: 300, dtype: int8
2024-09-08 11:26:48,310:INFO:get_config() successfully completed......................................
2024-09-08 11:26:48,310:INFO:Initializing finalize_model()
2024-09-08 11:26:48,310:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 11:26:48,310:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:26:48,324:INFO:Initializing create_model()
2024-09-08 11:26:48,324:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:26:48,324:INFO:Checking exceptions
2024-09-08 11:26:48,324:INFO:Importing libraries
2024-09-08 11:26:48,326:INFO:Copying training dataset
2024-09-08 11:26:48,326:INFO:Defining folds
2024-09-08 11:26:48,326:INFO:Declaring metric variables
2024-09-08 11:26:48,326:INFO:Importing untrained model
2024-09-08 11:26:48,326:INFO:Declaring custom model
2024-09-08 11:26:48,326:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:26:48,335:INFO:Cross validation set to False
2024-09-08 11:26:48,335:INFO:Fitting Model
2024-09-08 11:26:49,914:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 11:26:49,914:INFO:create_model() successfully completed......................................
2024-09-08 11:26:50,085:INFO:_master_model_container: 20
2024-09-08 11:26:50,085:INFO:_display_container: 7
2024-09-08 11:26:50,464:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 11:26:50,464:INFO:finalize_model() successfully completed......................................
2024-09-08 11:26:51,048:INFO:Initializing predict_model()
2024-09-08 11:26:51,048:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021B0F097A90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021B114B0B80>)
2024-09-08 11:26:51,048:INFO:Checking exceptions
2024-09-08 11:26:51,048:INFO:Preloading libraries
2024-09-08 11:26:51,048:INFO:Set up data.
2024-09-08 11:26:51,089:INFO:Set up index.
2024-09-08 11:38:05,439:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:38:05,444:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:38:05,447:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:38:05,448:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:38:33,107:INFO:PyCaret ClassificationExperiment
2024-09-08 11:38:33,107:INFO:Logging name: clf-default-name
2024-09-08 11:38:33,107:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 11:38:33,107:INFO:version 3.3.2
2024-09-08 11:38:33,111:INFO:Initializing setup()
2024-09-08 11:38:33,111:INFO:self.USI: ddf9
2024-09-08 11:38:33,111:INFO:self._variable_keys: {'X_test', 'idx', 'y', 'target_param', 'X_train', 'is_multiclass', 'exp_id', 'seed', 'fold_generator', 'memory', 'gpu_n_jobs_param', 'html_param', 'fold_groups_param', 'y_train', 'pipeline', 'logging_param', 'exp_name_log', 'fold_shuffle_param', 'y_test', 'fix_imbalance', 'USI', 'n_jobs_param', 'gpu_param', '_available_plots', 'data', 'X', 'log_plots_param', '_ml_usecase'}
2024-09-08 11:38:33,111:INFO:Checking environment
2024-09-08 11:38:33,111:INFO:python_version: 3.10.11
2024-09-08 11:38:33,111:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 11:38:33,111:INFO:machine: AMD64
2024-09-08 11:38:33,134:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 11:38:33,137:INFO:Memory: svmem(total=16407719936, available=5742313472, percent=65.0, used=10665406464, free=5742313472)
2024-09-08 11:38:33,137:INFO:Physical Core: 4
2024-09-08 11:38:33,137:INFO:Logical Core: 8
2024-09-08 11:38:33,137:INFO:Checking libraries
2024-09-08 11:38:33,137:INFO:System:
2024-09-08 11:38:33,137:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 11:38:33,137:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 11:38:33,137:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 11:38:33,137:INFO:PyCaret required dependencies:
2024-09-08 11:38:33,242:INFO:                 pip: 24.2
2024-09-08 11:38:33,246:INFO:          setuptools: 72.1.0
2024-09-08 11:38:33,246:INFO:             pycaret: 3.3.2
2024-09-08 11:38:33,246:INFO:             IPython: 8.25.0
2024-09-08 11:38:33,246:INFO:          ipywidgets: 8.1.5
2024-09-08 11:38:33,246:INFO:                tqdm: 4.66.5
2024-09-08 11:38:33,246:INFO:               numpy: 1.26.4
2024-09-08 11:38:33,246:INFO:              pandas: 2.1.4
2024-09-08 11:38:33,246:INFO:              jinja2: 3.1.4
2024-09-08 11:38:33,250:INFO:               scipy: 1.11.4
2024-09-08 11:38:33,250:INFO:              joblib: 1.3.2
2024-09-08 11:38:33,250:INFO:             sklearn: 1.4.2
2024-09-08 11:38:33,250:INFO:                pyod: 2.0.1
2024-09-08 11:38:33,250:INFO:            imblearn: 0.12.3
2024-09-08 11:38:33,250:INFO:   category_encoders: 2.6.3
2024-09-08 11:38:33,250:INFO:            lightgbm: 4.5.0
2024-09-08 11:38:33,250:INFO:               numba: 0.60.0
2024-09-08 11:38:33,250:INFO:            requests: 2.32.3
2024-09-08 11:38:33,250:INFO:          matplotlib: 3.7.5
2024-09-08 11:38:33,250:INFO:          scikitplot: 0.3.7
2024-09-08 11:38:33,250:INFO:         yellowbrick: 1.5
2024-09-08 11:38:33,250:INFO:              plotly: 5.24.0
2024-09-08 11:38:33,250:INFO:    plotly-resampler: Not installed
2024-09-08 11:38:33,250:INFO:             kaleido: 0.2.1
2024-09-08 11:38:33,250:INFO:           schemdraw: 0.15
2024-09-08 11:38:33,250:INFO:         statsmodels: 0.14.2
2024-09-08 11:38:33,250:INFO:              sktime: 0.26.0
2024-09-08 11:38:33,250:INFO:               tbats: 1.1.3
2024-09-08 11:38:33,250:INFO:            pmdarima: 2.0.4
2024-09-08 11:38:33,250:INFO:              psutil: 5.9.0
2024-09-08 11:38:33,250:INFO:          markupsafe: 2.1.3
2024-09-08 11:38:33,250:INFO:             pickle5: Not installed
2024-09-08 11:38:33,250:INFO:         cloudpickle: 3.0.0
2024-09-08 11:38:33,250:INFO:         deprecation: 2.1.0
2024-09-08 11:38:33,250:INFO:              xxhash: 3.5.0
2024-09-08 11:38:33,250:INFO:           wurlitzer: Not installed
2024-09-08 11:38:33,250:INFO:PyCaret optional dependencies:
2024-09-08 11:38:33,275:INFO:                shap: Not installed
2024-09-08 11:38:33,277:INFO:           interpret: Not installed
2024-09-08 11:38:33,277:INFO:                umap: Not installed
2024-09-08 11:38:33,277:INFO:     ydata_profiling: Not installed
2024-09-08 11:38:33,277:INFO:  explainerdashboard: Not installed
2024-09-08 11:38:33,277:INFO:             autoviz: Not installed
2024-09-08 11:38:33,277:INFO:           fairlearn: Not installed
2024-09-08 11:38:33,277:INFO:          deepchecks: Not installed
2024-09-08 11:38:33,277:INFO:             xgboost: 2.1.1
2024-09-08 11:38:33,277:INFO:            catboost: Not installed
2024-09-08 11:38:33,277:INFO:              kmodes: Not installed
2024-09-08 11:38:33,277:INFO:             mlxtend: Not installed
2024-09-08 11:38:33,277:INFO:       statsforecast: Not installed
2024-09-08 11:38:33,277:INFO:        tune_sklearn: Not installed
2024-09-08 11:38:33,277:INFO:                 ray: Not installed
2024-09-08 11:38:33,277:INFO:            hyperopt: 0.2.7
2024-09-08 11:38:33,277:INFO:              optuna: 4.0.0
2024-09-08 11:38:33,277:INFO:               skopt: 0.10.2
2024-09-08 11:38:33,277:INFO:              mlflow: Not installed
2024-09-08 11:38:33,277:INFO:              gradio: Not installed
2024-09-08 11:38:33,277:INFO:             fastapi: Not installed
2024-09-08 11:38:33,277:INFO:             uvicorn: Not installed
2024-09-08 11:38:33,277:INFO:              m2cgen: Not installed
2024-09-08 11:38:33,277:INFO:           evidently: Not installed
2024-09-08 11:38:33,277:INFO:               fugue: Not installed
2024-09-08 11:38:33,277:INFO:           streamlit: 1.38.0
2024-09-08 11:38:33,277:INFO:             prophet: Not installed
2024-09-08 11:38:33,277:INFO:None
2024-09-08 11:38:33,277:INFO:Set up data.
2024-09-08 11:38:33,299:INFO:Set up folding strategy.
2024-09-08 11:38:33,299:INFO:Set up train/test split.
2024-09-08 11:38:33,322:INFO:Set up index.
2024-09-08 11:38:33,322:INFO:Assigning column types.
2024-09-08 11:38:33,327:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 11:38:33,417:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 11:38:33,423:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:38:33,480:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:38:33,488:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:38:33,579:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 11:38:33,579:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:38:33,637:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:38:33,644:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:38:33,646:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 11:38:33,727:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:38:33,787:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:38:33,797:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:38:33,883:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:38:33,937:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:38:33,942:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:38:33,942:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 11:38:34,096:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:38:34,098:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:38:34,240:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:38:34,240:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:38:34,247:INFO:Preparing preprocessing pipeline...
2024-09-08 11:38:34,247:INFO:Set up simple imputation.
2024-09-08 11:38:34,271:INFO:Set up encoding of ordinal features.
2024-09-08 11:38:34,306:INFO:Set up encoding of categorical features.
2024-09-08 11:38:34,306:INFO:Set up column name cleaning.
2024-09-08 11:38:34,843:INFO:Finished creating preprocessing pipeline.
2024-09-08 11:38:35,203:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                                                    'qualification',
                                                                    'Fathers '
                                                                    'qualification',
                                                                    'Mothers '
                                                                    'occupation',
                                                                    'Fathers '
                                                                    'occupation'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 11:38:35,203:INFO:Creating final display dataframe.
2024-09-08 11:38:35,677:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (1000, 38)
4        Transformed data shape       (1000, 129)
5   Transformed train set shape        (700, 129)
6    Transformed test set shape        (300, 129)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              ddf9
2024-09-08 11:38:35,853:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:38:35,857:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:38:35,997:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:38:36,007:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:38:36,007:INFO:setup() successfully completed in 2.93s...............
2024-09-08 11:38:36,007:INFO:Initializing compare_models()
2024-09-08 11:38:36,007:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 11:38:36,007:INFO:Checking exceptions
2024-09-08 11:38:36,022:INFO:Preparing display monitor
2024-09-08 11:38:36,027:INFO:Initializing Logistic Regression
2024-09-08 11:38:36,027:INFO:Total runtime is 0.0 minutes
2024-09-08 11:38:36,027:INFO:SubProcess create_model() called ==================================
2024-09-08 11:38:36,027:INFO:Initializing create_model()
2024-09-08 11:38:36,027:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:38:36,027:INFO:Checking exceptions
2024-09-08 11:38:36,027:INFO:Importing libraries
2024-09-08 11:38:36,027:INFO:Copying training dataset
2024-09-08 11:38:36,038:INFO:Defining folds
2024-09-08 11:38:36,038:INFO:Declaring metric variables
2024-09-08 11:38:36,038:INFO:Importing untrained model
2024-09-08 11:38:36,038:INFO:Logistic Regression Imported successfully
2024-09-08 11:38:36,038:INFO:Starting cross validation
2024-09-08 11:38:36,047:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:38:47,999:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:38:48,118:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:38:48,189:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:38:48,243:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:38:48,278:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:38:48,318:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:48,408:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:48,495:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:48,503:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:38:48,538:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:38:48,548:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:48,563:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:48,638:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:38:48,750:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:48,775:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:48,875:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:49,898:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:38:49,918:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:38:50,051:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:50,076:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:50,098:INFO:Calculating mean and std
2024-09-08 11:38:50,098:INFO:Creating metrics dataframe
2024-09-08 11:38:50,101:INFO:Uploading results into container
2024-09-08 11:38:50,101:INFO:Uploading model into container now
2024-09-08 11:38:50,101:INFO:_master_model_container: 1
2024-09-08 11:38:50,101:INFO:_display_container: 2
2024-09-08 11:38:50,101:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 11:38:50,101:INFO:create_model() successfully completed......................................
2024-09-08 11:38:50,238:INFO:SubProcess create_model() end ==================================
2024-09-08 11:38:50,238:INFO:Creating metrics dataframe
2024-09-08 11:38:50,241:INFO:Initializing K Neighbors Classifier
2024-09-08 11:38:50,241:INFO:Total runtime is 0.23690566221872966 minutes
2024-09-08 11:38:50,241:INFO:SubProcess create_model() called ==================================
2024-09-08 11:38:50,241:INFO:Initializing create_model()
2024-09-08 11:38:50,241:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:38:50,241:INFO:Checking exceptions
2024-09-08 11:38:50,241:INFO:Importing libraries
2024-09-08 11:38:50,241:INFO:Copying training dataset
2024-09-08 11:38:50,257:INFO:Defining folds
2024-09-08 11:38:50,258:INFO:Declaring metric variables
2024-09-08 11:38:50,258:INFO:Importing untrained model
2024-09-08 11:38:50,258:INFO:K Neighbors Classifier Imported successfully
2024-09-08 11:38:50,258:INFO:Starting cross validation
2024-09-08 11:38:50,266:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:38:52,157:INFO:Calculating mean and std
2024-09-08 11:38:52,158:INFO:Creating metrics dataframe
2024-09-08 11:38:52,158:INFO:Uploading results into container
2024-09-08 11:38:52,158:INFO:Uploading model into container now
2024-09-08 11:38:52,163:INFO:_master_model_container: 2
2024-09-08 11:38:52,163:INFO:_display_container: 2
2024-09-08 11:38:52,163:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 11:38:52,163:INFO:create_model() successfully completed......................................
2024-09-08 11:38:52,298:INFO:SubProcess create_model() end ==================================
2024-09-08 11:38:52,298:INFO:Creating metrics dataframe
2024-09-08 11:38:52,298:INFO:Initializing Naive Bayes
2024-09-08 11:38:52,298:INFO:Total runtime is 0.2711857795715332 minutes
2024-09-08 11:38:52,298:INFO:SubProcess create_model() called ==================================
2024-09-08 11:38:52,298:INFO:Initializing create_model()
2024-09-08 11:38:52,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:38:52,298:INFO:Checking exceptions
2024-09-08 11:38:52,298:INFO:Importing libraries
2024-09-08 11:38:52,298:INFO:Copying training dataset
2024-09-08 11:38:52,318:INFO:Defining folds
2024-09-08 11:38:52,318:INFO:Declaring metric variables
2024-09-08 11:38:52,318:INFO:Importing untrained model
2024-09-08 11:38:52,318:INFO:Naive Bayes Imported successfully
2024-09-08 11:38:52,318:INFO:Starting cross validation
2024-09-08 11:38:52,323:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:38:53,429:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:38:54,241:INFO:Calculating mean and std
2024-09-08 11:38:54,241:INFO:Creating metrics dataframe
2024-09-08 11:38:54,241:INFO:Uploading results into container
2024-09-08 11:38:54,241:INFO:Uploading model into container now
2024-09-08 11:38:54,241:INFO:_master_model_container: 3
2024-09-08 11:38:54,241:INFO:_display_container: 2
2024-09-08 11:38:54,248:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 11:38:54,248:INFO:create_model() successfully completed......................................
2024-09-08 11:38:54,390:INFO:SubProcess create_model() end ==================================
2024-09-08 11:38:54,390:INFO:Creating metrics dataframe
2024-09-08 11:38:54,390:INFO:Initializing Decision Tree Classifier
2024-09-08 11:38:54,398:INFO:Total runtime is 0.3061857223510742 minutes
2024-09-08 11:38:54,398:INFO:SubProcess create_model() called ==================================
2024-09-08 11:38:54,398:INFO:Initializing create_model()
2024-09-08 11:38:54,398:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:38:54,398:INFO:Checking exceptions
2024-09-08 11:38:54,398:INFO:Importing libraries
2024-09-08 11:38:54,398:INFO:Copying training dataset
2024-09-08 11:38:54,418:INFO:Defining folds
2024-09-08 11:38:54,418:INFO:Declaring metric variables
2024-09-08 11:38:54,418:INFO:Importing untrained model
2024-09-08 11:38:54,418:INFO:Decision Tree Classifier Imported successfully
2024-09-08 11:38:54,418:INFO:Starting cross validation
2024-09-08 11:38:54,432:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:38:56,361:INFO:Calculating mean and std
2024-09-08 11:38:56,361:INFO:Creating metrics dataframe
2024-09-08 11:38:56,361:INFO:Uploading results into container
2024-09-08 11:38:56,361:INFO:Uploading model into container now
2024-09-08 11:38:56,361:INFO:_master_model_container: 4
2024-09-08 11:38:56,361:INFO:_display_container: 2
2024-09-08 11:38:56,369:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 11:38:56,369:INFO:create_model() successfully completed......................................
2024-09-08 11:38:56,561:INFO:SubProcess create_model() end ==================================
2024-09-08 11:38:56,561:INFO:Creating metrics dataframe
2024-09-08 11:38:56,568:INFO:Initializing SVM - Linear Kernel
2024-09-08 11:38:56,568:INFO:Total runtime is 0.3423532207806905 minutes
2024-09-08 11:38:56,575:INFO:SubProcess create_model() called ==================================
2024-09-08 11:38:56,575:INFO:Initializing create_model()
2024-09-08 11:38:56,575:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:38:56,575:INFO:Checking exceptions
2024-09-08 11:38:56,575:INFO:Importing libraries
2024-09-08 11:38:56,575:INFO:Copying training dataset
2024-09-08 11:38:56,598:INFO:Defining folds
2024-09-08 11:38:56,598:INFO:Declaring metric variables
2024-09-08 11:38:56,598:INFO:Importing untrained model
2024-09-08 11:38:56,598:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 11:38:56,598:INFO:Starting cross validation
2024-09-08 11:38:56,615:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:38:57,849:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:57,873:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:57,873:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:57,890:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:38:57,953:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:57,969:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:38:57,981:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:57,989:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:57,999:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:38:58,030:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:58,063:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:58,606:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:58,618:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:58,640:INFO:Calculating mean and std
2024-09-08 11:38:58,640:INFO:Creating metrics dataframe
2024-09-08 11:38:58,640:INFO:Uploading results into container
2024-09-08 11:38:58,640:INFO:Uploading model into container now
2024-09-08 11:38:58,648:INFO:_master_model_container: 5
2024-09-08 11:38:58,648:INFO:_display_container: 2
2024-09-08 11:38:58,648:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 11:38:58,648:INFO:create_model() successfully completed......................................
2024-09-08 11:38:58,791:INFO:SubProcess create_model() end ==================================
2024-09-08 11:38:58,791:INFO:Creating metrics dataframe
2024-09-08 11:38:58,799:INFO:Initializing Ridge Classifier
2024-09-08 11:38:58,799:INFO:Total runtime is 0.3795246640841166 minutes
2024-09-08 11:38:58,799:INFO:SubProcess create_model() called ==================================
2024-09-08 11:38:58,799:INFO:Initializing create_model()
2024-09-08 11:38:58,799:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:38:58,799:INFO:Checking exceptions
2024-09-08 11:38:58,799:INFO:Importing libraries
2024-09-08 11:38:58,799:INFO:Copying training dataset
2024-09-08 11:38:58,818:INFO:Defining folds
2024-09-08 11:38:58,818:INFO:Declaring metric variables
2024-09-08 11:38:58,818:INFO:Importing untrained model
2024-09-08 11:38:58,818:INFO:Ridge Classifier Imported successfully
2024-09-08 11:38:58,818:INFO:Starting cross validation
2024-09-08 11:38:58,829:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:38:59,834:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:59,834:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:59,854:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:59,862:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:59,879:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:59,888:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:59,893:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:38:59,895:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:00,507:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:00,532:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:00,565:INFO:Calculating mean and std
2024-09-08 11:39:00,565:INFO:Creating metrics dataframe
2024-09-08 11:39:00,569:INFO:Uploading results into container
2024-09-08 11:39:00,569:INFO:Uploading model into container now
2024-09-08 11:39:00,569:INFO:_master_model_container: 6
2024-09-08 11:39:00,569:INFO:_display_container: 2
2024-09-08 11:39:00,569:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 11:39:00,569:INFO:create_model() successfully completed......................................
2024-09-08 11:39:00,709:INFO:SubProcess create_model() end ==================================
2024-09-08 11:39:00,709:INFO:Creating metrics dataframe
2024-09-08 11:39:00,709:INFO:Initializing Random Forest Classifier
2024-09-08 11:39:00,715:INFO:Total runtime is 0.41146646738052367 minutes
2024-09-08 11:39:00,715:INFO:SubProcess create_model() called ==================================
2024-09-08 11:39:00,715:INFO:Initializing create_model()
2024-09-08 11:39:00,715:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:00,715:INFO:Checking exceptions
2024-09-08 11:39:00,715:INFO:Importing libraries
2024-09-08 11:39:00,715:INFO:Copying training dataset
2024-09-08 11:39:00,729:INFO:Defining folds
2024-09-08 11:39:00,729:INFO:Declaring metric variables
2024-09-08 11:39:00,729:INFO:Importing untrained model
2024-09-08 11:39:00,729:INFO:Random Forest Classifier Imported successfully
2024-09-08 11:39:00,732:INFO:Starting cross validation
2024-09-08 11:39:00,732:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:39:03,847:INFO:Calculating mean and std
2024-09-08 11:39:03,849:INFO:Creating metrics dataframe
2024-09-08 11:39:03,849:INFO:Uploading results into container
2024-09-08 11:39:03,849:INFO:Uploading model into container now
2024-09-08 11:39:03,849:INFO:_master_model_container: 7
2024-09-08 11:39:03,849:INFO:_display_container: 2
2024-09-08 11:39:03,849:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 11:39:03,849:INFO:create_model() successfully completed......................................
2024-09-08 11:39:03,990:INFO:SubProcess create_model() end ==================================
2024-09-08 11:39:03,990:INFO:Creating metrics dataframe
2024-09-08 11:39:03,994:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 11:39:03,994:INFO:Total runtime is 0.4661182920138041 minutes
2024-09-08 11:39:03,994:INFO:SubProcess create_model() called ==================================
2024-09-08 11:39:03,994:INFO:Initializing create_model()
2024-09-08 11:39:03,994:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:03,994:INFO:Checking exceptions
2024-09-08 11:39:03,994:INFO:Importing libraries
2024-09-08 11:39:03,994:INFO:Copying training dataset
2024-09-08 11:39:04,009:INFO:Defining folds
2024-09-08 11:39:04,009:INFO:Declaring metric variables
2024-09-08 11:39:04,009:INFO:Importing untrained model
2024-09-08 11:39:04,009:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 11:39:04,009:INFO:Starting cross validation
2024-09-08 11:39:04,014:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:39:05,060:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:39:05,099:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:39:05,113:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:39:05,121:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:39:05,129:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:39:05,149:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:39:05,169:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:39:05,179:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:39:05,324:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:05,352:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:05,361:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:05,370:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:05,386:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:05,394:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:05,419:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:05,424:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:05,840:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:39:05,849:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:39:05,999:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:06,004:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:06,038:INFO:Calculating mean and std
2024-09-08 11:39:06,049:INFO:Creating metrics dataframe
2024-09-08 11:39:06,049:INFO:Uploading results into container
2024-09-08 11:39:06,049:INFO:Uploading model into container now
2024-09-08 11:39:06,054:INFO:_master_model_container: 8
2024-09-08 11:39:06,054:INFO:_display_container: 2
2024-09-08 11:39:06,054:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 11:39:06,054:INFO:create_model() successfully completed......................................
2024-09-08 11:39:06,189:INFO:SubProcess create_model() end ==================================
2024-09-08 11:39:06,189:INFO:Creating metrics dataframe
2024-09-08 11:39:06,199:INFO:Initializing Ada Boost Classifier
2024-09-08 11:39:06,199:INFO:Total runtime is 0.5028644363085428 minutes
2024-09-08 11:39:06,199:INFO:SubProcess create_model() called ==================================
2024-09-08 11:39:06,199:INFO:Initializing create_model()
2024-09-08 11:39:06,199:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:06,199:INFO:Checking exceptions
2024-09-08 11:39:06,199:INFO:Importing libraries
2024-09-08 11:39:06,199:INFO:Copying training dataset
2024-09-08 11:39:06,211:INFO:Defining folds
2024-09-08 11:39:06,211:INFO:Declaring metric variables
2024-09-08 11:39:06,211:INFO:Importing untrained model
2024-09-08 11:39:06,211:INFO:Ada Boost Classifier Imported successfully
2024-09-08 11:39:06,211:INFO:Starting cross validation
2024-09-08 11:39:06,224:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:39:07,004:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:39:07,009:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:39:07,021:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:39:07,049:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:39:07,059:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:39:07,062:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:39:07,063:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:39:07,079:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:39:07,665:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:07,676:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:07,693:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:07,701:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:07,701:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:07,710:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:07,729:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:07,751:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:08,209:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:39:08,224:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:39:08,666:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:08,689:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:08,721:INFO:Calculating mean and std
2024-09-08 11:39:08,721:INFO:Creating metrics dataframe
2024-09-08 11:39:08,721:INFO:Uploading results into container
2024-09-08 11:39:08,721:INFO:Uploading model into container now
2024-09-08 11:39:08,721:INFO:_master_model_container: 9
2024-09-08 11:39:08,721:INFO:_display_container: 2
2024-09-08 11:39:08,721:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 11:39:08,721:INFO:create_model() successfully completed......................................
2024-09-08 11:39:08,860:INFO:SubProcess create_model() end ==================================
2024-09-08 11:39:08,860:INFO:Creating metrics dataframe
2024-09-08 11:39:08,866:INFO:Initializing Gradient Boosting Classifier
2024-09-08 11:39:08,866:INFO:Total runtime is 0.5473075032234191 minutes
2024-09-08 11:39:08,866:INFO:SubProcess create_model() called ==================================
2024-09-08 11:39:08,866:INFO:Initializing create_model()
2024-09-08 11:39:08,866:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:08,866:INFO:Checking exceptions
2024-09-08 11:39:08,866:INFO:Importing libraries
2024-09-08 11:39:08,866:INFO:Copying training dataset
2024-09-08 11:39:08,880:INFO:Defining folds
2024-09-08 11:39:08,880:INFO:Declaring metric variables
2024-09-08 11:39:08,880:INFO:Importing untrained model
2024-09-08 11:39:08,880:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 11:39:08,880:INFO:Starting cross validation
2024-09-08 11:39:08,888:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:39:11,912:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:11,961:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:12,090:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:12,286:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:12,465:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:12,485:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:12,570:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:12,750:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:14,782:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:14,790:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:14,814:INFO:Calculating mean and std
2024-09-08 11:39:14,814:INFO:Creating metrics dataframe
2024-09-08 11:39:14,819:INFO:Uploading results into container
2024-09-08 11:39:14,819:INFO:Uploading model into container now
2024-09-08 11:39:14,822:INFO:_master_model_container: 10
2024-09-08 11:39:14,822:INFO:_display_container: 2
2024-09-08 11:39:14,822:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 11:39:14,822:INFO:create_model() successfully completed......................................
2024-09-08 11:39:14,981:INFO:SubProcess create_model() end ==================================
2024-09-08 11:39:14,981:INFO:Creating metrics dataframe
2024-09-08 11:39:14,981:INFO:Initializing Linear Discriminant Analysis
2024-09-08 11:39:14,981:INFO:Total runtime is 0.6492299278577168 minutes
2024-09-08 11:39:14,989:INFO:SubProcess create_model() called ==================================
2024-09-08 11:39:14,990:INFO:Initializing create_model()
2024-09-08 11:39:14,990:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:14,990:INFO:Checking exceptions
2024-09-08 11:39:14,990:INFO:Importing libraries
2024-09-08 11:39:14,990:INFO:Copying training dataset
2024-09-08 11:39:15,005:INFO:Defining folds
2024-09-08 11:39:15,005:INFO:Declaring metric variables
2024-09-08 11:39:15,005:INFO:Importing untrained model
2024-09-08 11:39:15,005:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 11:39:15,005:INFO:Starting cross validation
2024-09-08 11:39:15,013:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:39:16,111:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:16,125:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:16,137:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:16,153:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:16,153:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:16,186:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:16,194:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:16,800:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:16,813:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:16,848:INFO:Calculating mean and std
2024-09-08 11:39:16,850:INFO:Creating metrics dataframe
2024-09-08 11:39:16,850:INFO:Uploading results into container
2024-09-08 11:39:16,850:INFO:Uploading model into container now
2024-09-08 11:39:16,850:INFO:_master_model_container: 11
2024-09-08 11:39:16,850:INFO:_display_container: 2
2024-09-08 11:39:16,857:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 11:39:16,857:INFO:create_model() successfully completed......................................
2024-09-08 11:39:16,990:INFO:SubProcess create_model() end ==================================
2024-09-08 11:39:16,990:INFO:Creating metrics dataframe
2024-09-08 11:39:16,997:INFO:Initializing Extra Trees Classifier
2024-09-08 11:39:16,997:INFO:Total runtime is 0.6828256368637085 minutes
2024-09-08 11:39:17,000:INFO:SubProcess create_model() called ==================================
2024-09-08 11:39:17,000:INFO:Initializing create_model()
2024-09-08 11:39:17,000:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:17,000:INFO:Checking exceptions
2024-09-08 11:39:17,000:INFO:Importing libraries
2024-09-08 11:39:17,000:INFO:Copying training dataset
2024-09-08 11:39:17,014:INFO:Defining folds
2024-09-08 11:39:17,014:INFO:Declaring metric variables
2024-09-08 11:39:17,014:INFO:Importing untrained model
2024-09-08 11:39:17,014:INFO:Extra Trees Classifier Imported successfully
2024-09-08 11:39:17,014:INFO:Starting cross validation
2024-09-08 11:39:17,024:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:39:20,070:INFO:Calculating mean and std
2024-09-08 11:39:20,070:INFO:Creating metrics dataframe
2024-09-08 11:39:20,080:INFO:Uploading results into container
2024-09-08 11:39:20,080:INFO:Uploading model into container now
2024-09-08 11:39:20,080:INFO:_master_model_container: 12
2024-09-08 11:39:20,080:INFO:_display_container: 2
2024-09-08 11:39:20,080:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 11:39:20,080:INFO:create_model() successfully completed......................................
2024-09-08 11:39:20,225:INFO:SubProcess create_model() end ==================================
2024-09-08 11:39:20,225:INFO:Creating metrics dataframe
2024-09-08 11:39:20,234:INFO:Initializing Extreme Gradient Boosting
2024-09-08 11:39:20,234:INFO:Total runtime is 0.7367748936017354 minutes
2024-09-08 11:39:20,234:INFO:SubProcess create_model() called ==================================
2024-09-08 11:39:20,234:INFO:Initializing create_model()
2024-09-08 11:39:20,234:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:20,234:INFO:Checking exceptions
2024-09-08 11:39:20,234:INFO:Importing libraries
2024-09-08 11:39:20,234:INFO:Copying training dataset
2024-09-08 11:39:20,242:INFO:Defining folds
2024-09-08 11:39:20,242:INFO:Declaring metric variables
2024-09-08 11:39:20,242:INFO:Importing untrained model
2024-09-08 11:39:20,242:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:39:20,250:INFO:Starting cross validation
2024-09-08 11:39:20,255:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:39:23,888:INFO:Calculating mean and std
2024-09-08 11:39:23,888:INFO:Creating metrics dataframe
2024-09-08 11:39:23,888:INFO:Uploading results into container
2024-09-08 11:39:23,888:INFO:Uploading model into container now
2024-09-08 11:39:23,894:INFO:_master_model_container: 13
2024-09-08 11:39:23,894:INFO:_display_container: 2
2024-09-08 11:39:23,894:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 11:39:23,894:INFO:create_model() successfully completed......................................
2024-09-08 11:39:24,028:INFO:SubProcess create_model() end ==================================
2024-09-08 11:39:24,028:INFO:Creating metrics dataframe
2024-09-08 11:39:24,033:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 11:39:24,033:INFO:Total runtime is 0.8001022537549337 minutes
2024-09-08 11:39:24,033:INFO:SubProcess create_model() called ==================================
2024-09-08 11:39:24,033:INFO:Initializing create_model()
2024-09-08 11:39:24,033:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:24,033:INFO:Checking exceptions
2024-09-08 11:39:24,033:INFO:Importing libraries
2024-09-08 11:39:24,033:INFO:Copying training dataset
2024-09-08 11:39:24,044:INFO:Defining folds
2024-09-08 11:39:24,044:INFO:Declaring metric variables
2024-09-08 11:39:24,044:INFO:Importing untrained model
2024-09-08 11:39:24,044:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:39:24,044:INFO:Starting cross validation
2024-09-08 11:39:24,053:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:39:32,159:INFO:Calculating mean and std
2024-09-08 11:39:32,161:INFO:Creating metrics dataframe
2024-09-08 11:39:32,166:INFO:Uploading results into container
2024-09-08 11:39:32,166:INFO:Uploading model into container now
2024-09-08 11:39:32,166:INFO:_master_model_container: 14
2024-09-08 11:39:32,166:INFO:_display_container: 2
2024-09-08 11:39:32,166:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:39:32,166:INFO:create_model() successfully completed......................................
2024-09-08 11:39:32,326:INFO:SubProcess create_model() end ==================================
2024-09-08 11:39:32,326:INFO:Creating metrics dataframe
2024-09-08 11:39:32,331:INFO:Initializing Dummy Classifier
2024-09-08 11:39:32,331:INFO:Total runtime is 0.9383970300356548 minutes
2024-09-08 11:39:32,331:INFO:SubProcess create_model() called ==================================
2024-09-08 11:39:32,331:INFO:Initializing create_model()
2024-09-08 11:39:32,331:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF80B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:32,331:INFO:Checking exceptions
2024-09-08 11:39:32,331:INFO:Importing libraries
2024-09-08 11:39:32,331:INFO:Copying training dataset
2024-09-08 11:39:32,341:INFO:Defining folds
2024-09-08 11:39:32,341:INFO:Declaring metric variables
2024-09-08 11:39:32,341:INFO:Importing untrained model
2024-09-08 11:39:32,341:INFO:Dummy Classifier Imported successfully
2024-09-08 11:39:32,341:INFO:Starting cross validation
2024-09-08 11:39:32,352:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:39:33,378:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:39:33,403:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:39:33,403:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:39:33,421:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:39:33,471:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:39:33,486:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:39:33,491:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:39:33,491:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:39:34,018:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:39:34,026:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:39:34,051:INFO:Calculating mean and std
2024-09-08 11:39:34,051:INFO:Creating metrics dataframe
2024-09-08 11:39:34,051:INFO:Uploading results into container
2024-09-08 11:39:34,051:INFO:Uploading model into container now
2024-09-08 11:39:34,051:INFO:_master_model_container: 15
2024-09-08 11:39:34,051:INFO:_display_container: 2
2024-09-08 11:39:34,051:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 11:39:34,051:INFO:create_model() successfully completed......................................
2024-09-08 11:39:34,208:INFO:SubProcess create_model() end ==================================
2024-09-08 11:39:34,208:INFO:Creating metrics dataframe
2024-09-08 11:39:34,211:INFO:Initializing create_model()
2024-09-08 11:39:34,211:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:34,211:INFO:Checking exceptions
2024-09-08 11:39:34,211:INFO:Importing libraries
2024-09-08 11:39:34,211:INFO:Copying training dataset
2024-09-08 11:39:34,231:INFO:Defining folds
2024-09-08 11:39:34,231:INFO:Declaring metric variables
2024-09-08 11:39:34,231:INFO:Importing untrained model
2024-09-08 11:39:34,231:INFO:Declaring custom model
2024-09-08 11:39:34,236:INFO:Random Forest Classifier Imported successfully
2024-09-08 11:39:34,248:INFO:Cross validation set to False
2024-09-08 11:39:34,248:INFO:Fitting Model
2024-09-08 11:39:34,935:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 11:39:34,935:INFO:create_model() successfully completed......................................
2024-09-08 11:39:35,071:INFO:Initializing create_model()
2024-09-08 11:39:35,078:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:35,078:INFO:Checking exceptions
2024-09-08 11:39:35,078:INFO:Importing libraries
2024-09-08 11:39:35,078:INFO:Copying training dataset
2024-09-08 11:39:35,091:INFO:Defining folds
2024-09-08 11:39:35,091:INFO:Declaring metric variables
2024-09-08 11:39:35,091:INFO:Importing untrained model
2024-09-08 11:39:35,091:INFO:Declaring custom model
2024-09-08 11:39:35,094:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:39:35,101:INFO:Cross validation set to False
2024-09-08 11:39:35,101:INFO:Fitting Model
2024-09-08 11:39:35,461:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 11:39:35,461:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000599 seconds.
2024-09-08 11:39:35,461:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 11:39:35,461:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 11:39:35,461:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 11:39:35,461:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 11:39:35,466:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 11:39:35,466:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 11:39:35,466:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 11:39:35,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:35,792:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:39:35,792:INFO:create_model() successfully completed......................................
2024-09-08 11:39:35,943:INFO:Initializing create_model()
2024-09-08 11:39:35,943:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:35,943:INFO:Checking exceptions
2024-09-08 11:39:35,943:INFO:Importing libraries
2024-09-08 11:39:35,943:INFO:Copying training dataset
2024-09-08 11:39:35,961:INFO:Defining folds
2024-09-08 11:39:35,961:INFO:Declaring metric variables
2024-09-08 11:39:35,961:INFO:Importing untrained model
2024-09-08 11:39:35,961:INFO:Declaring custom model
2024-09-08 11:39:35,961:INFO:Extra Trees Classifier Imported successfully
2024-09-08 11:39:35,971:INFO:Cross validation set to False
2024-09-08 11:39:35,971:INFO:Fitting Model
2024-09-08 11:39:36,546:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 11:39:36,546:INFO:create_model() successfully completed......................................
2024-09-08 11:39:36,701:INFO:_master_model_container: 15
2024-09-08 11:39:36,701:INFO:_display_container: 2
2024-09-08 11:39:36,701:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)]
2024-09-08 11:39:36,701:INFO:compare_models() successfully completed......................................
2024-09-08 11:39:36,701:INFO:Initializing create_model()
2024-09-08 11:39:36,701:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:36,701:INFO:Checking exceptions
2024-09-08 11:39:36,701:INFO:Importing libraries
2024-09-08 11:39:36,701:INFO:Copying training dataset
2024-09-08 11:39:36,728:INFO:Defining folds
2024-09-08 11:39:36,728:INFO:Declaring metric variables
2024-09-08 11:39:36,728:INFO:Importing untrained model
2024-09-08 11:39:36,728:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:39:36,728:INFO:Starting cross validation
2024-09-08 11:39:36,735:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:39:43,627:INFO:Calculating mean and std
2024-09-08 11:39:43,627:INFO:Creating metrics dataframe
2024-09-08 11:39:43,632:INFO:Finalizing model
2024-09-08 11:39:44,088:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 11:39:44,088:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000524 seconds.
2024-09-08 11:39:44,088:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 11:39:44,088:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 11:39:44,088:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 11:39:44,088:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 11:39:44,092:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 11:39:44,092:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 11:39:44,092:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 11:39:44,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:39:44,532:INFO:Uploading results into container
2024-09-08 11:39:44,537:INFO:Uploading model into container now
2024-09-08 11:39:44,564:INFO:_master_model_container: 16
2024-09-08 11:39:44,564:INFO:_display_container: 3
2024-09-08 11:39:44,564:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:39:44,566:INFO:create_model() successfully completed......................................
2024-09-08 11:39:44,717:INFO:Initializing create_model()
2024-09-08 11:39:44,717:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:44,717:INFO:Checking exceptions
2024-09-08 11:39:44,722:INFO:Importing libraries
2024-09-08 11:39:44,722:INFO:Copying training dataset
2024-09-08 11:39:44,742:INFO:Defining folds
2024-09-08 11:39:44,742:INFO:Declaring metric variables
2024-09-08 11:39:44,742:INFO:Importing untrained model
2024-09-08 11:39:44,742:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:39:44,742:INFO:Starting cross validation
2024-09-08 11:39:44,753:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:39:48,288:INFO:Calculating mean and std
2024-09-08 11:39:48,288:INFO:Creating metrics dataframe
2024-09-08 11:39:48,292:INFO:Finalizing model
2024-09-08 11:39:49,082:INFO:Uploading results into container
2024-09-08 11:39:49,087:INFO:Uploading model into container now
2024-09-08 11:39:49,107:INFO:_master_model_container: 17
2024-09-08 11:39:49,109:INFO:_display_container: 4
2024-09-08 11:39:49,109:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:39:49,109:INFO:create_model() successfully completed......................................
2024-09-08 11:39:49,257:INFO:Initializing create_model()
2024-09-08 11:39:49,257:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:39:49,257:INFO:Checking exceptions
2024-09-08 11:39:49,257:INFO:Importing libraries
2024-09-08 11:39:49,257:INFO:Copying training dataset
2024-09-08 11:39:49,272:INFO:Defining folds
2024-09-08 11:39:49,272:INFO:Declaring metric variables
2024-09-08 11:39:49,272:INFO:Importing untrained model
2024-09-08 11:39:49,272:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 11:39:49,272:INFO:Starting cross validation
2024-09-08 11:39:49,282:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:39:52,373:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:52,428:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:52,452:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:52,488:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:52,507:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:52,507:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:52,520:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:52,543:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:54,599:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:54,658:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:39:54,678:INFO:Calculating mean and std
2024-09-08 11:39:54,679:INFO:Creating metrics dataframe
2024-09-08 11:39:54,679:INFO:Finalizing model
2024-09-08 11:39:56,753:INFO:Uploading results into container
2024-09-08 11:39:56,753:INFO:Uploading model into container now
2024-09-08 11:39:56,773:INFO:_master_model_container: 18
2024-09-08 11:39:56,773:INFO:_display_container: 5
2024-09-08 11:39:56,773:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 11:39:56,773:INFO:create_model() successfully completed......................................
2024-09-08 11:39:56,922:INFO:Initializing tune_model()
2024-09-08 11:39:56,922:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>)
2024-09-08 11:39:56,922:INFO:Checking exceptions
2024-09-08 11:39:56,922:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 11:39:56,998:INFO:Copying training dataset
2024-09-08 11:39:57,008:INFO:Checking base model
2024-09-08 11:39:57,008:INFO:Base model : Extreme Gradient Boosting
2024-09-08 11:39:57,008:INFO:Declaring metric variables
2024-09-08 11:39:57,008:INFO:Defining Hyperparameters
2024-09-08 11:39:57,153:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 11:39:57,153:INFO:Tuning with n_jobs=-1
2024-09-08 11:39:57,161:INFO:Initializing skopt.BayesSearchCV
2024-09-08 11:40:11,774:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.6), ('actual_estimator__gamma', 0), ('actual_estimator__learning_rate', 0.01), ('actual_estimator__max_depth', 7), ('actual_estimator__min_child_weight', 1), ('actual_estimator__n_estimators', 100), ('actual_estimator__subsample', 0.6)])
2024-09-08 11:40:11,779:INFO:Hyperparameter search completed
2024-09-08 11:40:11,779:INFO:SubProcess create_model() called ==================================
2024-09-08 11:40:11,780:INFO:Initializing create_model()
2024-09-08 11:40:11,780:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000205BF716350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.6})
2024-09-08 11:40:11,780:INFO:Checking exceptions
2024-09-08 11:40:11,780:INFO:Importing libraries
2024-09-08 11:40:11,780:INFO:Copying training dataset
2024-09-08 11:40:11,794:INFO:Defining folds
2024-09-08 11:40:11,794:INFO:Declaring metric variables
2024-09-08 11:40:11,796:INFO:Importing untrained model
2024-09-08 11:40:11,796:INFO:Declaring custom model
2024-09-08 11:40:11,796:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:40:11,796:INFO:Starting cross validation
2024-09-08 11:40:11,804:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:40:13,511:INFO:Calculating mean and std
2024-09-08 11:40:13,511:INFO:Creating metrics dataframe
2024-09-08 11:40:13,514:INFO:Finalizing model
2024-09-08 11:40:14,554:INFO:Uploading results into container
2024-09-08 11:40:14,554:INFO:Uploading model into container now
2024-09-08 11:40:14,554:INFO:_master_model_container: 19
2024-09-08 11:40:14,554:INFO:_display_container: 6
2024-09-08 11:40:14,559:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:40:14,559:INFO:create_model() successfully completed......................................
2024-09-08 11:40:14,714:INFO:SubProcess create_model() end ==================================
2024-09-08 11:40:14,714:INFO:choose_better activated
2024-09-08 11:40:14,714:INFO:SubProcess create_model() called ==================================
2024-09-08 11:40:14,714:INFO:Initializing create_model()
2024-09-08 11:40:14,714:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:40:14,714:INFO:Checking exceptions
2024-09-08 11:40:14,714:INFO:Importing libraries
2024-09-08 11:40:14,714:INFO:Copying training dataset
2024-09-08 11:40:14,728:INFO:Defining folds
2024-09-08 11:40:14,728:INFO:Declaring metric variables
2024-09-08 11:40:14,728:INFO:Importing untrained model
2024-09-08 11:40:14,728:INFO:Declaring custom model
2024-09-08 11:40:14,734:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:40:14,734:INFO:Starting cross validation
2024-09-08 11:40:14,739:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:40:16,418:INFO:Calculating mean and std
2024-09-08 11:40:16,418:INFO:Creating metrics dataframe
2024-09-08 11:40:16,423:INFO:Finalizing model
2024-09-08 11:40:17,344:INFO:Uploading results into container
2024-09-08 11:40:17,346:INFO:Uploading model into container now
2024-09-08 11:40:17,346:INFO:_master_model_container: 20
2024-09-08 11:40:17,346:INFO:_display_container: 7
2024-09-08 11:40:17,346:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:40:17,346:INFO:create_model() successfully completed......................................
2024-09-08 11:40:17,564:INFO:SubProcess create_model() end ==================================
2024-09-08 11:40:17,564:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8186
2024-09-08 11:40:17,569:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8386
2024-09-08 11:40:17,569:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 11:40:17,569:INFO:choose_better completed
2024-09-08 11:40:17,604:INFO:_master_model_container: 20
2024-09-08 11:40:17,604:INFO:_display_container: 6
2024-09-08 11:40:17,604:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:40:17,604:INFO:tune_model() successfully completed......................................
2024-09-08 11:40:17,769:INFO:Initializing predict_model()
2024-09-08 11:40:17,769:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000205A8CF1480>)
2024-09-08 11:40:17,769:INFO:Checking exceptions
2024-09-08 11:40:17,769:INFO:Preloading libraries
2024-09-08 11:40:18,399:INFO:Initializing get_config()
2024-09-08 11:40:18,399:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, variable=X_train)
2024-09-08 11:40:18,399:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 11:40:18,474:INFO:Variable:  returned as       id Marital status Application mode Application order  ... Curricular units 2nd sem (without evaluations) Unemployment rate Inflation rate   GDP
765  765              1                1                 6  ...                                              0         10.800000            1.4  1.74
323  323              1               17                 1  ...                                              0          7.600000            2.6  0.32
909  909              1                1                 1  ...                                              0         12.400000            0.5  1.79
774  774              1               17                 1  ...                                              0          8.900000            1.4  3.51
931  931              1               39                 1  ...                                              0         16.200001            0.3 -0.92
..   ...            ...              ...               ...  ...                                            ...               ...            ...   ...
806  806              4               39                 1  ...                                              0         10.800000            1.4  1.74
492  492              1                1                 3  ...                                              0          7.600000            2.6  0.32
698  698              1               39                 1  ...                                              1         10.800000            1.4  1.74
44    44              1                1                 1  ...                                              0          9.400000           -0.8 -3.12
604  604              1                1                 1  ...                                              0         10.800000            1.4  1.74

[700 rows x 37 columns]
2024-09-08 11:40:18,474:INFO:get_config() successfully completed......................................
2024-09-08 11:40:18,474:INFO:Initializing predict_model()
2024-09-08 11:40:18,474:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000205BF7DDC60>)
2024-09-08 11:40:18,474:INFO:Checking exceptions
2024-09-08 11:40:18,474:INFO:Preloading libraries
2024-09-08 11:40:18,474:INFO:Set up data.
2024-09-08 11:40:18,489:INFO:Set up index.
2024-09-08 11:40:18,924:INFO:Initializing get_config()
2024-09-08 11:40:18,924:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, variable=y_train)
2024-09-08 11:40:18,924:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 11:40:18,932:INFO:Variable:  returned as 765    0
323    1
909    0
774    0
931    1
      ..
806    1
492    0
698    1
44     0
604    1
Name: Target, Length: 700, dtype: int8
2024-09-08 11:40:18,932:INFO:get_config() successfully completed......................................
2024-09-08 11:40:18,932:INFO:Initializing get_config()
2024-09-08 11:40:18,932:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, variable=y_test)
2024-09-08 11:40:18,932:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 11:40:18,939:INFO:Variable:  returned as 47     1
233    1
132    2
34     0
162    0
      ..
772    0
632    0
365    2
136    0
299    1
Name: Target, Length: 300, dtype: int8
2024-09-08 11:40:18,939:INFO:get_config() successfully completed......................................
2024-09-08 11:40:18,939:INFO:Initializing finalize_model()
2024-09-08 11:40:18,939:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 11:40:18,939:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:40:18,954:INFO:Initializing create_model()
2024-09-08 11:40:18,954:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:40:18,954:INFO:Checking exceptions
2024-09-08 11:40:18,954:INFO:Importing libraries
2024-09-08 11:40:18,954:INFO:Copying training dataset
2024-09-08 11:40:18,954:INFO:Defining folds
2024-09-08 11:40:18,954:INFO:Declaring metric variables
2024-09-08 11:40:18,954:INFO:Importing untrained model
2024-09-08 11:40:18,954:INFO:Declaring custom model
2024-09-08 11:40:18,959:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:40:18,965:INFO:Cross validation set to False
2024-09-08 11:40:18,965:INFO:Fitting Model
2024-09-08 11:40:20,230:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 11:40:20,230:INFO:create_model() successfully completed......................................
2024-09-08 11:40:20,366:INFO:_master_model_container: 20
2024-09-08 11:40:20,366:INFO:_display_container: 7
2024-09-08 11:40:20,635:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 11:40:20,635:INFO:finalize_model() successfully completed......................................
2024-09-08 11:40:21,119:INFO:Initializing predict_model()
2024-09-08 11:40:21,119:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000205BD847400>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000205BFB14E50>)
2024-09-08 11:40:21,120:INFO:Checking exceptions
2024-09-08 11:40:21,120:INFO:Preloading libraries
2024-09-08 11:40:21,120:INFO:Set up data.
2024-09-08 11:40:21,145:INFO:Set up index.
2024-09-08 11:42:13,393:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:42:13,398:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:42:13,398:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:42:13,398:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:42:42,420:INFO:PyCaret ClassificationExperiment
2024-09-08 11:42:42,423:INFO:Logging name: clf-default-name
2024-09-08 11:42:42,425:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 11:42:42,425:INFO:version 3.3.2
2024-09-08 11:42:42,425:INFO:Initializing setup()
2024-09-08 11:42:42,425:INFO:self.USI: 5800
2024-09-08 11:42:42,425:INFO:self._variable_keys: {'fold_shuffle_param', 'pipeline', 'fix_imbalance', '_ml_usecase', 'n_jobs_param', 'exp_id', 'USI', 'X', 'html_param', 'is_multiclass', 'gpu_param', 'X_train', 'seed', 'y', 'target_param', 'logging_param', 'data', 'log_plots_param', 'fold_generator', '_available_plots', 'memory', 'y_train', 'y_test', 'exp_name_log', 'gpu_n_jobs_param', 'idx', 'X_test', 'fold_groups_param'}
2024-09-08 11:42:42,425:INFO:Checking environment
2024-09-08 11:42:42,430:INFO:python_version: 3.10.11
2024-09-08 11:42:42,430:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 11:42:42,431:INFO:machine: AMD64
2024-09-08 11:42:42,448:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 11:42:42,448:INFO:Memory: svmem(total=16407719936, available=6030176256, percent=63.2, used=10377543680, free=6030176256)
2024-09-08 11:42:42,448:INFO:Physical Core: 4
2024-09-08 11:42:42,448:INFO:Logical Core: 8
2024-09-08 11:42:42,448:INFO:Checking libraries
2024-09-08 11:42:42,448:INFO:System:
2024-09-08 11:42:42,448:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 11:42:42,448:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 11:42:42,448:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 11:42:42,448:INFO:PyCaret required dependencies:
2024-09-08 11:42:42,555:INFO:                 pip: 24.2
2024-09-08 11:42:42,555:INFO:          setuptools: 72.1.0
2024-09-08 11:42:42,555:INFO:             pycaret: 3.3.2
2024-09-08 11:42:42,555:INFO:             IPython: 8.25.0
2024-09-08 11:42:42,555:INFO:          ipywidgets: 8.1.5
2024-09-08 11:42:42,555:INFO:                tqdm: 4.66.5
2024-09-08 11:42:42,560:INFO:               numpy: 1.26.4
2024-09-08 11:42:42,560:INFO:              pandas: 2.1.4
2024-09-08 11:42:42,560:INFO:              jinja2: 3.1.4
2024-09-08 11:42:42,561:INFO:               scipy: 1.11.4
2024-09-08 11:42:42,561:INFO:              joblib: 1.3.2
2024-09-08 11:42:42,561:INFO:             sklearn: 1.4.2
2024-09-08 11:42:42,561:INFO:                pyod: 2.0.1
2024-09-08 11:42:42,561:INFO:            imblearn: 0.12.3
2024-09-08 11:42:42,561:INFO:   category_encoders: 2.6.3
2024-09-08 11:42:42,561:INFO:            lightgbm: 4.5.0
2024-09-08 11:42:42,561:INFO:               numba: 0.60.0
2024-09-08 11:42:42,564:INFO:            requests: 2.32.3
2024-09-08 11:42:42,564:INFO:          matplotlib: 3.7.5
2024-09-08 11:42:42,565:INFO:          scikitplot: 0.3.7
2024-09-08 11:42:42,565:INFO:         yellowbrick: 1.5
2024-09-08 11:42:42,565:INFO:              plotly: 5.24.0
2024-09-08 11:42:42,565:INFO:    plotly-resampler: Not installed
2024-09-08 11:42:42,565:INFO:             kaleido: 0.2.1
2024-09-08 11:42:42,565:INFO:           schemdraw: 0.15
2024-09-08 11:42:42,565:INFO:         statsmodels: 0.14.2
2024-09-08 11:42:42,565:INFO:              sktime: 0.26.0
2024-09-08 11:42:42,565:INFO:               tbats: 1.1.3
2024-09-08 11:42:42,565:INFO:            pmdarima: 2.0.4
2024-09-08 11:42:42,565:INFO:              psutil: 5.9.0
2024-09-08 11:42:42,565:INFO:          markupsafe: 2.1.3
2024-09-08 11:42:42,565:INFO:             pickle5: Not installed
2024-09-08 11:42:42,565:INFO:         cloudpickle: 3.0.0
2024-09-08 11:42:42,565:INFO:         deprecation: 2.1.0
2024-09-08 11:42:42,565:INFO:              xxhash: 3.5.0
2024-09-08 11:42:42,572:INFO:           wurlitzer: Not installed
2024-09-08 11:42:42,572:INFO:PyCaret optional dependencies:
2024-09-08 11:42:42,596:INFO:                shap: Not installed
2024-09-08 11:42:42,596:INFO:           interpret: Not installed
2024-09-08 11:42:42,596:INFO:                umap: Not installed
2024-09-08 11:42:42,596:INFO:     ydata_profiling: Not installed
2024-09-08 11:42:42,596:INFO:  explainerdashboard: Not installed
2024-09-08 11:42:42,596:INFO:             autoviz: Not installed
2024-09-08 11:42:42,596:INFO:           fairlearn: Not installed
2024-09-08 11:42:42,596:INFO:          deepchecks: Not installed
2024-09-08 11:42:42,596:INFO:             xgboost: 2.1.1
2024-09-08 11:42:42,596:INFO:            catboost: Not installed
2024-09-08 11:42:42,596:INFO:              kmodes: Not installed
2024-09-08 11:42:42,596:INFO:             mlxtend: Not installed
2024-09-08 11:42:42,596:INFO:       statsforecast: Not installed
2024-09-08 11:42:42,596:INFO:        tune_sklearn: Not installed
2024-09-08 11:42:42,596:INFO:                 ray: Not installed
2024-09-08 11:42:42,596:INFO:            hyperopt: 0.2.7
2024-09-08 11:42:42,596:INFO:              optuna: 4.0.0
2024-09-08 11:42:42,596:INFO:               skopt: 0.10.2
2024-09-08 11:42:42,596:INFO:              mlflow: Not installed
2024-09-08 11:42:42,596:INFO:              gradio: Not installed
2024-09-08 11:42:42,596:INFO:             fastapi: Not installed
2024-09-08 11:42:42,596:INFO:             uvicorn: Not installed
2024-09-08 11:42:42,596:INFO:              m2cgen: Not installed
2024-09-08 11:42:42,596:INFO:           evidently: Not installed
2024-09-08 11:42:42,596:INFO:               fugue: Not installed
2024-09-08 11:42:42,596:INFO:           streamlit: 1.38.0
2024-09-08 11:42:42,596:INFO:             prophet: Not installed
2024-09-08 11:42:42,596:INFO:None
2024-09-08 11:42:42,596:INFO:Set up data.
2024-09-08 11:42:42,625:INFO:Set up folding strategy.
2024-09-08 11:42:42,625:INFO:Set up train/test split.
2024-09-08 11:42:42,639:INFO:Set up index.
2024-09-08 11:42:42,645:INFO:Assigning column types.
2024-09-08 11:42:42,647:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 11:42:42,738:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 11:42:42,747:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:42:42,805:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:42:42,813:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:42:42,895:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 11:42:42,902:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:42:42,972:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:42:42,978:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:42:42,984:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 11:42:43,075:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:42:43,132:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:42:43,135:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:42:43,225:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:42:43,280:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:42:43,288:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:42:43,288:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 11:42:43,436:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:42:43,445:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:42:43,585:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:42:43,590:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:42:43,595:INFO:Preparing preprocessing pipeline...
2024-09-08 11:42:43,598:INFO:Set up simple imputation.
2024-09-08 11:42:43,607:INFO:Set up encoding of ordinal features.
2024-09-08 11:42:43,635:INFO:Set up encoding of categorical features.
2024-09-08 11:42:43,640:INFO:Set up column name cleaning.
2024-09-08 11:42:44,028:INFO:Finished creating preprocessing pipeline.
2024-09-08 11:42:44,298:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                                                    'qualification',
                                                                    'Fathers '
                                                                    'qualification',
                                                                    'Mothers '
                                                                    'occupation',
                                                                    'Fathers '
                                                                    'occupation'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 11:42:44,298:INFO:Creating final display dataframe.
2024-09-08 11:42:44,745:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (1000, 38)
4        Transformed data shape       (1000, 129)
5   Transformed train set shape        (700, 129)
6    Transformed test set shape        (300, 129)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              5800
2024-09-08 11:42:44,908:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:42:44,908:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:42:45,057:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:42:45,066:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:42:45,066:INFO:setup() successfully completed in 2.68s...............
2024-09-08 11:42:45,066:INFO:Initializing compare_models()
2024-09-08 11:42:45,066:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 11:42:45,066:INFO:Checking exceptions
2024-09-08 11:42:45,075:INFO:Preparing display monitor
2024-09-08 11:42:45,082:INFO:Initializing Logistic Regression
2024-09-08 11:42:45,082:INFO:Total runtime is 0.0 minutes
2024-09-08 11:42:45,082:INFO:SubProcess create_model() called ==================================
2024-09-08 11:42:45,082:INFO:Initializing create_model()
2024-09-08 11:42:45,082:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:42:45,082:INFO:Checking exceptions
2024-09-08 11:42:45,085:INFO:Importing libraries
2024-09-08 11:42:45,085:INFO:Copying training dataset
2024-09-08 11:42:45,099:INFO:Defining folds
2024-09-08 11:42:45,099:INFO:Declaring metric variables
2024-09-08 11:42:45,099:INFO:Importing untrained model
2024-09-08 11:42:45,099:INFO:Logistic Regression Imported successfully
2024-09-08 11:42:45,099:INFO:Starting cross validation
2024-09-08 11:42:45,107:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:42:55,721:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:42:55,747:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:42:56,006:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:42:56,036:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:42:56,063:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:42:56,080:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:42:56,337:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:42:56,346:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:42:56,406:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:42:56,518:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:42:56,518:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:42:56,587:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:42:56,661:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:42:56,766:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:42:56,768:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:42:57,891:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:42:57,916:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:42:58,052:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:42:58,086:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:42:58,118:INFO:Calculating mean and std
2024-09-08 11:42:58,118:INFO:Creating metrics dataframe
2024-09-08 11:42:58,126:INFO:Uploading results into container
2024-09-08 11:42:58,126:INFO:Uploading model into container now
2024-09-08 11:42:58,126:INFO:_master_model_container: 1
2024-09-08 11:42:58,126:INFO:_display_container: 2
2024-09-08 11:42:58,126:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 11:42:58,126:INFO:create_model() successfully completed......................................
2024-09-08 11:42:58,316:INFO:SubProcess create_model() end ==================================
2024-09-08 11:42:58,316:INFO:Creating metrics dataframe
2024-09-08 11:42:58,316:INFO:Initializing K Neighbors Classifier
2024-09-08 11:42:58,316:INFO:Total runtime is 0.22056977748870848 minutes
2024-09-08 11:42:58,316:INFO:SubProcess create_model() called ==================================
2024-09-08 11:42:58,316:INFO:Initializing create_model()
2024-09-08 11:42:58,316:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:42:58,324:INFO:Checking exceptions
2024-09-08 11:42:58,324:INFO:Importing libraries
2024-09-08 11:42:58,324:INFO:Copying training dataset
2024-09-08 11:42:58,340:INFO:Defining folds
2024-09-08 11:42:58,340:INFO:Declaring metric variables
2024-09-08 11:42:58,340:INFO:Importing untrained model
2024-09-08 11:42:58,340:INFO:K Neighbors Classifier Imported successfully
2024-09-08 11:42:58,346:INFO:Starting cross validation
2024-09-08 11:42:58,356:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:00,847:INFO:Calculating mean and std
2024-09-08 11:43:00,847:INFO:Creating metrics dataframe
2024-09-08 11:43:00,847:INFO:Uploading results into container
2024-09-08 11:43:00,847:INFO:Uploading model into container now
2024-09-08 11:43:00,854:INFO:_master_model_container: 2
2024-09-08 11:43:00,854:INFO:_display_container: 2
2024-09-08 11:43:00,854:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 11:43:00,854:INFO:create_model() successfully completed......................................
2024-09-08 11:43:01,002:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:01,002:INFO:Creating metrics dataframe
2024-09-08 11:43:01,007:INFO:Initializing Naive Bayes
2024-09-08 11:43:01,007:INFO:Total runtime is 0.26541295448939006 minutes
2024-09-08 11:43:01,007:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:01,007:INFO:Initializing create_model()
2024-09-08 11:43:01,007:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:01,007:INFO:Checking exceptions
2024-09-08 11:43:01,007:INFO:Importing libraries
2024-09-08 11:43:01,007:INFO:Copying training dataset
2024-09-08 11:43:01,019:INFO:Defining folds
2024-09-08 11:43:01,019:INFO:Declaring metric variables
2024-09-08 11:43:01,019:INFO:Importing untrained model
2024-09-08 11:43:01,019:INFO:Naive Bayes Imported successfully
2024-09-08 11:43:01,019:INFO:Starting cross validation
2024-09-08 11:43:01,031:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:02,580:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:03,254:INFO:Calculating mean and std
2024-09-08 11:43:03,256:INFO:Creating metrics dataframe
2024-09-08 11:43:03,257:INFO:Uploading results into container
2024-09-08 11:43:03,257:INFO:Uploading model into container now
2024-09-08 11:43:03,257:INFO:_master_model_container: 3
2024-09-08 11:43:03,257:INFO:_display_container: 2
2024-09-08 11:43:03,257:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 11:43:03,257:INFO:create_model() successfully completed......................................
2024-09-08 11:43:03,395:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:03,395:INFO:Creating metrics dataframe
2024-09-08 11:43:03,397:INFO:Initializing Decision Tree Classifier
2024-09-08 11:43:03,397:INFO:Total runtime is 0.30524592399597167 minutes
2024-09-08 11:43:03,397:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:03,401:INFO:Initializing create_model()
2024-09-08 11:43:03,401:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:03,401:INFO:Checking exceptions
2024-09-08 11:43:03,401:INFO:Importing libraries
2024-09-08 11:43:03,401:INFO:Copying training dataset
2024-09-08 11:43:03,417:INFO:Defining folds
2024-09-08 11:43:03,417:INFO:Declaring metric variables
2024-09-08 11:43:03,417:INFO:Importing untrained model
2024-09-08 11:43:03,417:INFO:Decision Tree Classifier Imported successfully
2024-09-08 11:43:03,417:INFO:Starting cross validation
2024-09-08 11:43:03,420:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:05,298:INFO:Calculating mean and std
2024-09-08 11:43:05,298:INFO:Creating metrics dataframe
2024-09-08 11:43:05,298:INFO:Uploading results into container
2024-09-08 11:43:05,298:INFO:Uploading model into container now
2024-09-08 11:43:05,298:INFO:_master_model_container: 4
2024-09-08 11:43:05,298:INFO:_display_container: 2
2024-09-08 11:43:05,298:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 11:43:05,298:INFO:create_model() successfully completed......................................
2024-09-08 11:43:05,447:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:05,447:INFO:Creating metrics dataframe
2024-09-08 11:43:05,453:INFO:Initializing SVM - Linear Kernel
2024-09-08 11:43:05,453:INFO:Total runtime is 0.33951588869094845 minutes
2024-09-08 11:43:05,453:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:05,453:INFO:Initializing create_model()
2024-09-08 11:43:05,453:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:05,457:INFO:Checking exceptions
2024-09-08 11:43:05,457:INFO:Importing libraries
2024-09-08 11:43:05,457:INFO:Copying training dataset
2024-09-08 11:43:05,470:INFO:Defining folds
2024-09-08 11:43:05,470:INFO:Declaring metric variables
2024-09-08 11:43:05,470:INFO:Importing untrained model
2024-09-08 11:43:05,470:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 11:43:05,470:INFO:Starting cross validation
2024-09-08 11:43:05,479:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:06,602:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:06,607:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:06,610:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:06,610:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:06,617:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:06,622:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:06,627:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:06,627:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:06,627:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:06,657:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:06,669:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:07,331:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:07,331:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:07,365:INFO:Calculating mean and std
2024-09-08 11:43:07,365:INFO:Creating metrics dataframe
2024-09-08 11:43:07,367:INFO:Uploading results into container
2024-09-08 11:43:07,367:INFO:Uploading model into container now
2024-09-08 11:43:07,367:INFO:_master_model_container: 5
2024-09-08 11:43:07,367:INFO:_display_container: 2
2024-09-08 11:43:07,372:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 11:43:07,372:INFO:create_model() successfully completed......................................
2024-09-08 11:43:07,507:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:07,507:INFO:Creating metrics dataframe
2024-09-08 11:43:07,512:INFO:Initializing Ridge Classifier
2024-09-08 11:43:07,513:INFO:Total runtime is 0.3738542199134826 minutes
2024-09-08 11:43:07,513:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:07,513:INFO:Initializing create_model()
2024-09-08 11:43:07,513:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:07,513:INFO:Checking exceptions
2024-09-08 11:43:07,513:INFO:Importing libraries
2024-09-08 11:43:07,513:INFO:Copying training dataset
2024-09-08 11:43:07,527:INFO:Defining folds
2024-09-08 11:43:07,527:INFO:Declaring metric variables
2024-09-08 11:43:07,530:INFO:Importing untrained model
2024-09-08 11:43:07,530:INFO:Ridge Classifier Imported successfully
2024-09-08 11:43:07,530:INFO:Starting cross validation
2024-09-08 11:43:07,538:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:08,682:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:08,690:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:08,717:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:08,752:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:08,774:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:08,799:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:08,799:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:08,837:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:09,686:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:09,687:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:09,711:INFO:Calculating mean and std
2024-09-08 11:43:09,711:INFO:Creating metrics dataframe
2024-09-08 11:43:09,717:INFO:Uploading results into container
2024-09-08 11:43:09,720:INFO:Uploading model into container now
2024-09-08 11:43:09,720:INFO:_master_model_container: 6
2024-09-08 11:43:09,720:INFO:_display_container: 2
2024-09-08 11:43:09,720:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 11:43:09,720:INFO:create_model() successfully completed......................................
2024-09-08 11:43:09,887:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:09,887:INFO:Creating metrics dataframe
2024-09-08 11:43:09,887:INFO:Initializing Random Forest Classifier
2024-09-08 11:43:09,887:INFO:Total runtime is 0.41341614325841264 minutes
2024-09-08 11:43:09,894:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:09,894:INFO:Initializing create_model()
2024-09-08 11:43:09,894:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:09,894:INFO:Checking exceptions
2024-09-08 11:43:09,894:INFO:Importing libraries
2024-09-08 11:43:09,894:INFO:Copying training dataset
2024-09-08 11:43:09,913:INFO:Defining folds
2024-09-08 11:43:09,913:INFO:Declaring metric variables
2024-09-08 11:43:09,913:INFO:Importing untrained model
2024-09-08 11:43:09,913:INFO:Random Forest Classifier Imported successfully
2024-09-08 11:43:09,917:INFO:Starting cross validation
2024-09-08 11:43:09,918:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:13,977:INFO:Calculating mean and std
2024-09-08 11:43:13,979:INFO:Creating metrics dataframe
2024-09-08 11:43:13,982:INFO:Uploading results into container
2024-09-08 11:43:13,982:INFO:Uploading model into container now
2024-09-08 11:43:13,988:INFO:_master_model_container: 7
2024-09-08 11:43:13,988:INFO:_display_container: 2
2024-09-08 11:43:13,988:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 11:43:13,988:INFO:create_model() successfully completed......................................
2024-09-08 11:43:14,181:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:14,181:INFO:Creating metrics dataframe
2024-09-08 11:43:14,189:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 11:43:14,189:INFO:Total runtime is 0.48511164188385003 minutes
2024-09-08 11:43:14,189:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:14,189:INFO:Initializing create_model()
2024-09-08 11:43:14,189:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:14,189:INFO:Checking exceptions
2024-09-08 11:43:14,189:INFO:Importing libraries
2024-09-08 11:43:14,189:INFO:Copying training dataset
2024-09-08 11:43:14,207:INFO:Defining folds
2024-09-08 11:43:14,207:INFO:Declaring metric variables
2024-09-08 11:43:14,207:INFO:Importing untrained model
2024-09-08 11:43:14,207:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 11:43:14,207:INFO:Starting cross validation
2024-09-08 11:43:14,217:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:15,428:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:43:15,433:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:43:15,532:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:43:15,540:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:43:15,577:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:43:15,592:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:43:15,613:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:43:15,633:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:43:15,878:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:15,897:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:15,907:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:15,938:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:15,972:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:16,021:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:16,021:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:16,043:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:16,580:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:43:16,580:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:43:16,763:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:16,771:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:16,813:INFO:Calculating mean and std
2024-09-08 11:43:16,813:INFO:Creating metrics dataframe
2024-09-08 11:43:16,818:INFO:Uploading results into container
2024-09-08 11:43:16,818:INFO:Uploading model into container now
2024-09-08 11:43:16,821:INFO:_master_model_container: 8
2024-09-08 11:43:16,821:INFO:_display_container: 2
2024-09-08 11:43:16,821:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 11:43:16,821:INFO:create_model() successfully completed......................................
2024-09-08 11:43:16,977:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:16,977:INFO:Creating metrics dataframe
2024-09-08 11:43:16,984:INFO:Initializing Ada Boost Classifier
2024-09-08 11:43:16,984:INFO:Total runtime is 0.531697142124176 minutes
2024-09-08 11:43:16,984:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:16,984:INFO:Initializing create_model()
2024-09-08 11:43:16,984:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:16,984:INFO:Checking exceptions
2024-09-08 11:43:16,984:INFO:Importing libraries
2024-09-08 11:43:16,988:INFO:Copying training dataset
2024-09-08 11:43:17,008:INFO:Defining folds
2024-09-08 11:43:17,008:INFO:Declaring metric variables
2024-09-08 11:43:17,008:INFO:Importing untrained model
2024-09-08 11:43:17,009:INFO:Ada Boost Classifier Imported successfully
2024-09-08 11:43:17,009:INFO:Starting cross validation
2024-09-08 11:43:17,017:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:18,018:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:43:18,038:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:43:18,042:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:43:18,049:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:43:18,048:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:43:18,058:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:43:18,058:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:43:18,066:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:43:18,851:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:18,859:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:18,868:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:18,876:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:18,878:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:18,901:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:18,923:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:19,391:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:43:19,408:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:43:19,860:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:19,864:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:19,892:INFO:Calculating mean and std
2024-09-08 11:43:19,894:INFO:Creating metrics dataframe
2024-09-08 11:43:19,898:INFO:Uploading results into container
2024-09-08 11:43:19,898:INFO:Uploading model into container now
2024-09-08 11:43:19,901:INFO:_master_model_container: 9
2024-09-08 11:43:19,901:INFO:_display_container: 2
2024-09-08 11:43:19,901:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 11:43:19,901:INFO:create_model() successfully completed......................................
2024-09-08 11:43:20,060:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:20,067:INFO:Creating metrics dataframe
2024-09-08 11:43:20,068:INFO:Initializing Gradient Boosting Classifier
2024-09-08 11:43:20,068:INFO:Total runtime is 0.5830962220827738 minutes
2024-09-08 11:43:20,073:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:20,073:INFO:Initializing create_model()
2024-09-08 11:43:20,073:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:20,073:INFO:Checking exceptions
2024-09-08 11:43:20,073:INFO:Importing libraries
2024-09-08 11:43:20,074:INFO:Copying training dataset
2024-09-08 11:43:20,088:INFO:Defining folds
2024-09-08 11:43:20,088:INFO:Declaring metric variables
2024-09-08 11:43:20,088:INFO:Importing untrained model
2024-09-08 11:43:20,090:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 11:43:20,091:INFO:Starting cross validation
2024-09-08 11:43:20,098:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:24,050:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:24,068:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:24,067:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:24,100:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:24,168:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:24,173:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:24,231:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:24,239:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:26,462:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:26,478:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:26,512:INFO:Calculating mean and std
2024-09-08 11:43:26,513:INFO:Creating metrics dataframe
2024-09-08 11:43:26,518:INFO:Uploading results into container
2024-09-08 11:43:26,518:INFO:Uploading model into container now
2024-09-08 11:43:26,520:INFO:_master_model_container: 10
2024-09-08 11:43:26,520:INFO:_display_container: 2
2024-09-08 11:43:26,520:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 11:43:26,520:INFO:create_model() successfully completed......................................
2024-09-08 11:43:26,685:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:26,685:INFO:Creating metrics dataframe
2024-09-08 11:43:26,693:INFO:Initializing Linear Discriminant Analysis
2024-09-08 11:43:26,693:INFO:Total runtime is 0.6935231606165567 minutes
2024-09-08 11:43:26,693:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:26,693:INFO:Initializing create_model()
2024-09-08 11:43:26,693:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:26,693:INFO:Checking exceptions
2024-09-08 11:43:26,693:INFO:Importing libraries
2024-09-08 11:43:26,693:INFO:Copying training dataset
2024-09-08 11:43:26,710:INFO:Defining folds
2024-09-08 11:43:26,710:INFO:Declaring metric variables
2024-09-08 11:43:26,710:INFO:Importing untrained model
2024-09-08 11:43:26,710:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 11:43:26,710:INFO:Starting cross validation
2024-09-08 11:43:26,718:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:28,118:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:28,176:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:28,179:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:28,189:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:28,251:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:28,269:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:28,278:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:28,278:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:28,914:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:28,959:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:43:28,991:INFO:Calculating mean and std
2024-09-08 11:43:28,991:INFO:Creating metrics dataframe
2024-09-08 11:43:28,991:INFO:Uploading results into container
2024-09-08 11:43:28,991:INFO:Uploading model into container now
2024-09-08 11:43:28,991:INFO:_master_model_container: 11
2024-09-08 11:43:28,991:INFO:_display_container: 2
2024-09-08 11:43:28,998:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 11:43:28,998:INFO:create_model() successfully completed......................................
2024-09-08 11:43:29,169:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:29,171:INFO:Creating metrics dataframe
2024-09-08 11:43:29,173:INFO:Initializing Extra Trees Classifier
2024-09-08 11:43:29,173:INFO:Total runtime is 0.7348520477612812 minutes
2024-09-08 11:43:29,173:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:29,173:INFO:Initializing create_model()
2024-09-08 11:43:29,173:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:29,173:INFO:Checking exceptions
2024-09-08 11:43:29,173:INFO:Importing libraries
2024-09-08 11:43:29,173:INFO:Copying training dataset
2024-09-08 11:43:29,195:INFO:Defining folds
2024-09-08 11:43:29,195:INFO:Declaring metric variables
2024-09-08 11:43:29,195:INFO:Importing untrained model
2024-09-08 11:43:29,195:INFO:Extra Trees Classifier Imported successfully
2024-09-08 11:43:29,198:INFO:Starting cross validation
2024-09-08 11:43:29,208:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:32,724:INFO:Calculating mean and std
2024-09-08 11:43:32,724:INFO:Creating metrics dataframe
2024-09-08 11:43:32,729:INFO:Uploading results into container
2024-09-08 11:43:32,729:INFO:Uploading model into container now
2024-09-08 11:43:32,729:INFO:_master_model_container: 12
2024-09-08 11:43:32,729:INFO:_display_container: 2
2024-09-08 11:43:32,732:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 11:43:32,732:INFO:create_model() successfully completed......................................
2024-09-08 11:43:32,879:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:32,879:INFO:Creating metrics dataframe
2024-09-08 11:43:32,882:INFO:Initializing Extreme Gradient Boosting
2024-09-08 11:43:32,882:INFO:Total runtime is 0.7966660896937051 minutes
2024-09-08 11:43:32,882:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:32,882:INFO:Initializing create_model()
2024-09-08 11:43:32,882:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:32,882:INFO:Checking exceptions
2024-09-08 11:43:32,889:INFO:Importing libraries
2024-09-08 11:43:32,889:INFO:Copying training dataset
2024-09-08 11:43:32,899:INFO:Defining folds
2024-09-08 11:43:32,899:INFO:Declaring metric variables
2024-09-08 11:43:32,899:INFO:Importing untrained model
2024-09-08 11:43:32,907:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:43:32,907:INFO:Starting cross validation
2024-09-08 11:43:32,908:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:36,649:INFO:Calculating mean and std
2024-09-08 11:43:36,649:INFO:Creating metrics dataframe
2024-09-08 11:43:36,652:INFO:Uploading results into container
2024-09-08 11:43:36,652:INFO:Uploading model into container now
2024-09-08 11:43:36,652:INFO:_master_model_container: 13
2024-09-08 11:43:36,652:INFO:_display_container: 2
2024-09-08 11:43:36,652:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 11:43:36,659:INFO:create_model() successfully completed......................................
2024-09-08 11:43:36,810:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:36,810:INFO:Creating metrics dataframe
2024-09-08 11:43:36,814:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 11:43:36,814:INFO:Total runtime is 0.8622007528940835 minutes
2024-09-08 11:43:36,814:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:36,814:INFO:Initializing create_model()
2024-09-08 11:43:36,814:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:36,814:INFO:Checking exceptions
2024-09-08 11:43:36,819:INFO:Importing libraries
2024-09-08 11:43:36,819:INFO:Copying training dataset
2024-09-08 11:43:36,834:INFO:Defining folds
2024-09-08 11:43:36,834:INFO:Declaring metric variables
2024-09-08 11:43:36,834:INFO:Importing untrained model
2024-09-08 11:43:36,834:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:43:36,834:INFO:Starting cross validation
2024-09-08 11:43:36,842:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:45,913:INFO:Calculating mean and std
2024-09-08 11:43:45,913:INFO:Creating metrics dataframe
2024-09-08 11:43:45,920:INFO:Uploading results into container
2024-09-08 11:43:45,920:INFO:Uploading model into container now
2024-09-08 11:43:45,921:INFO:_master_model_container: 14
2024-09-08 11:43:45,921:INFO:_display_container: 2
2024-09-08 11:43:45,921:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:43:45,921:INFO:create_model() successfully completed......................................
2024-09-08 11:43:46,073:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:46,078:INFO:Creating metrics dataframe
2024-09-08 11:43:46,080:INFO:Initializing Dummy Classifier
2024-09-08 11:43:46,080:INFO:Total runtime is 1.0166271766026813 minutes
2024-09-08 11:43:46,080:INFO:SubProcess create_model() called ==================================
2024-09-08 11:43:46,080:INFO:Initializing create_model()
2024-09-08 11:43:46,080:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A1985070A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:46,080:INFO:Checking exceptions
2024-09-08 11:43:46,080:INFO:Importing libraries
2024-09-08 11:43:46,080:INFO:Copying training dataset
2024-09-08 11:43:46,093:INFO:Defining folds
2024-09-08 11:43:46,093:INFO:Declaring metric variables
2024-09-08 11:43:46,093:INFO:Importing untrained model
2024-09-08 11:43:46,093:INFO:Dummy Classifier Imported successfully
2024-09-08 11:43:46,093:INFO:Starting cross validation
2024-09-08 11:43:46,105:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:47,132:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:47,132:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:47,148:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:47,205:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:47,355:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:47,355:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:47,372:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:47,450:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:47,860:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:47,930:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:43:47,940:INFO:Calculating mean and std
2024-09-08 11:43:47,940:INFO:Creating metrics dataframe
2024-09-08 11:43:47,940:INFO:Uploading results into container
2024-09-08 11:43:47,940:INFO:Uploading model into container now
2024-09-08 11:43:47,940:INFO:_master_model_container: 15
2024-09-08 11:43:47,940:INFO:_display_container: 2
2024-09-08 11:43:47,940:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 11:43:47,940:INFO:create_model() successfully completed......................................
2024-09-08 11:43:48,081:INFO:SubProcess create_model() end ==================================
2024-09-08 11:43:48,081:INFO:Creating metrics dataframe
2024-09-08 11:43:48,090:INFO:Initializing create_model()
2024-09-08 11:43:48,090:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:48,090:INFO:Checking exceptions
2024-09-08 11:43:48,090:INFO:Importing libraries
2024-09-08 11:43:48,098:INFO:Copying training dataset
2024-09-08 11:43:48,110:INFO:Defining folds
2024-09-08 11:43:48,115:INFO:Declaring metric variables
2024-09-08 11:43:48,115:INFO:Importing untrained model
2024-09-08 11:43:48,115:INFO:Declaring custom model
2024-09-08 11:43:48,115:INFO:Random Forest Classifier Imported successfully
2024-09-08 11:43:48,123:INFO:Cross validation set to False
2024-09-08 11:43:48,123:INFO:Fitting Model
2024-09-08 11:43:48,792:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 11:43:48,797:INFO:create_model() successfully completed......................................
2024-09-08 11:43:48,930:INFO:Initializing create_model()
2024-09-08 11:43:48,930:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:48,930:INFO:Checking exceptions
2024-09-08 11:43:48,930:INFO:Importing libraries
2024-09-08 11:43:48,930:INFO:Copying training dataset
2024-09-08 11:43:48,943:INFO:Defining folds
2024-09-08 11:43:48,943:INFO:Declaring metric variables
2024-09-08 11:43:48,943:INFO:Importing untrained model
2024-09-08 11:43:48,943:INFO:Declaring custom model
2024-09-08 11:43:48,943:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:43:48,952:INFO:Cross validation set to False
2024-09-08 11:43:48,952:INFO:Fitting Model
2024-09-08 11:43:49,320:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 11:43:49,323:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000584 seconds.
2024-09-08 11:43:49,323:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 11:43:49,323:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 11:43:49,323:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 11:43:49,323:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 11:43:49,323:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 11:43:49,323:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 11:43:49,323:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 11:43:49,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:49,653:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:43:49,653:INFO:create_model() successfully completed......................................
2024-09-08 11:43:49,811:INFO:Initializing create_model()
2024-09-08 11:43:49,811:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:49,811:INFO:Checking exceptions
2024-09-08 11:43:49,811:INFO:Importing libraries
2024-09-08 11:43:49,811:INFO:Copying training dataset
2024-09-08 11:43:49,827:INFO:Defining folds
2024-09-08 11:43:49,827:INFO:Declaring metric variables
2024-09-08 11:43:49,827:INFO:Importing untrained model
2024-09-08 11:43:49,827:INFO:Declaring custom model
2024-09-08 11:43:49,827:INFO:Extra Trees Classifier Imported successfully
2024-09-08 11:43:49,835:INFO:Cross validation set to False
2024-09-08 11:43:49,835:INFO:Fitting Model
2024-09-08 11:43:50,413:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 11:43:50,413:INFO:create_model() successfully completed......................................
2024-09-08 11:43:50,570:INFO:_master_model_container: 15
2024-09-08 11:43:50,570:INFO:_display_container: 2
2024-09-08 11:43:50,570:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)]
2024-09-08 11:43:50,570:INFO:compare_models() successfully completed......................................
2024-09-08 11:43:50,570:INFO:Initializing create_model()
2024-09-08 11:43:50,570:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:50,570:INFO:Checking exceptions
2024-09-08 11:43:50,577:INFO:Importing libraries
2024-09-08 11:43:50,577:INFO:Copying training dataset
2024-09-08 11:43:50,593:INFO:Defining folds
2024-09-08 11:43:50,593:INFO:Declaring metric variables
2024-09-08 11:43:50,593:INFO:Importing untrained model
2024-09-08 11:43:50,593:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:43:50,593:INFO:Starting cross validation
2024-09-08 11:43:50,601:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:43:57,092:INFO:Calculating mean and std
2024-09-08 11:43:57,092:INFO:Creating metrics dataframe
2024-09-08 11:43:57,096:INFO:Finalizing model
2024-09-08 11:43:57,531:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 11:43:57,531:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000779 seconds.
2024-09-08 11:43:57,531:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 11:43:57,531:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 11:43:57,531:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 11:43:57,535:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 11:43:57,537:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 11:43:57,537:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 11:43:57,537:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 11:43:57,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:43:57,975:INFO:Uploading results into container
2024-09-08 11:43:57,975:INFO:Uploading model into container now
2024-09-08 11:43:58,001:INFO:_master_model_container: 16
2024-09-08 11:43:58,001:INFO:_display_container: 3
2024-09-08 11:43:58,006:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:43:58,006:INFO:create_model() successfully completed......................................
2024-09-08 11:43:58,161:INFO:Initializing create_model()
2024-09-08 11:43:58,161:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:43:58,161:INFO:Checking exceptions
2024-09-08 11:43:58,161:INFO:Importing libraries
2024-09-08 11:43:58,161:INFO:Copying training dataset
2024-09-08 11:43:58,173:INFO:Defining folds
2024-09-08 11:43:58,180:INFO:Declaring metric variables
2024-09-08 11:43:58,180:INFO:Importing untrained model
2024-09-08 11:43:58,182:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:43:58,182:INFO:Starting cross validation
2024-09-08 11:43:58,192:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:44:01,609:INFO:Calculating mean and std
2024-09-08 11:44:01,611:INFO:Creating metrics dataframe
2024-09-08 11:44:01,611:INFO:Finalizing model
2024-09-08 11:44:02,463:INFO:Uploading results into container
2024-09-08 11:44:02,463:INFO:Uploading model into container now
2024-09-08 11:44:02,501:INFO:_master_model_container: 17
2024-09-08 11:44:02,501:INFO:_display_container: 4
2024-09-08 11:44:02,501:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:44:02,507:INFO:create_model() successfully completed......................................
2024-09-08 11:44:02,695:INFO:Initializing create_model()
2024-09-08 11:44:02,695:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:44:02,695:INFO:Checking exceptions
2024-09-08 11:44:02,696:INFO:Importing libraries
2024-09-08 11:44:02,698:INFO:Copying training dataset
2024-09-08 11:44:02,715:INFO:Defining folds
2024-09-08 11:44:02,715:INFO:Declaring metric variables
2024-09-08 11:44:02,715:INFO:Importing untrained model
2024-09-08 11:44:02,715:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 11:44:02,715:INFO:Starting cross validation
2024-09-08 11:44:02,732:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:44:05,811:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:44:05,836:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:44:05,873:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:44:05,901:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:44:05,932:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:44:05,946:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:44:06,046:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:44:06,055:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:44:08,141:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:44:08,221:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:44:08,239:INFO:Calculating mean and std
2024-09-08 11:44:08,241:INFO:Creating metrics dataframe
2024-09-08 11:44:08,241:INFO:Finalizing model
2024-09-08 11:44:10,196:INFO:Uploading results into container
2024-09-08 11:44:10,196:INFO:Uploading model into container now
2024-09-08 11:44:10,214:INFO:_master_model_container: 18
2024-09-08 11:44:10,214:INFO:_display_container: 5
2024-09-08 11:44:10,214:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 11:44:10,214:INFO:create_model() successfully completed......................................
2024-09-08 11:44:10,362:INFO:Initializing tune_model()
2024-09-08 11:44:10,362:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>)
2024-09-08 11:44:10,362:INFO:Checking exceptions
2024-09-08 11:44:10,362:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 11:44:10,435:INFO:Copying training dataset
2024-09-08 11:44:10,443:INFO:Checking base model
2024-09-08 11:44:10,443:INFO:Base model : Extreme Gradient Boosting
2024-09-08 11:44:10,443:INFO:Declaring metric variables
2024-09-08 11:44:10,443:INFO:Defining Hyperparameters
2024-09-08 11:44:10,592:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 11:44:10,592:INFO:Tuning with n_jobs=-1
2024-09-08 11:44:10,601:INFO:Initializing skopt.BayesSearchCV
2024-09-08 11:44:25,042:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.6), ('actual_estimator__gamma', 0), ('actual_estimator__learning_rate', 0.01), ('actual_estimator__max_depth', 7), ('actual_estimator__min_child_weight', 1), ('actual_estimator__n_estimators', 100), ('actual_estimator__subsample', 0.6)])
2024-09-08 11:44:25,042:INFO:Hyperparameter search completed
2024-09-08 11:44:25,042:INFO:SubProcess create_model() called ==================================
2024-09-08 11:44:25,042:INFO:Initializing create_model()
2024-09-08 11:44:25,042:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A191375600>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.6})
2024-09-08 11:44:25,042:INFO:Checking exceptions
2024-09-08 11:44:25,042:INFO:Importing libraries
2024-09-08 11:44:25,042:INFO:Copying training dataset
2024-09-08 11:44:25,064:INFO:Defining folds
2024-09-08 11:44:25,064:INFO:Declaring metric variables
2024-09-08 11:44:25,064:INFO:Importing untrained model
2024-09-08 11:44:25,064:INFO:Declaring custom model
2024-09-08 11:44:25,066:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:44:25,066:INFO:Starting cross validation
2024-09-08 11:44:25,074:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:44:26,816:INFO:Calculating mean and std
2024-09-08 11:44:26,817:INFO:Creating metrics dataframe
2024-09-08 11:44:26,820:INFO:Finalizing model
2024-09-08 11:44:27,806:INFO:Uploading results into container
2024-09-08 11:44:27,806:INFO:Uploading model into container now
2024-09-08 11:44:27,813:INFO:_master_model_container: 19
2024-09-08 11:44:27,813:INFO:_display_container: 6
2024-09-08 11:44:27,813:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:44:27,813:INFO:create_model() successfully completed......................................
2024-09-08 11:44:27,968:INFO:SubProcess create_model() end ==================================
2024-09-08 11:44:27,968:INFO:choose_better activated
2024-09-08 11:44:27,968:INFO:SubProcess create_model() called ==================================
2024-09-08 11:44:27,968:INFO:Initializing create_model()
2024-09-08 11:44:27,971:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:44:27,971:INFO:Checking exceptions
2024-09-08 11:44:27,973:INFO:Importing libraries
2024-09-08 11:44:27,973:INFO:Copying training dataset
2024-09-08 11:44:27,988:INFO:Defining folds
2024-09-08 11:44:27,988:INFO:Declaring metric variables
2024-09-08 11:44:27,988:INFO:Importing untrained model
2024-09-08 11:44:27,988:INFO:Declaring custom model
2024-09-08 11:44:27,988:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:44:27,993:INFO:Starting cross validation
2024-09-08 11:44:27,996:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:44:29,615:INFO:Calculating mean and std
2024-09-08 11:44:29,616:INFO:Creating metrics dataframe
2024-09-08 11:44:29,616:INFO:Finalizing model
2024-09-08 11:44:30,523:INFO:Uploading results into container
2024-09-08 11:44:30,528:INFO:Uploading model into container now
2024-09-08 11:44:30,528:INFO:_master_model_container: 20
2024-09-08 11:44:30,528:INFO:_display_container: 7
2024-09-08 11:44:30,528:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:44:30,533:INFO:create_model() successfully completed......................................
2024-09-08 11:44:30,685:INFO:SubProcess create_model() end ==================================
2024-09-08 11:44:30,685:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8186
2024-09-08 11:44:30,692:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8386
2024-09-08 11:44:30,693:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 11:44:30,693:INFO:choose_better completed
2024-09-08 11:44:30,708:INFO:_master_model_container: 20
2024-09-08 11:44:30,708:INFO:_display_container: 6
2024-09-08 11:44:30,708:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:44:30,708:INFO:tune_model() successfully completed......................................
2024-09-08 11:44:30,858:INFO:Initializing predict_model()
2024-09-08 11:44:30,858:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001A188741480>)
2024-09-08 11:44:30,858:INFO:Checking exceptions
2024-09-08 11:44:30,858:INFO:Preloading libraries
2024-09-08 11:44:31,415:INFO:Initializing get_config()
2024-09-08 11:44:31,415:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, variable=X_train)
2024-09-08 11:44:31,415:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 11:44:31,493:INFO:Variable:  returned as       id Marital status Application mode Application order  ... Curricular units 2nd sem (without evaluations) Unemployment rate Inflation rate   GDP
765  765              1                1                 6  ...                                              0         10.800000            1.4  1.74
323  323              1               17                 1  ...                                              0          7.600000            2.6  0.32
909  909              1                1                 1  ...                                              0         12.400000            0.5  1.79
774  774              1               17                 1  ...                                              0          8.900000            1.4  3.51
931  931              1               39                 1  ...                                              0         16.200001            0.3 -0.92
..   ...            ...              ...               ...  ...                                            ...               ...            ...   ...
806  806              4               39                 1  ...                                              0         10.800000            1.4  1.74
492  492              1                1                 3  ...                                              0          7.600000            2.6  0.32
698  698              1               39                 1  ...                                              1         10.800000            1.4  1.74
44    44              1                1                 1  ...                                              0          9.400000           -0.8 -3.12
604  604              1                1                 1  ...                                              0         10.800000            1.4  1.74

[700 rows x 37 columns]
2024-09-08 11:44:31,493:INFO:get_config() successfully completed......................................
2024-09-08 11:44:31,493:INFO:Initializing predict_model()
2024-09-08 11:44:31,493:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001A19845DC60>)
2024-09-08 11:44:31,493:INFO:Checking exceptions
2024-09-08 11:44:31,493:INFO:Preloading libraries
2024-09-08 11:44:31,493:INFO:Set up data.
2024-09-08 11:44:31,506:INFO:Set up index.
2024-09-08 11:44:31,911:INFO:Initializing get_config()
2024-09-08 11:44:31,912:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, variable=y_train)
2024-09-08 11:44:31,912:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 11:44:31,918:INFO:Variable:  returned as 765    0
323    1
909    0
774    0
931    1
      ..
806    1
492    0
698    1
44     0
604    1
Name: Target, Length: 700, dtype: int8
2024-09-08 11:44:31,918:INFO:get_config() successfully completed......................................
2024-09-08 11:44:31,918:INFO:Initializing get_config()
2024-09-08 11:44:31,918:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, variable=y_test)
2024-09-08 11:44:31,918:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 11:44:31,923:INFO:Variable:  returned as 47     1
233    1
132    2
34     0
162    0
      ..
772    0
632    0
365    2
136    0
299    1
Name: Target, Length: 300, dtype: int8
2024-09-08 11:44:31,923:INFO:get_config() successfully completed......................................
2024-09-08 11:44:31,923:INFO:Initializing finalize_model()
2024-09-08 11:44:31,923:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 11:44:31,923:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:44:31,936:INFO:Initializing create_model()
2024-09-08 11:44:31,936:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:44:31,936:INFO:Checking exceptions
2024-09-08 11:44:31,936:INFO:Importing libraries
2024-09-08 11:44:31,936:INFO:Copying training dataset
2024-09-08 11:44:31,936:INFO:Defining folds
2024-09-08 11:44:31,936:INFO:Declaring metric variables
2024-09-08 11:44:31,936:INFO:Importing untrained model
2024-09-08 11:44:31,936:INFO:Declaring custom model
2024-09-08 11:44:31,936:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:44:31,943:INFO:Cross validation set to False
2024-09-08 11:44:31,943:INFO:Fitting Model
2024-09-08 11:44:33,468:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 11:44:33,468:INFO:create_model() successfully completed......................................
2024-09-08 11:44:33,606:INFO:_master_model_container: 20
2024-09-08 11:44:33,606:INFO:_display_container: 7
2024-09-08 11:44:33,893:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 11:44:33,893:INFO:finalize_model() successfully completed......................................
2024-09-08 11:44:34,365:INFO:Initializing predict_model()
2024-09-08 11:44:34,365:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A18B3C7400>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001A198794E50>)
2024-09-08 11:44:34,365:INFO:Checking exceptions
2024-09-08 11:44:34,365:INFO:Preloading libraries
2024-09-08 11:44:34,365:INFO:Set up data.
2024-09-08 11:44:34,403:INFO:Set up index.
2024-09-08 11:46:31,677:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:46:31,677:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:46:31,681:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:46:31,681:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:46:59,554:INFO:PyCaret ClassificationExperiment
2024-09-08 11:46:59,554:INFO:Logging name: clf-default-name
2024-09-08 11:46:59,554:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 11:46:59,554:INFO:version 3.3.2
2024-09-08 11:46:59,554:INFO:Initializing setup()
2024-09-08 11:46:59,554:INFO:self.USI: f801
2024-09-08 11:46:59,554:INFO:self._variable_keys: {'y_train', 'logging_param', 'X', '_ml_usecase', 'memory', 'target_param', 'fold_shuffle_param', 'fold_generator', 'seed', 'X_test', 'gpu_param', 'y', 'USI', 'log_plots_param', 'is_multiclass', 'data', 'idx', 'y_test', '_available_plots', 'fold_groups_param', 'fix_imbalance', 'X_train', 'exp_id', 'gpu_n_jobs_param', 'pipeline', 'exp_name_log', 'n_jobs_param', 'html_param'}
2024-09-08 11:46:59,559:INFO:Checking environment
2024-09-08 11:46:59,559:INFO:python_version: 3.10.11
2024-09-08 11:46:59,560:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 11:46:59,560:INFO:machine: AMD64
2024-09-08 11:46:59,576:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 11:46:59,576:INFO:Memory: svmem(total=16407719936, available=5834072064, percent=64.4, used=10573647872, free=5834072064)
2024-09-08 11:46:59,576:INFO:Physical Core: 4
2024-09-08 11:46:59,576:INFO:Logical Core: 8
2024-09-08 11:46:59,576:INFO:Checking libraries
2024-09-08 11:46:59,576:INFO:System:
2024-09-08 11:46:59,576:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 11:46:59,576:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 11:46:59,576:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 11:46:59,576:INFO:PyCaret required dependencies:
2024-09-08 11:46:59,675:INFO:                 pip: 24.2
2024-09-08 11:46:59,675:INFO:          setuptools: 72.1.0
2024-09-08 11:46:59,675:INFO:             pycaret: 3.3.2
2024-09-08 11:46:59,675:INFO:             IPython: 8.25.0
2024-09-08 11:46:59,675:INFO:          ipywidgets: 8.1.5
2024-09-08 11:46:59,675:INFO:                tqdm: 4.66.5
2024-09-08 11:46:59,675:INFO:               numpy: 1.26.4
2024-09-08 11:46:59,675:INFO:              pandas: 2.1.4
2024-09-08 11:46:59,675:INFO:              jinja2: 3.1.4
2024-09-08 11:46:59,675:INFO:               scipy: 1.11.4
2024-09-08 11:46:59,675:INFO:              joblib: 1.3.2
2024-09-08 11:46:59,675:INFO:             sklearn: 1.4.2
2024-09-08 11:46:59,675:INFO:                pyod: 2.0.1
2024-09-08 11:46:59,675:INFO:            imblearn: 0.12.3
2024-09-08 11:46:59,675:INFO:   category_encoders: 2.6.3
2024-09-08 11:46:59,675:INFO:            lightgbm: 4.5.0
2024-09-08 11:46:59,675:INFO:               numba: 0.60.0
2024-09-08 11:46:59,684:INFO:            requests: 2.32.3
2024-09-08 11:46:59,684:INFO:          matplotlib: 3.7.5
2024-09-08 11:46:59,684:INFO:          scikitplot: 0.3.7
2024-09-08 11:46:59,684:INFO:         yellowbrick: 1.5
2024-09-08 11:46:59,684:INFO:              plotly: 5.24.0
2024-09-08 11:46:59,684:INFO:    plotly-resampler: Not installed
2024-09-08 11:46:59,684:INFO:             kaleido: 0.2.1
2024-09-08 11:46:59,684:INFO:           schemdraw: 0.15
2024-09-08 11:46:59,684:INFO:         statsmodels: 0.14.2
2024-09-08 11:46:59,684:INFO:              sktime: 0.26.0
2024-09-08 11:46:59,684:INFO:               tbats: 1.1.3
2024-09-08 11:46:59,684:INFO:            pmdarima: 2.0.4
2024-09-08 11:46:59,684:INFO:              psutil: 5.9.0
2024-09-08 11:46:59,684:INFO:          markupsafe: 2.1.3
2024-09-08 11:46:59,684:INFO:             pickle5: Not installed
2024-09-08 11:46:59,686:INFO:         cloudpickle: 3.0.0
2024-09-08 11:46:59,686:INFO:         deprecation: 2.1.0
2024-09-08 11:46:59,686:INFO:              xxhash: 3.5.0
2024-09-08 11:46:59,686:INFO:           wurlitzer: Not installed
2024-09-08 11:46:59,686:INFO:PyCaret optional dependencies:
2024-09-08 11:46:59,704:INFO:                shap: Not installed
2024-09-08 11:46:59,704:INFO:           interpret: Not installed
2024-09-08 11:46:59,704:INFO:                umap: Not installed
2024-09-08 11:46:59,704:INFO:     ydata_profiling: Not installed
2024-09-08 11:46:59,704:INFO:  explainerdashboard: Not installed
2024-09-08 11:46:59,704:INFO:             autoviz: Not installed
2024-09-08 11:46:59,704:INFO:           fairlearn: Not installed
2024-09-08 11:46:59,704:INFO:          deepchecks: Not installed
2024-09-08 11:46:59,704:INFO:             xgboost: 2.1.1
2024-09-08 11:46:59,704:INFO:            catboost: Not installed
2024-09-08 11:46:59,704:INFO:              kmodes: Not installed
2024-09-08 11:46:59,708:INFO:             mlxtend: Not installed
2024-09-08 11:46:59,708:INFO:       statsforecast: Not installed
2024-09-08 11:46:59,708:INFO:        tune_sklearn: Not installed
2024-09-08 11:46:59,708:INFO:                 ray: Not installed
2024-09-08 11:46:59,708:INFO:            hyperopt: 0.2.7
2024-09-08 11:46:59,708:INFO:              optuna: 4.0.0
2024-09-08 11:46:59,708:INFO:               skopt: 0.10.2
2024-09-08 11:46:59,708:INFO:              mlflow: Not installed
2024-09-08 11:46:59,708:INFO:              gradio: Not installed
2024-09-08 11:46:59,708:INFO:             fastapi: Not installed
2024-09-08 11:46:59,708:INFO:             uvicorn: Not installed
2024-09-08 11:46:59,708:INFO:              m2cgen: Not installed
2024-09-08 11:46:59,708:INFO:           evidently: Not installed
2024-09-08 11:46:59,708:INFO:               fugue: Not installed
2024-09-08 11:46:59,708:INFO:           streamlit: 1.38.0
2024-09-08 11:46:59,708:INFO:             prophet: Not installed
2024-09-08 11:46:59,708:INFO:None
2024-09-08 11:46:59,708:INFO:Set up data.
2024-09-08 11:46:59,735:INFO:Set up folding strategy.
2024-09-08 11:46:59,735:INFO:Set up train/test split.
2024-09-08 11:46:59,754:INFO:Set up index.
2024-09-08 11:46:59,754:INFO:Assigning column types.
2024-09-08 11:46:59,766:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 11:46:59,849:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 11:46:59,854:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:46:59,915:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:46:59,924:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:47:00,014:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 11:47:00,014:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:47:00,074:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:47:00,080:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:47:00,080:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 11:47:00,179:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:47:00,245:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:47:00,249:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:47:00,345:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:47:00,395:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:47:00,404:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:47:00,404:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 11:47:00,554:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:47:00,561:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:47:00,694:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:47:00,700:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:47:00,704:INFO:Preparing preprocessing pipeline...
2024-09-08 11:47:00,704:INFO:Set up simple imputation.
2024-09-08 11:47:00,717:INFO:Set up encoding of ordinal features.
2024-09-08 11:47:00,744:INFO:Set up encoding of categorical features.
2024-09-08 11:47:00,744:INFO:Set up column name cleaning.
2024-09-08 11:47:01,108:INFO:Finished creating preprocessing pipeline.
2024-09-08 11:47:01,380:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                                                    'qualification',
                                                                    'Fathers '
                                                                    'qualification',
                                                                    'Mothers '
                                                                    'occupation',
                                                                    'Fathers '
                                                                    'occupation'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 11:47:01,380:INFO:Creating final display dataframe.
2024-09-08 11:47:01,961:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (1000, 38)
4        Transformed data shape       (1000, 129)
5   Transformed train set shape        (700, 129)
6    Transformed test set shape        (300, 129)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              f801
2024-09-08 11:47:02,115:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:47:02,124:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:47:02,264:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:47:02,273:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:47:02,274:INFO:setup() successfully completed in 2.75s...............
2024-09-08 11:47:02,274:INFO:Initializing compare_models()
2024-09-08 11:47:02,274:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 11:47:02,274:INFO:Checking exceptions
2024-09-08 11:47:02,284:INFO:Preparing display monitor
2024-09-08 11:47:02,290:INFO:Initializing Logistic Regression
2024-09-08 11:47:02,290:INFO:Total runtime is 0.0 minutes
2024-09-08 11:47:02,290:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:02,290:INFO:Initializing create_model()
2024-09-08 11:47:02,290:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:02,290:INFO:Checking exceptions
2024-09-08 11:47:02,290:INFO:Importing libraries
2024-09-08 11:47:02,290:INFO:Copying training dataset
2024-09-08 11:47:02,306:INFO:Defining folds
2024-09-08 11:47:02,306:INFO:Declaring metric variables
2024-09-08 11:47:02,306:INFO:Importing untrained model
2024-09-08 11:47:02,306:INFO:Logistic Regression Imported successfully
2024-09-08 11:47:02,306:INFO:Starting cross validation
2024-09-08 11:47:02,314:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:13,195:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:47:13,345:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:47:13,361:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:47:13,545:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:47:13,605:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:13,615:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:13,650:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:13,970:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:14,178:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:47:14,250:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:47:14,285:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:47:14,390:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:47:14,459:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:14,516:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:14,565:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:14,643:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:15,585:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:47:15,595:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:47:15,725:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:15,741:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:15,789:INFO:Calculating mean and std
2024-09-08 11:47:15,789:INFO:Creating metrics dataframe
2024-09-08 11:47:15,795:INFO:Uploading results into container
2024-09-08 11:47:15,795:INFO:Uploading model into container now
2024-09-08 11:47:15,795:INFO:_master_model_container: 1
2024-09-08 11:47:15,795:INFO:_display_container: 2
2024-09-08 11:47:15,795:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 11:47:15,795:INFO:create_model() successfully completed......................................
2024-09-08 11:47:16,005:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:16,005:INFO:Creating metrics dataframe
2024-09-08 11:47:16,010:INFO:Initializing K Neighbors Classifier
2024-09-08 11:47:16,010:INFO:Total runtime is 0.22866670687993368 minutes
2024-09-08 11:47:16,010:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:16,010:INFO:Initializing create_model()
2024-09-08 11:47:16,010:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:16,010:INFO:Checking exceptions
2024-09-08 11:47:16,010:INFO:Importing libraries
2024-09-08 11:47:16,010:INFO:Copying training dataset
2024-09-08 11:47:16,029:INFO:Defining folds
2024-09-08 11:47:16,029:INFO:Declaring metric variables
2024-09-08 11:47:16,030:INFO:Importing untrained model
2024-09-08 11:47:16,030:INFO:K Neighbors Classifier Imported successfully
2024-09-08 11:47:16,032:INFO:Starting cross validation
2024-09-08 11:47:16,035:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:18,286:INFO:Calculating mean and std
2024-09-08 11:47:18,286:INFO:Creating metrics dataframe
2024-09-08 11:47:18,291:INFO:Uploading results into container
2024-09-08 11:47:18,291:INFO:Uploading model into container now
2024-09-08 11:47:18,291:INFO:_master_model_container: 2
2024-09-08 11:47:18,291:INFO:_display_container: 2
2024-09-08 11:47:18,291:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 11:47:18,291:INFO:create_model() successfully completed......................................
2024-09-08 11:47:18,430:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:18,430:INFO:Creating metrics dataframe
2024-09-08 11:47:18,436:INFO:Initializing Naive Bayes
2024-09-08 11:47:18,436:INFO:Total runtime is 0.2690942049026489 minutes
2024-09-08 11:47:18,436:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:18,436:INFO:Initializing create_model()
2024-09-08 11:47:18,436:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:18,436:INFO:Checking exceptions
2024-09-08 11:47:18,436:INFO:Importing libraries
2024-09-08 11:47:18,436:INFO:Copying training dataset
2024-09-08 11:47:18,445:INFO:Defining folds
2024-09-08 11:47:18,445:INFO:Declaring metric variables
2024-09-08 11:47:18,445:INFO:Importing untrained model
2024-09-08 11:47:18,445:INFO:Naive Bayes Imported successfully
2024-09-08 11:47:18,445:INFO:Starting cross validation
2024-09-08 11:47:18,460:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:19,536:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:20,164:INFO:Calculating mean and std
2024-09-08 11:47:20,166:INFO:Creating metrics dataframe
2024-09-08 11:47:20,166:INFO:Uploading results into container
2024-09-08 11:47:20,166:INFO:Uploading model into container now
2024-09-08 11:47:20,166:INFO:_master_model_container: 3
2024-09-08 11:47:20,166:INFO:_display_container: 2
2024-09-08 11:47:20,171:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 11:47:20,171:INFO:create_model() successfully completed......................................
2024-09-08 11:47:20,306:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:20,306:INFO:Creating metrics dataframe
2024-09-08 11:47:20,309:INFO:Initializing Decision Tree Classifier
2024-09-08 11:47:20,309:INFO:Total runtime is 0.30032026767730713 minutes
2024-09-08 11:47:20,309:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:20,309:INFO:Initializing create_model()
2024-09-08 11:47:20,309:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:20,309:INFO:Checking exceptions
2024-09-08 11:47:20,309:INFO:Importing libraries
2024-09-08 11:47:20,309:INFO:Copying training dataset
2024-09-08 11:47:20,326:INFO:Defining folds
2024-09-08 11:47:20,326:INFO:Declaring metric variables
2024-09-08 11:47:20,326:INFO:Importing untrained model
2024-09-08 11:47:20,326:INFO:Decision Tree Classifier Imported successfully
2024-09-08 11:47:20,326:INFO:Starting cross validation
2024-09-08 11:47:20,336:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:22,012:INFO:Calculating mean and std
2024-09-08 11:47:22,012:INFO:Creating metrics dataframe
2024-09-08 11:47:22,016:INFO:Uploading results into container
2024-09-08 11:47:22,016:INFO:Uploading model into container now
2024-09-08 11:47:22,016:INFO:_master_model_container: 4
2024-09-08 11:47:22,016:INFO:_display_container: 2
2024-09-08 11:47:22,020:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 11:47:22,020:INFO:create_model() successfully completed......................................
2024-09-08 11:47:22,158:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:22,166:INFO:Creating metrics dataframe
2024-09-08 11:47:22,166:INFO:Initializing SVM - Linear Kernel
2024-09-08 11:47:22,166:INFO:Total runtime is 0.33125925064086914 minutes
2024-09-08 11:47:22,166:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:22,173:INFO:Initializing create_model()
2024-09-08 11:47:22,173:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:22,173:INFO:Checking exceptions
2024-09-08 11:47:22,173:INFO:Importing libraries
2024-09-08 11:47:22,173:INFO:Copying training dataset
2024-09-08 11:47:22,189:INFO:Defining folds
2024-09-08 11:47:22,189:INFO:Declaring metric variables
2024-09-08 11:47:22,189:INFO:Importing untrained model
2024-09-08 11:47:22,189:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 11:47:22,189:INFO:Starting cross validation
2024-09-08 11:47:22,196:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:23,267:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:23,286:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:23,311:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:23,314:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:23,316:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:23,321:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:23,321:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:23,330:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:23,336:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:23,341:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:23,386:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:23,896:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:23,926:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:23,956:INFO:Calculating mean and std
2024-09-08 11:47:23,956:INFO:Creating metrics dataframe
2024-09-08 11:47:23,961:INFO:Uploading results into container
2024-09-08 11:47:23,961:INFO:Uploading model into container now
2024-09-08 11:47:23,962:INFO:_master_model_container: 5
2024-09-08 11:47:23,962:INFO:_display_container: 2
2024-09-08 11:47:23,962:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 11:47:23,962:INFO:create_model() successfully completed......................................
2024-09-08 11:47:24,096:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:24,096:INFO:Creating metrics dataframe
2024-09-08 11:47:24,100:INFO:Initializing Ridge Classifier
2024-09-08 11:47:24,100:INFO:Total runtime is 0.3635037620862325 minutes
2024-09-08 11:47:24,100:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:24,100:INFO:Initializing create_model()
2024-09-08 11:47:24,100:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:24,100:INFO:Checking exceptions
2024-09-08 11:47:24,100:INFO:Importing libraries
2024-09-08 11:47:24,100:INFO:Copying training dataset
2024-09-08 11:47:24,116:INFO:Defining folds
2024-09-08 11:47:24,116:INFO:Declaring metric variables
2024-09-08 11:47:24,116:INFO:Importing untrained model
2024-09-08 11:47:24,116:INFO:Ridge Classifier Imported successfully
2024-09-08 11:47:24,116:INFO:Starting cross validation
2024-09-08 11:47:24,121:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:25,151:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:25,160:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:25,160:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:25,171:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:25,176:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:25,186:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:25,224:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:25,231:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:25,771:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:25,806:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:25,845:INFO:Calculating mean and std
2024-09-08 11:47:25,846:INFO:Creating metrics dataframe
2024-09-08 11:47:25,851:INFO:Uploading results into container
2024-09-08 11:47:25,851:INFO:Uploading model into container now
2024-09-08 11:47:25,851:INFO:_master_model_container: 6
2024-09-08 11:47:25,851:INFO:_display_container: 2
2024-09-08 11:47:25,851:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 11:47:25,851:INFO:create_model() successfully completed......................................
2024-09-08 11:47:26,012:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:26,013:INFO:Creating metrics dataframe
2024-09-08 11:47:26,018:INFO:Initializing Random Forest Classifier
2024-09-08 11:47:26,018:INFO:Total runtime is 0.39546219905217483 minutes
2024-09-08 11:47:26,018:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:26,018:INFO:Initializing create_model()
2024-09-08 11:47:26,018:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:26,018:INFO:Checking exceptions
2024-09-08 11:47:26,018:INFO:Importing libraries
2024-09-08 11:47:26,018:INFO:Copying training dataset
2024-09-08 11:47:26,035:INFO:Defining folds
2024-09-08 11:47:26,035:INFO:Declaring metric variables
2024-09-08 11:47:26,036:INFO:Importing untrained model
2024-09-08 11:47:26,036:INFO:Random Forest Classifier Imported successfully
2024-09-08 11:47:26,036:INFO:Starting cross validation
2024-09-08 11:47:26,041:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:29,229:INFO:Calculating mean and std
2024-09-08 11:47:29,229:INFO:Creating metrics dataframe
2024-09-08 11:47:29,229:INFO:Uploading results into container
2024-09-08 11:47:29,229:INFO:Uploading model into container now
2024-09-08 11:47:29,236:INFO:_master_model_container: 7
2024-09-08 11:47:29,236:INFO:_display_container: 2
2024-09-08 11:47:29,236:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 11:47:29,236:INFO:create_model() successfully completed......................................
2024-09-08 11:47:29,376:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:29,376:INFO:Creating metrics dataframe
2024-09-08 11:47:29,376:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 11:47:29,376:INFO:Total runtime is 0.4514358798662821 minutes
2024-09-08 11:47:29,376:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:29,376:INFO:Initializing create_model()
2024-09-08 11:47:29,376:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:29,376:INFO:Checking exceptions
2024-09-08 11:47:29,376:INFO:Importing libraries
2024-09-08 11:47:29,376:INFO:Copying training dataset
2024-09-08 11:47:29,397:INFO:Defining folds
2024-09-08 11:47:29,397:INFO:Declaring metric variables
2024-09-08 11:47:29,397:INFO:Importing untrained model
2024-09-08 11:47:29,397:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 11:47:29,397:INFO:Starting cross validation
2024-09-08 11:47:29,406:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:30,166:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:47:30,251:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:47:30,271:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:47:30,422:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:47:30,426:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:47:30,431:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:30,448:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:47:30,448:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:47:30,448:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:47:30,519:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:30,556:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:30,687:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:30,687:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:30,717:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:30,717:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:30,717:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:31,016:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:47:31,087:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:47:31,186:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:31,237:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:31,259:INFO:Calculating mean and std
2024-09-08 11:47:31,259:INFO:Creating metrics dataframe
2024-09-08 11:47:31,264:INFO:Uploading results into container
2024-09-08 11:47:31,264:INFO:Uploading model into container now
2024-09-08 11:47:31,264:INFO:_master_model_container: 8
2024-09-08 11:47:31,264:INFO:_display_container: 2
2024-09-08 11:47:31,264:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 11:47:31,264:INFO:create_model() successfully completed......................................
2024-09-08 11:47:31,402:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:31,402:INFO:Creating metrics dataframe
2024-09-08 11:47:31,406:INFO:Initializing Ada Boost Classifier
2024-09-08 11:47:31,406:INFO:Total runtime is 0.4852710286776224 minutes
2024-09-08 11:47:31,406:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:31,406:INFO:Initializing create_model()
2024-09-08 11:47:31,406:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:31,406:INFO:Checking exceptions
2024-09-08 11:47:31,406:INFO:Importing libraries
2024-09-08 11:47:31,406:INFO:Copying training dataset
2024-09-08 11:47:31,421:INFO:Defining folds
2024-09-08 11:47:31,421:INFO:Declaring metric variables
2024-09-08 11:47:31,421:INFO:Importing untrained model
2024-09-08 11:47:31,421:INFO:Ada Boost Classifier Imported successfully
2024-09-08 11:47:31,421:INFO:Starting cross validation
2024-09-08 11:47:31,426:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:32,222:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:47:32,241:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:47:32,247:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:47:32,247:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:47:32,260:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:47:32,272:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:47:32,277:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:47:32,286:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:47:32,869:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:32,891:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:32,907:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:32,912:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:32,918:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:32,918:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:32,931:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:32,942:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:33,307:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:47:33,316:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:47:33,759:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:33,768:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:33,791:INFO:Calculating mean and std
2024-09-08 11:47:33,791:INFO:Creating metrics dataframe
2024-09-08 11:47:33,791:INFO:Uploading results into container
2024-09-08 11:47:33,791:INFO:Uploading model into container now
2024-09-08 11:47:33,797:INFO:_master_model_container: 9
2024-09-08 11:47:33,797:INFO:_display_container: 2
2024-09-08 11:47:33,797:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 11:47:33,797:INFO:create_model() successfully completed......................................
2024-09-08 11:47:33,926:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:33,926:INFO:Creating metrics dataframe
2024-09-08 11:47:33,937:INFO:Initializing Gradient Boosting Classifier
2024-09-08 11:47:33,937:INFO:Total runtime is 0.5274463375409444 minutes
2024-09-08 11:47:33,937:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:33,937:INFO:Initializing create_model()
2024-09-08 11:47:33,937:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:33,937:INFO:Checking exceptions
2024-09-08 11:47:33,937:INFO:Importing libraries
2024-09-08 11:47:33,937:INFO:Copying training dataset
2024-09-08 11:47:33,951:INFO:Defining folds
2024-09-08 11:47:33,951:INFO:Declaring metric variables
2024-09-08 11:47:33,951:INFO:Importing untrained model
2024-09-08 11:47:33,951:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 11:47:33,951:INFO:Starting cross validation
2024-09-08 11:47:33,962:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:36,997:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:37,014:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:37,038:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:37,063:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:37,072:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:37,087:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:37,097:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:37,102:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:39,177:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:39,192:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:39,213:INFO:Calculating mean and std
2024-09-08 11:47:39,213:INFO:Creating metrics dataframe
2024-09-08 11:47:39,217:INFO:Uploading results into container
2024-09-08 11:47:39,217:INFO:Uploading model into container now
2024-09-08 11:47:39,217:INFO:_master_model_container: 10
2024-09-08 11:47:39,217:INFO:_display_container: 2
2024-09-08 11:47:39,217:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 11:47:39,217:INFO:create_model() successfully completed......................................
2024-09-08 11:47:39,354:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:39,354:INFO:Creating metrics dataframe
2024-09-08 11:47:39,357:INFO:Initializing Linear Discriminant Analysis
2024-09-08 11:47:39,357:INFO:Total runtime is 0.6177846272786458 minutes
2024-09-08 11:47:39,357:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:39,357:INFO:Initializing create_model()
2024-09-08 11:47:39,357:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:39,362:INFO:Checking exceptions
2024-09-08 11:47:39,362:INFO:Importing libraries
2024-09-08 11:47:39,362:INFO:Copying training dataset
2024-09-08 11:47:39,377:INFO:Defining folds
2024-09-08 11:47:39,377:INFO:Declaring metric variables
2024-09-08 11:47:39,377:INFO:Importing untrained model
2024-09-08 11:47:39,377:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 11:47:39,377:INFO:Starting cross validation
2024-09-08 11:47:39,386:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:40,407:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:40,412:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:40,422:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:40,444:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:40,459:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:40,463:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:40,480:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:40,499:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:41,009:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:41,038:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:47:41,065:INFO:Calculating mean and std
2024-09-08 11:47:41,067:INFO:Creating metrics dataframe
2024-09-08 11:47:41,067:INFO:Uploading results into container
2024-09-08 11:47:41,067:INFO:Uploading model into container now
2024-09-08 11:47:41,067:INFO:_master_model_container: 11
2024-09-08 11:47:41,067:INFO:_display_container: 2
2024-09-08 11:47:41,067:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 11:47:41,067:INFO:create_model() successfully completed......................................
2024-09-08 11:47:41,204:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:41,204:INFO:Creating metrics dataframe
2024-09-08 11:47:41,207:INFO:Initializing Extra Trees Classifier
2024-09-08 11:47:41,207:INFO:Total runtime is 0.6486196239789327 minutes
2024-09-08 11:47:41,207:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:41,207:INFO:Initializing create_model()
2024-09-08 11:47:41,207:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:41,207:INFO:Checking exceptions
2024-09-08 11:47:41,207:INFO:Importing libraries
2024-09-08 11:47:41,207:INFO:Copying training dataset
2024-09-08 11:47:41,220:INFO:Defining folds
2024-09-08 11:47:41,220:INFO:Declaring metric variables
2024-09-08 11:47:41,220:INFO:Importing untrained model
2024-09-08 11:47:41,227:INFO:Extra Trees Classifier Imported successfully
2024-09-08 11:47:41,227:INFO:Starting cross validation
2024-09-08 11:47:41,232:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:44,159:INFO:Calculating mean and std
2024-09-08 11:47:44,159:INFO:Creating metrics dataframe
2024-09-08 11:47:44,159:INFO:Uploading results into container
2024-09-08 11:47:44,159:INFO:Uploading model into container now
2024-09-08 11:47:44,159:INFO:_master_model_container: 12
2024-09-08 11:47:44,167:INFO:_display_container: 2
2024-09-08 11:47:44,167:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 11:47:44,167:INFO:create_model() successfully completed......................................
2024-09-08 11:47:44,319:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:44,319:INFO:Creating metrics dataframe
2024-09-08 11:47:44,319:INFO:Initializing Extreme Gradient Boosting
2024-09-08 11:47:44,319:INFO:Total runtime is 0.7004879037539165 minutes
2024-09-08 11:47:44,319:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:44,328:INFO:Initializing create_model()
2024-09-08 11:47:44,328:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:44,328:INFO:Checking exceptions
2024-09-08 11:47:44,328:INFO:Importing libraries
2024-09-08 11:47:44,328:INFO:Copying training dataset
2024-09-08 11:47:44,347:INFO:Defining folds
2024-09-08 11:47:44,347:INFO:Declaring metric variables
2024-09-08 11:47:44,347:INFO:Importing untrained model
2024-09-08 11:47:44,347:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:47:44,351:INFO:Starting cross validation
2024-09-08 11:47:44,357:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:48,025:INFO:Calculating mean and std
2024-09-08 11:47:48,028:INFO:Creating metrics dataframe
2024-09-08 11:47:48,028:INFO:Uploading results into container
2024-09-08 11:47:48,028:INFO:Uploading model into container now
2024-09-08 11:47:48,032:INFO:_master_model_container: 13
2024-09-08 11:47:48,032:INFO:_display_container: 2
2024-09-08 11:47:48,034:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 11:47:48,034:INFO:create_model() successfully completed......................................
2024-09-08 11:47:48,168:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:48,168:INFO:Creating metrics dataframe
2024-09-08 11:47:48,171:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 11:47:48,171:INFO:Total runtime is 0.7646789948145549 minutes
2024-09-08 11:47:48,171:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:48,171:INFO:Initializing create_model()
2024-09-08 11:47:48,171:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:48,171:INFO:Checking exceptions
2024-09-08 11:47:48,171:INFO:Importing libraries
2024-09-08 11:47:48,171:INFO:Copying training dataset
2024-09-08 11:47:48,187:INFO:Defining folds
2024-09-08 11:47:48,187:INFO:Declaring metric variables
2024-09-08 11:47:48,188:INFO:Importing untrained model
2024-09-08 11:47:48,188:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:47:48,188:INFO:Starting cross validation
2024-09-08 11:47:48,188:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:55,468:INFO:Calculating mean and std
2024-09-08 11:47:55,468:INFO:Creating metrics dataframe
2024-09-08 11:47:55,473:INFO:Uploading results into container
2024-09-08 11:47:55,473:INFO:Uploading model into container now
2024-09-08 11:47:55,473:INFO:_master_model_container: 14
2024-09-08 11:47:55,473:INFO:_display_container: 2
2024-09-08 11:47:55,473:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:47:55,473:INFO:create_model() successfully completed......................................
2024-09-08 11:47:55,620:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:55,620:INFO:Creating metrics dataframe
2024-09-08 11:47:55,620:INFO:Initializing Dummy Classifier
2024-09-08 11:47:55,620:INFO:Total runtime is 0.8888283212979635 minutes
2024-09-08 11:47:55,620:INFO:SubProcess create_model() called ==================================
2024-09-08 11:47:55,620:INFO:Initializing create_model()
2024-09-08 11:47:55,620:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209FBFDB0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:55,620:INFO:Checking exceptions
2024-09-08 11:47:55,620:INFO:Importing libraries
2024-09-08 11:47:55,620:INFO:Copying training dataset
2024-09-08 11:47:55,638:INFO:Defining folds
2024-09-08 11:47:55,638:INFO:Declaring metric variables
2024-09-08 11:47:55,638:INFO:Importing untrained model
2024-09-08 11:47:55,638:INFO:Dummy Classifier Imported successfully
2024-09-08 11:47:55,638:INFO:Starting cross validation
2024-09-08 11:47:55,648:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:47:56,688:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:56,708:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:56,708:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:56,723:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:56,730:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:56,778:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:56,778:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:56,842:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:57,288:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:57,323:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:47:57,350:INFO:Calculating mean and std
2024-09-08 11:47:57,350:INFO:Creating metrics dataframe
2024-09-08 11:47:57,350:INFO:Uploading results into container
2024-09-08 11:47:57,350:INFO:Uploading model into container now
2024-09-08 11:47:57,350:INFO:_master_model_container: 15
2024-09-08 11:47:57,350:INFO:_display_container: 2
2024-09-08 11:47:57,350:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 11:47:57,350:INFO:create_model() successfully completed......................................
2024-09-08 11:47:57,508:INFO:SubProcess create_model() end ==================================
2024-09-08 11:47:57,508:INFO:Creating metrics dataframe
2024-09-08 11:47:57,518:INFO:Initializing create_model()
2024-09-08 11:47:57,518:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:57,518:INFO:Checking exceptions
2024-09-08 11:47:57,518:INFO:Importing libraries
2024-09-08 11:47:57,518:INFO:Copying training dataset
2024-09-08 11:47:57,528:INFO:Defining folds
2024-09-08 11:47:57,528:INFO:Declaring metric variables
2024-09-08 11:47:57,528:INFO:Importing untrained model
2024-09-08 11:47:57,528:INFO:Declaring custom model
2024-09-08 11:47:57,539:INFO:Random Forest Classifier Imported successfully
2024-09-08 11:47:57,548:INFO:Cross validation set to False
2024-09-08 11:47:57,548:INFO:Fitting Model
2024-09-08 11:47:58,389:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 11:47:58,389:INFO:create_model() successfully completed......................................
2024-09-08 11:47:58,539:INFO:Initializing create_model()
2024-09-08 11:47:58,539:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:58,539:INFO:Checking exceptions
2024-09-08 11:47:58,539:INFO:Importing libraries
2024-09-08 11:47:58,539:INFO:Copying training dataset
2024-09-08 11:47:58,555:INFO:Defining folds
2024-09-08 11:47:58,555:INFO:Declaring metric variables
2024-09-08 11:47:58,555:INFO:Importing untrained model
2024-09-08 11:47:58,555:INFO:Declaring custom model
2024-09-08 11:47:58,555:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:47:58,564:INFO:Cross validation set to False
2024-09-08 11:47:58,564:INFO:Fitting Model
2024-09-08 11:47:58,964:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 11:47:58,966:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000564 seconds.
2024-09-08 11:47:58,968:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 11:47:58,968:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 11:47:58,968:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 11:47:58,968:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 11:47:58,971:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 11:47:58,971:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 11:47:58,971:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 11:47:58,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:58,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:47:59,419:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:47:59,419:INFO:create_model() successfully completed......................................
2024-09-08 11:47:59,568:INFO:Initializing create_model()
2024-09-08 11:47:59,568:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:47:59,568:INFO:Checking exceptions
2024-09-08 11:47:59,578:INFO:Importing libraries
2024-09-08 11:47:59,578:INFO:Copying training dataset
2024-09-08 11:47:59,594:INFO:Defining folds
2024-09-08 11:47:59,594:INFO:Declaring metric variables
2024-09-08 11:47:59,594:INFO:Importing untrained model
2024-09-08 11:47:59,594:INFO:Declaring custom model
2024-09-08 11:47:59,598:INFO:Extra Trees Classifier Imported successfully
2024-09-08 11:47:59,606:INFO:Cross validation set to False
2024-09-08 11:47:59,606:INFO:Fitting Model
2024-09-08 11:48:00,264:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 11:48:00,264:INFO:create_model() successfully completed......................................
2024-09-08 11:48:00,437:INFO:_master_model_container: 15
2024-09-08 11:48:00,437:INFO:_display_container: 2
2024-09-08 11:48:00,439:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)]
2024-09-08 11:48:00,439:INFO:compare_models() successfully completed......................................
2024-09-08 11:48:00,439:INFO:Initializing create_model()
2024-09-08 11:48:00,439:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:48:00,439:INFO:Checking exceptions
2024-09-08 11:48:00,443:INFO:Importing libraries
2024-09-08 11:48:00,443:INFO:Copying training dataset
2024-09-08 11:48:00,462:INFO:Defining folds
2024-09-08 11:48:00,462:INFO:Declaring metric variables
2024-09-08 11:48:00,462:INFO:Importing untrained model
2024-09-08 11:48:00,462:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:48:00,465:INFO:Starting cross validation
2024-09-08 11:48:00,469:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:48:07,631:INFO:Calculating mean and std
2024-09-08 11:48:07,631:INFO:Creating metrics dataframe
2024-09-08 11:48:07,631:INFO:Finalizing model
2024-09-08 11:48:07,989:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 11:48:07,991:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001025 seconds.
2024-09-08 11:48:07,992:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-09-08 11:48:07,992:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 11:48:07,992:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 11:48:07,992:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 11:48:07,992:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 11:48:07,992:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 11:48:07,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:07,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:07,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:07,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:48:08,329:INFO:Uploading results into container
2024-09-08 11:48:08,329:INFO:Uploading model into container now
2024-09-08 11:48:08,350:INFO:_master_model_container: 16
2024-09-08 11:48:08,350:INFO:_display_container: 3
2024-09-08 11:48:08,350:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:48:08,350:INFO:create_model() successfully completed......................................
2024-09-08 11:48:08,501:INFO:Initializing create_model()
2024-09-08 11:48:08,505:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:48:08,505:INFO:Checking exceptions
2024-09-08 11:48:08,505:INFO:Importing libraries
2024-09-08 11:48:08,505:INFO:Copying training dataset
2024-09-08 11:48:08,519:INFO:Defining folds
2024-09-08 11:48:08,519:INFO:Declaring metric variables
2024-09-08 11:48:08,519:INFO:Importing untrained model
2024-09-08 11:48:08,522:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:48:08,522:INFO:Starting cross validation
2024-09-08 11:48:08,530:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:48:11,729:INFO:Calculating mean and std
2024-09-08 11:48:11,729:INFO:Creating metrics dataframe
2024-09-08 11:48:11,729:INFO:Finalizing model
2024-09-08 11:48:12,604:INFO:Uploading results into container
2024-09-08 11:48:12,604:INFO:Uploading model into container now
2024-09-08 11:48:12,630:INFO:_master_model_container: 17
2024-09-08 11:48:12,630:INFO:_display_container: 4
2024-09-08 11:48:12,630:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:48:12,630:INFO:create_model() successfully completed......................................
2024-09-08 11:48:12,784:INFO:Initializing create_model()
2024-09-08 11:48:12,784:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:48:12,784:INFO:Checking exceptions
2024-09-08 11:48:12,790:INFO:Importing libraries
2024-09-08 11:48:12,790:INFO:Copying training dataset
2024-09-08 11:48:12,800:INFO:Defining folds
2024-09-08 11:48:12,800:INFO:Declaring metric variables
2024-09-08 11:48:12,800:INFO:Importing untrained model
2024-09-08 11:48:12,800:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 11:48:12,800:INFO:Starting cross validation
2024-09-08 11:48:12,810:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:48:15,870:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:48:15,900:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:48:15,902:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:48:16,040:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:48:16,633:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:48:16,640:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:48:16,699:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:48:16,770:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:48:18,631:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:48:18,670:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:48:18,694:INFO:Calculating mean and std
2024-09-08 11:48:18,694:INFO:Creating metrics dataframe
2024-09-08 11:48:18,694:INFO:Finalizing model
2024-09-08 11:48:20,680:INFO:Uploading results into container
2024-09-08 11:48:20,680:INFO:Uploading model into container now
2024-09-08 11:48:20,700:INFO:_master_model_container: 18
2024-09-08 11:48:20,700:INFO:_display_container: 5
2024-09-08 11:48:20,700:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 11:48:20,700:INFO:create_model() successfully completed......................................
2024-09-08 11:48:20,843:INFO:Initializing tune_model()
2024-09-08 11:48:20,843:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>)
2024-09-08 11:48:20,843:INFO:Checking exceptions
2024-09-08 11:48:20,843:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 11:48:20,920:INFO:Copying training dataset
2024-09-08 11:48:20,929:INFO:Checking base model
2024-09-08 11:48:20,931:INFO:Base model : Extreme Gradient Boosting
2024-09-08 11:48:20,931:INFO:Declaring metric variables
2024-09-08 11:48:20,931:INFO:Defining Hyperparameters
2024-09-08 11:48:21,080:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 11:48:21,080:INFO:Tuning with n_jobs=-1
2024-09-08 11:48:21,090:INFO:Initializing skopt.BayesSearchCV
2024-09-08 11:48:35,562:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.6), ('actual_estimator__gamma', 0), ('actual_estimator__learning_rate', 0.01), ('actual_estimator__max_depth', 7), ('actual_estimator__min_child_weight', 1), ('actual_estimator__n_estimators', 100), ('actual_estimator__subsample', 0.6)])
2024-09-08 11:48:35,566:INFO:Hyperparameter search completed
2024-09-08 11:48:35,566:INFO:SubProcess create_model() called ==================================
2024-09-08 11:48:35,566:INFO:Initializing create_model()
2024-09-08 11:48:35,566:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000209F4F439D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.6})
2024-09-08 11:48:35,566:INFO:Checking exceptions
2024-09-08 11:48:35,566:INFO:Importing libraries
2024-09-08 11:48:35,566:INFO:Copying training dataset
2024-09-08 11:48:35,581:INFO:Defining folds
2024-09-08 11:48:35,581:INFO:Declaring metric variables
2024-09-08 11:48:35,581:INFO:Importing untrained model
2024-09-08 11:48:35,581:INFO:Declaring custom model
2024-09-08 11:48:35,581:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:48:35,581:INFO:Starting cross validation
2024-09-08 11:48:35,592:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:48:37,323:INFO:Calculating mean and std
2024-09-08 11:48:37,323:INFO:Creating metrics dataframe
2024-09-08 11:48:37,323:INFO:Finalizing model
2024-09-08 11:48:38,222:INFO:Uploading results into container
2024-09-08 11:48:38,222:INFO:Uploading model into container now
2024-09-08 11:48:38,222:INFO:_master_model_container: 19
2024-09-08 11:48:38,222:INFO:_display_container: 6
2024-09-08 11:48:38,226:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:48:38,226:INFO:create_model() successfully completed......................................
2024-09-08 11:48:38,381:INFO:SubProcess create_model() end ==================================
2024-09-08 11:48:38,381:INFO:choose_better activated
2024-09-08 11:48:38,381:INFO:SubProcess create_model() called ==================================
2024-09-08 11:48:38,381:INFO:Initializing create_model()
2024-09-08 11:48:38,384:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:48:38,384:INFO:Checking exceptions
2024-09-08 11:48:38,384:INFO:Importing libraries
2024-09-08 11:48:38,384:INFO:Copying training dataset
2024-09-08 11:48:38,396:INFO:Defining folds
2024-09-08 11:48:38,396:INFO:Declaring metric variables
2024-09-08 11:48:38,396:INFO:Importing untrained model
2024-09-08 11:48:38,396:INFO:Declaring custom model
2024-09-08 11:48:38,402:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:48:38,402:INFO:Starting cross validation
2024-09-08 11:48:38,402:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:48:39,939:INFO:Calculating mean and std
2024-09-08 11:48:39,941:INFO:Creating metrics dataframe
2024-09-08 11:48:39,941:INFO:Finalizing model
2024-09-08 11:48:40,765:INFO:Uploading results into container
2024-09-08 11:48:40,765:INFO:Uploading model into container now
2024-09-08 11:48:40,772:INFO:_master_model_container: 20
2024-09-08 11:48:40,772:INFO:_display_container: 7
2024-09-08 11:48:40,772:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:48:40,772:INFO:create_model() successfully completed......................................
2024-09-08 11:48:40,922:INFO:SubProcess create_model() end ==================================
2024-09-08 11:48:40,922:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8186
2024-09-08 11:48:40,926:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8386
2024-09-08 11:48:40,926:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 11:48:40,926:INFO:choose_better completed
2024-09-08 11:48:40,942:INFO:_master_model_container: 20
2024-09-08 11:48:40,942:INFO:_display_container: 6
2024-09-08 11:48:40,942:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:48:40,942:INFO:tune_model() successfully completed......................................
2024-09-08 11:48:41,094:INFO:Initializing predict_model()
2024-09-08 11:48:41,094:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000209DA361480>)
2024-09-08 11:48:41,094:INFO:Checking exceptions
2024-09-08 11:48:41,094:INFO:Preloading libraries
2024-09-08 11:48:41,616:INFO:Initializing get_config()
2024-09-08 11:48:41,616:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, variable=X_train)
2024-09-08 11:48:41,622:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 11:48:41,677:INFO:Variable:  returned as       id Marital status Application mode Application order  ... Curricular units 2nd sem (without evaluations) Unemployment rate Inflation rate   GDP
765  765              1                1                 6  ...                                              0         10.800000            1.4  1.74
323  323              1               17                 1  ...                                              0          7.600000            2.6  0.32
909  909              1                1                 1  ...                                              0         12.400000            0.5  1.79
774  774              1               17                 1  ...                                              0          8.900000            1.4  3.51
931  931              1               39                 1  ...                                              0         16.200001            0.3 -0.92
..   ...            ...              ...               ...  ...                                            ...               ...            ...   ...
806  806              4               39                 1  ...                                              0         10.800000            1.4  1.74
492  492              1                1                 3  ...                                              0          7.600000            2.6  0.32
698  698              1               39                 1  ...                                              1         10.800000            1.4  1.74
44    44              1                1                 1  ...                                              0          9.400000           -0.8 -3.12
604  604              1                1                 1  ...                                              0         10.800000            1.4  1.74

[700 rows x 37 columns]
2024-09-08 11:48:41,677:INFO:get_config() successfully completed......................................
2024-09-08 11:48:41,682:INFO:Initializing predict_model()
2024-09-08 11:48:41,682:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000209FBFADC60>)
2024-09-08 11:48:41,682:INFO:Checking exceptions
2024-09-08 11:48:41,682:INFO:Preloading libraries
2024-09-08 11:48:41,682:INFO:Set up data.
2024-09-08 11:48:41,692:INFO:Set up index.
2024-09-08 11:48:42,055:INFO:Initializing get_config()
2024-09-08 11:48:42,055:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, variable=y_train)
2024-09-08 11:48:42,059:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 11:48:42,062:INFO:Variable:  returned as 765    0
323    1
909    0
774    0
931    1
      ..
806    1
492    0
698    1
44     0
604    1
Name: Target, Length: 700, dtype: int8
2024-09-08 11:48:42,062:INFO:get_config() successfully completed......................................
2024-09-08 11:48:42,062:INFO:Initializing get_config()
2024-09-08 11:48:42,062:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, variable=y_test)
2024-09-08 11:48:42,062:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 11:48:42,062:INFO:Variable:  returned as 47     1
233    1
132    2
34     0
162    0
      ..
772    0
632    0
365    2
136    0
299    1
Name: Target, Length: 300, dtype: int8
2024-09-08 11:48:42,062:INFO:get_config() successfully completed......................................
2024-09-08 11:48:42,062:INFO:Initializing finalize_model()
2024-09-08 11:48:42,062:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 11:48:42,071:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:48:42,071:INFO:Initializing create_model()
2024-09-08 11:48:42,071:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:48:42,071:INFO:Checking exceptions
2024-09-08 11:48:42,082:INFO:Importing libraries
2024-09-08 11:48:42,082:INFO:Copying training dataset
2024-09-08 11:48:42,082:INFO:Defining folds
2024-09-08 11:48:42,082:INFO:Declaring metric variables
2024-09-08 11:48:42,082:INFO:Importing untrained model
2024-09-08 11:48:42,082:INFO:Declaring custom model
2024-09-08 11:48:42,082:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:48:42,092:INFO:Cross validation set to False
2024-09-08 11:48:42,092:INFO:Fitting Model
2024-09-08 11:48:43,402:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 11:48:43,402:INFO:create_model() successfully completed......................................
2024-09-08 11:48:43,537:INFO:_master_model_container: 20
2024-09-08 11:48:43,537:INFO:_display_container: 7
2024-09-08 11:48:43,806:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 11:48:43,806:INFO:finalize_model() successfully completed......................................
2024-09-08 11:48:44,247:INFO:Initializing predict_model()
2024-09-08 11:48:44,247:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000209EEEB7400>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000209FC2E4E50>)
2024-09-08 11:48:44,247:INFO:Checking exceptions
2024-09-08 11:48:44,247:INFO:Preloading libraries
2024-09-08 11:48:44,247:INFO:Set up data.
2024-09-08 11:48:44,277:INFO:Set up index.
2024-09-08 11:52:23,748:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:52:23,748:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:52:23,748:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:52:23,748:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 11:52:50,360:INFO:PyCaret ClassificationExperiment
2024-09-08 11:52:50,360:INFO:Logging name: clf-default-name
2024-09-08 11:52:50,360:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 11:52:50,365:INFO:version 3.3.2
2024-09-08 11:52:50,365:INFO:Initializing setup()
2024-09-08 11:52:50,366:INFO:self.USI: f325
2024-09-08 11:52:50,366:INFO:self._variable_keys: {'X_train', 'gpu_n_jobs_param', 'pipeline', 'gpu_param', 'logging_param', 'exp_name_log', 'exp_id', 'memory', 'fold_groups_param', 'target_param', 'fix_imbalance', 'X_test', 'data', 'html_param', 'y', 'fold_shuffle_param', 'n_jobs_param', 'y_train', 'idx', 'seed', '_available_plots', '_ml_usecase', 'USI', 'log_plots_param', 'X', 'fold_generator', 'y_test', 'is_multiclass'}
2024-09-08 11:52:50,366:INFO:Checking environment
2024-09-08 11:52:50,366:INFO:python_version: 3.10.11
2024-09-08 11:52:50,366:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 11:52:50,366:INFO:machine: AMD64
2024-09-08 11:52:50,380:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 11:52:50,380:INFO:Memory: svmem(total=16407719936, available=5854982144, percent=64.3, used=10552737792, free=5854982144)
2024-09-08 11:52:50,380:INFO:Physical Core: 4
2024-09-08 11:52:50,380:INFO:Logical Core: 8
2024-09-08 11:52:50,380:INFO:Checking libraries
2024-09-08 11:52:50,386:INFO:System:
2024-09-08 11:52:50,387:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 11:52:50,387:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 11:52:50,387:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 11:52:50,387:INFO:PyCaret required dependencies:
2024-09-08 11:52:50,492:INFO:                 pip: 24.2
2024-09-08 11:52:50,499:INFO:          setuptools: 72.1.0
2024-09-08 11:52:50,499:INFO:             pycaret: 3.3.2
2024-09-08 11:52:50,499:INFO:             IPython: 8.25.0
2024-09-08 11:52:50,500:INFO:          ipywidgets: 8.1.5
2024-09-08 11:52:50,500:INFO:                tqdm: 4.66.5
2024-09-08 11:52:50,500:INFO:               numpy: 1.26.4
2024-09-08 11:52:50,500:INFO:              pandas: 2.1.4
2024-09-08 11:52:50,500:INFO:              jinja2: 3.1.4
2024-09-08 11:52:50,500:INFO:               scipy: 1.11.4
2024-09-08 11:52:50,500:INFO:              joblib: 1.3.2
2024-09-08 11:52:50,500:INFO:             sklearn: 1.4.2
2024-09-08 11:52:50,504:INFO:                pyod: 2.0.1
2024-09-08 11:52:50,505:INFO:            imblearn: 0.12.3
2024-09-08 11:52:50,505:INFO:   category_encoders: 2.6.3
2024-09-08 11:52:50,505:INFO:            lightgbm: 4.5.0
2024-09-08 11:52:50,506:INFO:               numba: 0.60.0
2024-09-08 11:52:50,506:INFO:            requests: 2.32.3
2024-09-08 11:52:50,506:INFO:          matplotlib: 3.7.5
2024-09-08 11:52:50,506:INFO:          scikitplot: 0.3.7
2024-09-08 11:52:50,506:INFO:         yellowbrick: 1.5
2024-09-08 11:52:50,507:INFO:              plotly: 5.24.0
2024-09-08 11:52:50,507:INFO:    plotly-resampler: Not installed
2024-09-08 11:52:50,507:INFO:             kaleido: 0.2.1
2024-09-08 11:52:50,507:INFO:           schemdraw: 0.15
2024-09-08 11:52:50,507:INFO:         statsmodels: 0.14.2
2024-09-08 11:52:50,507:INFO:              sktime: 0.26.0
2024-09-08 11:52:50,507:INFO:               tbats: 1.1.3
2024-09-08 11:52:50,507:INFO:            pmdarima: 2.0.4
2024-09-08 11:52:50,507:INFO:              psutil: 5.9.0
2024-09-08 11:52:50,507:INFO:          markupsafe: 2.1.3
2024-09-08 11:52:50,507:INFO:             pickle5: Not installed
2024-09-08 11:52:50,507:INFO:         cloudpickle: 3.0.0
2024-09-08 11:52:50,507:INFO:         deprecation: 2.1.0
2024-09-08 11:52:50,507:INFO:              xxhash: 3.5.0
2024-09-08 11:52:50,507:INFO:           wurlitzer: Not installed
2024-09-08 11:52:50,507:INFO:PyCaret optional dependencies:
2024-09-08 11:52:50,530:INFO:                shap: Not installed
2024-09-08 11:52:50,530:INFO:           interpret: Not installed
2024-09-08 11:52:50,530:INFO:                umap: Not installed
2024-09-08 11:52:50,530:INFO:     ydata_profiling: Not installed
2024-09-08 11:52:50,530:INFO:  explainerdashboard: Not installed
2024-09-08 11:52:50,530:INFO:             autoviz: Not installed
2024-09-08 11:52:50,530:INFO:           fairlearn: Not installed
2024-09-08 11:52:50,530:INFO:          deepchecks: Not installed
2024-09-08 11:52:50,530:INFO:             xgboost: 2.1.1
2024-09-08 11:52:50,530:INFO:            catboost: Not installed
2024-09-08 11:52:50,530:INFO:              kmodes: Not installed
2024-09-08 11:52:50,530:INFO:             mlxtend: Not installed
2024-09-08 11:52:50,530:INFO:       statsforecast: Not installed
2024-09-08 11:52:50,530:INFO:        tune_sklearn: Not installed
2024-09-08 11:52:50,530:INFO:                 ray: Not installed
2024-09-08 11:52:50,530:INFO:            hyperopt: 0.2.7
2024-09-08 11:52:50,530:INFO:              optuna: 4.0.0
2024-09-08 11:52:50,530:INFO:               skopt: 0.10.2
2024-09-08 11:52:50,530:INFO:              mlflow: Not installed
2024-09-08 11:52:50,530:INFO:              gradio: Not installed
2024-09-08 11:52:50,530:INFO:             fastapi: Not installed
2024-09-08 11:52:50,530:INFO:             uvicorn: Not installed
2024-09-08 11:52:50,530:INFO:              m2cgen: Not installed
2024-09-08 11:52:50,530:INFO:           evidently: Not installed
2024-09-08 11:52:50,532:INFO:               fugue: Not installed
2024-09-08 11:52:50,532:INFO:           streamlit: 1.38.0
2024-09-08 11:52:50,532:INFO:             prophet: Not installed
2024-09-08 11:52:50,532:INFO:None
2024-09-08 11:52:50,532:INFO:Set up data.
2024-09-08 11:52:50,550:INFO:Set up folding strategy.
2024-09-08 11:52:50,550:INFO:Set up train/test split.
2024-09-08 11:52:50,580:INFO:Set up index.
2024-09-08 11:52:50,580:INFO:Assigning column types.
2024-09-08 11:52:50,592:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 11:52:50,670:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 11:52:50,680:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:52:50,747:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:52:50,750:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:52:50,830:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 11:52:50,840:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:52:50,890:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:52:50,898:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:52:50,898:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 11:52:50,990:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:52:51,045:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:52:51,050:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:52:51,140:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 11:52:51,210:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:52:51,210:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:52:51,210:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 11:52:51,392:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:52:51,395:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:52:51,543:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:52:51,550:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:52:51,550:INFO:Preparing preprocessing pipeline...
2024-09-08 11:52:51,558:INFO:Set up simple imputation.
2024-09-08 11:52:51,576:INFO:Set up encoding of ordinal features.
2024-09-08 11:52:51,641:INFO:Set up encoding of categorical features.
2024-09-08 11:52:51,641:INFO:Set up column name cleaning.
2024-09-08 11:52:52,110:INFO:Finished creating preprocessing pipeline.
2024-09-08 11:52:52,420:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                                                    'qualification',
                                                                    'Fathers '
                                                                    'qualification',
                                                                    'Mothers '
                                                                    'occupation',
                                                                    'Fathers '
                                                                    'occupation'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 11:52:52,420:INFO:Creating final display dataframe.
2024-09-08 11:52:52,860:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (1000, 38)
4        Transformed data shape       (1000, 129)
5   Transformed train set shape        (700, 129)
6    Transformed test set shape        (300, 129)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              f325
2024-09-08 11:52:53,020:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:52:53,025:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:52:53,174:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 11:52:53,174:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 11:52:53,182:INFO:setup() successfully completed in 2.85s...............
2024-09-08 11:52:53,182:INFO:Initializing compare_models()
2024-09-08 11:52:53,182:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 11:52:53,182:INFO:Checking exceptions
2024-09-08 11:52:53,191:INFO:Preparing display monitor
2024-09-08 11:52:53,200:INFO:Initializing Logistic Regression
2024-09-08 11:52:53,200:INFO:Total runtime is 0.0 minutes
2024-09-08 11:52:53,200:INFO:SubProcess create_model() called ==================================
2024-09-08 11:52:53,200:INFO:Initializing create_model()
2024-09-08 11:52:53,200:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:52:53,200:INFO:Checking exceptions
2024-09-08 11:52:53,200:INFO:Importing libraries
2024-09-08 11:52:53,200:INFO:Copying training dataset
2024-09-08 11:52:53,216:INFO:Defining folds
2024-09-08 11:52:53,216:INFO:Declaring metric variables
2024-09-08 11:52:53,220:INFO:Importing untrained model
2024-09-08 11:52:53,220:INFO:Logistic Regression Imported successfully
2024-09-08 11:52:53,220:INFO:Starting cross validation
2024-09-08 11:52:53,232:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:03,691:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:53:03,717:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:53:03,761:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:53:03,834:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:53:03,965:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:04,006:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:04,031:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:04,123:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:04,341:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:53:04,391:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:53:04,535:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:53:04,576:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:53:04,625:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:04,658:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:04,821:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:04,824:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:05,951:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:53:06,006:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 11:53:06,091:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:06,146:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:06,180:INFO:Calculating mean and std
2024-09-08 11:53:06,181:INFO:Creating metrics dataframe
2024-09-08 11:53:06,181:INFO:Uploading results into container
2024-09-08 11:53:06,186:INFO:Uploading model into container now
2024-09-08 11:53:06,186:INFO:_master_model_container: 1
2024-09-08 11:53:06,186:INFO:_display_container: 2
2024-09-08 11:53:06,186:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 11:53:06,188:INFO:create_model() successfully completed......................................
2024-09-08 11:53:06,321:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:06,321:INFO:Creating metrics dataframe
2024-09-08 11:53:06,326:INFO:Initializing K Neighbors Classifier
2024-09-08 11:53:06,326:INFO:Total runtime is 0.21877341667811076 minutes
2024-09-08 11:53:06,326:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:06,326:INFO:Initializing create_model()
2024-09-08 11:53:06,327:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:06,328:INFO:Checking exceptions
2024-09-08 11:53:06,328:INFO:Importing libraries
2024-09-08 11:53:06,328:INFO:Copying training dataset
2024-09-08 11:53:06,344:INFO:Defining folds
2024-09-08 11:53:06,344:INFO:Declaring metric variables
2024-09-08 11:53:06,344:INFO:Importing untrained model
2024-09-08 11:53:06,344:INFO:K Neighbors Classifier Imported successfully
2024-09-08 11:53:06,344:INFO:Starting cross validation
2024-09-08 11:53:06,352:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:08,560:INFO:Calculating mean and std
2024-09-08 11:53:08,562:INFO:Creating metrics dataframe
2024-09-08 11:53:08,564:INFO:Uploading results into container
2024-09-08 11:53:08,564:INFO:Uploading model into container now
2024-09-08 11:53:08,564:INFO:_master_model_container: 2
2024-09-08 11:53:08,564:INFO:_display_container: 2
2024-09-08 11:53:08,564:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 11:53:08,564:INFO:create_model() successfully completed......................................
2024-09-08 11:53:08,713:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:08,713:INFO:Creating metrics dataframe
2024-09-08 11:53:08,721:INFO:Initializing Naive Bayes
2024-09-08 11:53:08,721:INFO:Total runtime is 0.2586882988611857 minutes
2024-09-08 11:53:08,721:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:08,721:INFO:Initializing create_model()
2024-09-08 11:53:08,721:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:08,721:INFO:Checking exceptions
2024-09-08 11:53:08,721:INFO:Importing libraries
2024-09-08 11:53:08,721:INFO:Copying training dataset
2024-09-08 11:53:08,746:INFO:Defining folds
2024-09-08 11:53:08,746:INFO:Declaring metric variables
2024-09-08 11:53:08,746:INFO:Importing untrained model
2024-09-08 11:53:08,751:INFO:Naive Bayes Imported successfully
2024-09-08 11:53:08,751:INFO:Starting cross validation
2024-09-08 11:53:08,754:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:09,955:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:10,652:INFO:Calculating mean and std
2024-09-08 11:53:10,652:INFO:Creating metrics dataframe
2024-09-08 11:53:10,657:INFO:Uploading results into container
2024-09-08 11:53:10,657:INFO:Uploading model into container now
2024-09-08 11:53:10,657:INFO:_master_model_container: 3
2024-09-08 11:53:10,657:INFO:_display_container: 2
2024-09-08 11:53:10,657:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 11:53:10,657:INFO:create_model() successfully completed......................................
2024-09-08 11:53:10,796:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:10,796:INFO:Creating metrics dataframe
2024-09-08 11:53:10,804:INFO:Initializing Decision Tree Classifier
2024-09-08 11:53:10,804:INFO:Total runtime is 0.2934046467145284 minutes
2024-09-08 11:53:10,804:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:10,804:INFO:Initializing create_model()
2024-09-08 11:53:10,804:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:10,804:INFO:Checking exceptions
2024-09-08 11:53:10,804:INFO:Importing libraries
2024-09-08 11:53:10,804:INFO:Copying training dataset
2024-09-08 11:53:10,820:INFO:Defining folds
2024-09-08 11:53:10,822:INFO:Declaring metric variables
2024-09-08 11:53:10,822:INFO:Importing untrained model
2024-09-08 11:53:10,822:INFO:Decision Tree Classifier Imported successfully
2024-09-08 11:53:10,822:INFO:Starting cross validation
2024-09-08 11:53:10,829:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:12,732:INFO:Calculating mean and std
2024-09-08 11:53:12,733:INFO:Creating metrics dataframe
2024-09-08 11:53:12,733:INFO:Uploading results into container
2024-09-08 11:53:12,733:INFO:Uploading model into container now
2024-09-08 11:53:12,733:INFO:_master_model_container: 4
2024-09-08 11:53:12,733:INFO:_display_container: 2
2024-09-08 11:53:12,733:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 11:53:12,733:INFO:create_model() successfully completed......................................
2024-09-08 11:53:12,873:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:12,873:INFO:Creating metrics dataframe
2024-09-08 11:53:12,882:INFO:Initializing SVM - Linear Kernel
2024-09-08 11:53:12,882:INFO:Total runtime is 0.3280328472455343 minutes
2024-09-08 11:53:12,882:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:12,886:INFO:Initializing create_model()
2024-09-08 11:53:12,886:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:12,886:INFO:Checking exceptions
2024-09-08 11:53:12,886:INFO:Importing libraries
2024-09-08 11:53:12,886:INFO:Copying training dataset
2024-09-08 11:53:12,898:INFO:Defining folds
2024-09-08 11:53:12,898:INFO:Declaring metric variables
2024-09-08 11:53:12,898:INFO:Importing untrained model
2024-09-08 11:53:12,901:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 11:53:12,901:INFO:Starting cross validation
2024-09-08 11:53:12,907:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:13,993:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:14,007:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:14,022:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:14,043:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:14,052:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:14,062:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:14,077:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:14,092:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:14,094:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:14,102:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:14,127:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:14,657:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:14,666:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:14,700:INFO:Calculating mean and std
2024-09-08 11:53:14,702:INFO:Creating metrics dataframe
2024-09-08 11:53:14,702:INFO:Uploading results into container
2024-09-08 11:53:14,702:INFO:Uploading model into container now
2024-09-08 11:53:14,702:INFO:_master_model_container: 5
2024-09-08 11:53:14,702:INFO:_display_container: 2
2024-09-08 11:53:14,702:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 11:53:14,702:INFO:create_model() successfully completed......................................
2024-09-08 11:53:14,856:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:14,856:INFO:Creating metrics dataframe
2024-09-08 11:53:14,865:INFO:Initializing Ridge Classifier
2024-09-08 11:53:14,865:INFO:Total runtime is 0.3610798796017965 minutes
2024-09-08 11:53:14,865:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:14,865:INFO:Initializing create_model()
2024-09-08 11:53:14,865:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:14,865:INFO:Checking exceptions
2024-09-08 11:53:14,865:INFO:Importing libraries
2024-09-08 11:53:14,865:INFO:Copying training dataset
2024-09-08 11:53:14,882:INFO:Defining folds
2024-09-08 11:53:14,882:INFO:Declaring metric variables
2024-09-08 11:53:14,882:INFO:Importing untrained model
2024-09-08 11:53:14,882:INFO:Ridge Classifier Imported successfully
2024-09-08 11:53:14,882:INFO:Starting cross validation
2024-09-08 11:53:14,892:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:15,962:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:15,994:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:15,994:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:16,010:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:16,012:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:16,012:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:16,055:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:16,083:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:16,577:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:16,602:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:16,635:INFO:Calculating mean and std
2024-09-08 11:53:16,635:INFO:Creating metrics dataframe
2024-09-08 11:53:16,635:INFO:Uploading results into container
2024-09-08 11:53:16,635:INFO:Uploading model into container now
2024-09-08 11:53:16,642:INFO:_master_model_container: 6
2024-09-08 11:53:16,642:INFO:_display_container: 2
2024-09-08 11:53:16,643:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 11:53:16,643:INFO:create_model() successfully completed......................................
2024-09-08 11:53:16,784:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:16,784:INFO:Creating metrics dataframe
2024-09-08 11:53:16,788:INFO:Initializing Random Forest Classifier
2024-09-08 11:53:16,788:INFO:Total runtime is 0.39313883781433107 minutes
2024-09-08 11:53:16,788:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:16,789:INFO:Initializing create_model()
2024-09-08 11:53:16,789:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:16,789:INFO:Checking exceptions
2024-09-08 11:53:16,789:INFO:Importing libraries
2024-09-08 11:53:16,789:INFO:Copying training dataset
2024-09-08 11:53:16,802:INFO:Defining folds
2024-09-08 11:53:16,802:INFO:Declaring metric variables
2024-09-08 11:53:16,802:INFO:Importing untrained model
2024-09-08 11:53:16,802:INFO:Random Forest Classifier Imported successfully
2024-09-08 11:53:16,802:INFO:Starting cross validation
2024-09-08 11:53:16,810:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:20,152:INFO:Calculating mean and std
2024-09-08 11:53:20,162:INFO:Creating metrics dataframe
2024-09-08 11:53:20,162:INFO:Uploading results into container
2024-09-08 11:53:20,162:INFO:Uploading model into container now
2024-09-08 11:53:20,170:INFO:_master_model_container: 7
2024-09-08 11:53:20,170:INFO:_display_container: 2
2024-09-08 11:53:20,170:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 11:53:20,170:INFO:create_model() successfully completed......................................
2024-09-08 11:53:20,316:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:20,316:INFO:Creating metrics dataframe
2024-09-08 11:53:20,322:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 11:53:20,322:INFO:Total runtime is 0.4520357569058736 minutes
2024-09-08 11:53:20,322:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:20,324:INFO:Initializing create_model()
2024-09-08 11:53:20,324:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:20,324:INFO:Checking exceptions
2024-09-08 11:53:20,324:INFO:Importing libraries
2024-09-08 11:53:20,324:INFO:Copying training dataset
2024-09-08 11:53:20,337:INFO:Defining folds
2024-09-08 11:53:20,337:INFO:Declaring metric variables
2024-09-08 11:53:20,337:INFO:Importing untrained model
2024-09-08 11:53:20,340:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 11:53:20,341:INFO:Starting cross validation
2024-09-08 11:53:20,347:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:21,187:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:53:21,218:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:53:21,347:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:53:21,359:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:53:21,582:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:21,633:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:21,682:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:53:21,689:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:53:21,727:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:21,727:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:21,747:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:53:21,787:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:53:22,020:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:22,042:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:22,063:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:22,120:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:22,384:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:53:22,417:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 11:53:22,565:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:22,593:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:22,617:INFO:Calculating mean and std
2024-09-08 11:53:22,617:INFO:Creating metrics dataframe
2024-09-08 11:53:22,617:INFO:Uploading results into container
2024-09-08 11:53:22,617:INFO:Uploading model into container now
2024-09-08 11:53:22,623:INFO:_master_model_container: 8
2024-09-08 11:53:22,623:INFO:_display_container: 2
2024-09-08 11:53:22,623:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 11:53:22,623:INFO:create_model() successfully completed......................................
2024-09-08 11:53:22,772:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:22,772:INFO:Creating metrics dataframe
2024-09-08 11:53:22,774:INFO:Initializing Ada Boost Classifier
2024-09-08 11:53:22,774:INFO:Total runtime is 0.49289548794428506 minutes
2024-09-08 11:53:22,774:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:22,774:INFO:Initializing create_model()
2024-09-08 11:53:22,774:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:22,774:INFO:Checking exceptions
2024-09-08 11:53:22,774:INFO:Importing libraries
2024-09-08 11:53:22,774:INFO:Copying training dataset
2024-09-08 11:53:22,799:INFO:Defining folds
2024-09-08 11:53:22,799:INFO:Declaring metric variables
2024-09-08 11:53:22,799:INFO:Importing untrained model
2024-09-08 11:53:22,799:INFO:Ada Boost Classifier Imported successfully
2024-09-08 11:53:22,799:INFO:Starting cross validation
2024-09-08 11:53:22,802:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:23,529:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:53:23,546:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:53:23,572:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:53:23,712:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:53:23,728:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:53:23,743:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:53:23,757:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:53:23,812:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:53:24,193:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:24,218:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:24,253:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:24,393:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:24,409:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:24,434:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:24,434:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:24,493:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:24,733:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:53:24,734:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 11:53:25,123:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:25,123:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:25,168:INFO:Calculating mean and std
2024-09-08 11:53:25,168:INFO:Creating metrics dataframe
2024-09-08 11:53:25,173:INFO:Uploading results into container
2024-09-08 11:53:25,173:INFO:Uploading model into container now
2024-09-08 11:53:25,173:INFO:_master_model_container: 9
2024-09-08 11:53:25,173:INFO:_display_container: 2
2024-09-08 11:53:25,176:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 11:53:25,176:INFO:create_model() successfully completed......................................
2024-09-08 11:53:25,308:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:25,308:INFO:Creating metrics dataframe
2024-09-08 11:53:25,316:INFO:Initializing Gradient Boosting Classifier
2024-09-08 11:53:25,316:INFO:Total runtime is 0.5352630972862243 minutes
2024-09-08 11:53:25,316:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:25,316:INFO:Initializing create_model()
2024-09-08 11:53:25,316:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:25,316:INFO:Checking exceptions
2024-09-08 11:53:25,316:INFO:Importing libraries
2024-09-08 11:53:25,316:INFO:Copying training dataset
2024-09-08 11:53:25,324:INFO:Defining folds
2024-09-08 11:53:25,324:INFO:Declaring metric variables
2024-09-08 11:53:25,324:INFO:Importing untrained model
2024-09-08 11:53:25,324:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 11:53:25,332:INFO:Starting cross validation
2024-09-08 11:53:25,338:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:28,306:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:28,315:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:28,323:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:28,380:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:28,404:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:28,408:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:28,439:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:28,467:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:30,554:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:30,563:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:30,596:INFO:Calculating mean and std
2024-09-08 11:53:30,596:INFO:Creating metrics dataframe
2024-09-08 11:53:30,596:INFO:Uploading results into container
2024-09-08 11:53:30,596:INFO:Uploading model into container now
2024-09-08 11:53:30,603:INFO:_master_model_container: 10
2024-09-08 11:53:30,603:INFO:_display_container: 2
2024-09-08 11:53:30,605:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 11:53:30,605:INFO:create_model() successfully completed......................................
2024-09-08 11:53:30,738:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:30,738:INFO:Creating metrics dataframe
2024-09-08 11:53:30,743:INFO:Initializing Linear Discriminant Analysis
2024-09-08 11:53:30,743:INFO:Total runtime is 0.625716495513916 minutes
2024-09-08 11:53:30,743:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:30,743:INFO:Initializing create_model()
2024-09-08 11:53:30,743:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:30,743:INFO:Checking exceptions
2024-09-08 11:53:30,743:INFO:Importing libraries
2024-09-08 11:53:30,746:INFO:Copying training dataset
2024-09-08 11:53:30,754:INFO:Defining folds
2024-09-08 11:53:30,754:INFO:Declaring metric variables
2024-09-08 11:53:30,754:INFO:Importing untrained model
2024-09-08 11:53:30,754:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 11:53:30,754:INFO:Starting cross validation
2024-09-08 11:53:30,763:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:31,805:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:31,814:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:31,814:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:31,822:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:31,833:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:31,873:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:31,873:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:31,913:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:32,414:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:32,423:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:53:32,451:INFO:Calculating mean and std
2024-09-08 11:53:32,453:INFO:Creating metrics dataframe
2024-09-08 11:53:32,453:INFO:Uploading results into container
2024-09-08 11:53:32,453:INFO:Uploading model into container now
2024-09-08 11:53:32,453:INFO:_master_model_container: 11
2024-09-08 11:53:32,453:INFO:_display_container: 2
2024-09-08 11:53:32,453:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 11:53:32,453:INFO:create_model() successfully completed......................................
2024-09-08 11:53:32,593:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:32,593:INFO:Creating metrics dataframe
2024-09-08 11:53:32,593:INFO:Initializing Extra Trees Classifier
2024-09-08 11:53:32,593:INFO:Total runtime is 0.6565545598665873 minutes
2024-09-08 11:53:32,593:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:32,600:INFO:Initializing create_model()
2024-09-08 11:53:32,600:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:32,600:INFO:Checking exceptions
2024-09-08 11:53:32,600:INFO:Importing libraries
2024-09-08 11:53:32,600:INFO:Copying training dataset
2024-09-08 11:53:32,614:INFO:Defining folds
2024-09-08 11:53:32,614:INFO:Declaring metric variables
2024-09-08 11:53:32,614:INFO:Importing untrained model
2024-09-08 11:53:32,614:INFO:Extra Trees Classifier Imported successfully
2024-09-08 11:53:32,617:INFO:Starting cross validation
2024-09-08 11:53:32,623:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:35,429:INFO:Calculating mean and std
2024-09-08 11:53:35,431:INFO:Creating metrics dataframe
2024-09-08 11:53:35,433:INFO:Uploading results into container
2024-09-08 11:53:35,433:INFO:Uploading model into container now
2024-09-08 11:53:35,433:INFO:_master_model_container: 12
2024-09-08 11:53:35,433:INFO:_display_container: 2
2024-09-08 11:53:35,433:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 11:53:35,433:INFO:create_model() successfully completed......................................
2024-09-08 11:53:35,573:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:35,573:INFO:Creating metrics dataframe
2024-09-08 11:53:35,578:INFO:Initializing Extreme Gradient Boosting
2024-09-08 11:53:35,578:INFO:Total runtime is 0.7063043077786764 minutes
2024-09-08 11:53:35,581:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:35,581:INFO:Initializing create_model()
2024-09-08 11:53:35,581:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:35,581:INFO:Checking exceptions
2024-09-08 11:53:35,581:INFO:Importing libraries
2024-09-08 11:53:35,581:INFO:Copying training dataset
2024-09-08 11:53:35,593:INFO:Defining folds
2024-09-08 11:53:35,597:INFO:Declaring metric variables
2024-09-08 11:53:35,597:INFO:Importing untrained model
2024-09-08 11:53:35,597:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:53:35,597:INFO:Starting cross validation
2024-09-08 11:53:35,605:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:39,081:INFO:Calculating mean and std
2024-09-08 11:53:39,081:INFO:Creating metrics dataframe
2024-09-08 11:53:39,084:INFO:Uploading results into container
2024-09-08 11:53:39,084:INFO:Uploading model into container now
2024-09-08 11:53:39,084:INFO:_master_model_container: 13
2024-09-08 11:53:39,084:INFO:_display_container: 2
2024-09-08 11:53:39,089:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 11:53:39,089:INFO:create_model() successfully completed......................................
2024-09-08 11:53:39,233:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:39,233:INFO:Creating metrics dataframe
2024-09-08 11:53:39,240:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 11:53:39,240:INFO:Total runtime is 0.7673269232114156 minutes
2024-09-08 11:53:39,240:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:39,240:INFO:Initializing create_model()
2024-09-08 11:53:39,240:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:39,240:INFO:Checking exceptions
2024-09-08 11:53:39,240:INFO:Importing libraries
2024-09-08 11:53:39,240:INFO:Copying training dataset
2024-09-08 11:53:39,256:INFO:Defining folds
2024-09-08 11:53:39,258:INFO:Declaring metric variables
2024-09-08 11:53:39,258:INFO:Importing untrained model
2024-09-08 11:53:39,258:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:53:39,258:INFO:Starting cross validation
2024-09-08 11:53:39,264:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:46,144:INFO:Calculating mean and std
2024-09-08 11:53:46,144:INFO:Creating metrics dataframe
2024-09-08 11:53:46,150:INFO:Uploading results into container
2024-09-08 11:53:46,150:INFO:Uploading model into container now
2024-09-08 11:53:46,150:INFO:_master_model_container: 14
2024-09-08 11:53:46,150:INFO:_display_container: 2
2024-09-08 11:53:46,150:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:53:46,150:INFO:create_model() successfully completed......................................
2024-09-08 11:53:46,299:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:46,299:INFO:Creating metrics dataframe
2024-09-08 11:53:46,304:INFO:Initializing Dummy Classifier
2024-09-08 11:53:46,304:INFO:Total runtime is 0.8850695172945658 minutes
2024-09-08 11:53:46,304:INFO:SubProcess create_model() called ==================================
2024-09-08 11:53:46,304:INFO:Initializing create_model()
2024-09-08 11:53:46,304:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CC4AF80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:46,304:INFO:Checking exceptions
2024-09-08 11:53:46,304:INFO:Importing libraries
2024-09-08 11:53:46,304:INFO:Copying training dataset
2024-09-08 11:53:46,316:INFO:Defining folds
2024-09-08 11:53:46,316:INFO:Declaring metric variables
2024-09-08 11:53:46,316:INFO:Importing untrained model
2024-09-08 11:53:46,316:INFO:Dummy Classifier Imported successfully
2024-09-08 11:53:46,316:INFO:Starting cross validation
2024-09-08 11:53:46,324:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:47,364:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:47,371:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:47,374:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:47,409:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:47,424:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:47,446:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:47,454:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:47,496:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:47,974:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:47,977:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 11:53:48,002:INFO:Calculating mean and std
2024-09-08 11:53:48,004:INFO:Creating metrics dataframe
2024-09-08 11:53:48,004:INFO:Uploading results into container
2024-09-08 11:53:48,004:INFO:Uploading model into container now
2024-09-08 11:53:48,009:INFO:_master_model_container: 15
2024-09-08 11:53:48,009:INFO:_display_container: 2
2024-09-08 11:53:48,009:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 11:53:48,009:INFO:create_model() successfully completed......................................
2024-09-08 11:53:48,136:INFO:SubProcess create_model() end ==================================
2024-09-08 11:53:48,136:INFO:Creating metrics dataframe
2024-09-08 11:53:48,144:INFO:Initializing create_model()
2024-09-08 11:53:48,144:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:48,144:INFO:Checking exceptions
2024-09-08 11:53:48,150:INFO:Importing libraries
2024-09-08 11:53:48,150:INFO:Copying training dataset
2024-09-08 11:53:48,161:INFO:Defining folds
2024-09-08 11:53:48,161:INFO:Declaring metric variables
2024-09-08 11:53:48,161:INFO:Importing untrained model
2024-09-08 11:53:48,161:INFO:Declaring custom model
2024-09-08 11:53:48,164:INFO:Random Forest Classifier Imported successfully
2024-09-08 11:53:48,169:INFO:Cross validation set to False
2024-09-08 11:53:48,169:INFO:Fitting Model
2024-09-08 11:53:48,838:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 11:53:48,838:INFO:create_model() successfully completed......................................
2024-09-08 11:53:48,975:INFO:Initializing create_model()
2024-09-08 11:53:48,975:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:48,975:INFO:Checking exceptions
2024-09-08 11:53:48,975:INFO:Importing libraries
2024-09-08 11:53:48,975:INFO:Copying training dataset
2024-09-08 11:53:48,994:INFO:Defining folds
2024-09-08 11:53:48,994:INFO:Declaring metric variables
2024-09-08 11:53:48,994:INFO:Importing untrained model
2024-09-08 11:53:48,994:INFO:Declaring custom model
2024-09-08 11:53:48,994:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:53:49,006:INFO:Cross validation set to False
2024-09-08 11:53:49,006:INFO:Fitting Model
2024-09-08 11:53:49,464:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 11:53:49,469:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000470 seconds.
2024-09-08 11:53:49,469:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 11:53:49,469:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 11:53:49,469:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 11:53:49,469:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 11:53:49,469:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 11:53:49,469:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 11:53:49,469:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 11:53:49,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,584:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:49,894:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:53:49,897:INFO:create_model() successfully completed......................................
2024-09-08 11:53:50,074:INFO:Initializing create_model()
2024-09-08 11:53:50,074:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:50,074:INFO:Checking exceptions
2024-09-08 11:53:50,074:INFO:Importing libraries
2024-09-08 11:53:50,079:INFO:Copying training dataset
2024-09-08 11:53:50,103:INFO:Defining folds
2024-09-08 11:53:50,104:INFO:Declaring metric variables
2024-09-08 11:53:50,105:INFO:Importing untrained model
2024-09-08 11:53:50,105:INFO:Declaring custom model
2024-09-08 11:53:50,108:INFO:Extra Trees Classifier Imported successfully
2024-09-08 11:53:50,119:INFO:Cross validation set to False
2024-09-08 11:53:50,119:INFO:Fitting Model
2024-09-08 11:53:50,855:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 11:53:50,855:INFO:create_model() successfully completed......................................
2024-09-08 11:53:51,030:INFO:_master_model_container: 15
2024-09-08 11:53:51,034:INFO:_display_container: 2
2024-09-08 11:53:51,065:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)]
2024-09-08 11:53:51,065:INFO:compare_models() successfully completed......................................
2024-09-08 11:53:51,065:INFO:Initializing create_model()
2024-09-08 11:53:51,065:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:51,065:INFO:Checking exceptions
2024-09-08 11:53:51,090:INFO:Importing libraries
2024-09-08 11:53:51,090:INFO:Copying training dataset
2024-09-08 11:53:51,119:INFO:Defining folds
2024-09-08 11:53:51,119:INFO:Declaring metric variables
2024-09-08 11:53:51,119:INFO:Importing untrained model
2024-09-08 11:53:51,119:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 11:53:51,124:INFO:Starting cross validation
2024-09-08 11:53:51,134:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:53:58,785:INFO:Calculating mean and std
2024-09-08 11:53:58,785:INFO:Creating metrics dataframe
2024-09-08 11:53:58,788:INFO:Finalizing model
2024-09-08 11:53:59,139:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 11:53:59,139:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001089 seconds.
2024-09-08 11:53:59,139:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-09-08 11:53:59,139:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 11:53:59,139:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 11:53:59,145:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 11:53:59,145:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 11:53:59,145:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 11:53:59,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 11:53:59,500:INFO:Uploading results into container
2024-09-08 11:53:59,505:INFO:Uploading model into container now
2024-09-08 11:53:59,525:INFO:_master_model_container: 16
2024-09-08 11:53:59,525:INFO:_display_container: 3
2024-09-08 11:53:59,528:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 11:53:59,528:INFO:create_model() successfully completed......................................
2024-09-08 11:53:59,676:INFO:Initializing create_model()
2024-09-08 11:53:59,676:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:53:59,676:INFO:Checking exceptions
2024-09-08 11:53:59,678:INFO:Importing libraries
2024-09-08 11:53:59,678:INFO:Copying training dataset
2024-09-08 11:53:59,690:INFO:Defining folds
2024-09-08 11:53:59,690:INFO:Declaring metric variables
2024-09-08 11:53:59,690:INFO:Importing untrained model
2024-09-08 11:53:59,694:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:53:59,695:INFO:Starting cross validation
2024-09-08 11:53:59,695:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:54:03,081:INFO:Calculating mean and std
2024-09-08 11:54:03,081:INFO:Creating metrics dataframe
2024-09-08 11:54:03,085:INFO:Finalizing model
2024-09-08 11:54:03,906:INFO:Uploading results into container
2024-09-08 11:54:03,906:INFO:Uploading model into container now
2024-09-08 11:54:03,932:INFO:_master_model_container: 17
2024-09-08 11:54:03,932:INFO:_display_container: 4
2024-09-08 11:54:03,934:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:54:03,934:INFO:create_model() successfully completed......................................
2024-09-08 11:54:04,085:INFO:Initializing create_model()
2024-09-08 11:54:04,085:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:54:04,085:INFO:Checking exceptions
2024-09-08 11:54:04,085:INFO:Importing libraries
2024-09-08 11:54:04,090:INFO:Copying training dataset
2024-09-08 11:54:04,100:INFO:Defining folds
2024-09-08 11:54:04,105:INFO:Declaring metric variables
2024-09-08 11:54:04,105:INFO:Importing untrained model
2024-09-08 11:54:04,106:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 11:54:04,106:INFO:Starting cross validation
2024-09-08 11:54:04,110:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:54:07,161:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:54:07,266:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:54:07,300:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:54:07,311:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:54:07,571:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:54:07,616:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:54:07,651:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:54:07,687:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:54:09,637:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:54:09,646:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 11:54:09,669:INFO:Calculating mean and std
2024-09-08 11:54:09,669:INFO:Creating metrics dataframe
2024-09-08 11:54:09,669:INFO:Finalizing model
2024-09-08 11:54:12,011:INFO:Uploading results into container
2024-09-08 11:54:12,011:INFO:Uploading model into container now
2024-09-08 11:54:12,032:INFO:_master_model_container: 18
2024-09-08 11:54:12,032:INFO:_display_container: 5
2024-09-08 11:54:12,036:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 11:54:12,036:INFO:create_model() successfully completed......................................
2024-09-08 11:54:12,169:INFO:Initializing tune_model()
2024-09-08 11:54:12,169:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>)
2024-09-08 11:54:12,169:INFO:Checking exceptions
2024-09-08 11:54:12,169:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 11:54:12,246:INFO:Copying training dataset
2024-09-08 11:54:12,256:INFO:Checking base model
2024-09-08 11:54:12,256:INFO:Base model : Extreme Gradient Boosting
2024-09-08 11:54:12,256:INFO:Declaring metric variables
2024-09-08 11:54:12,256:INFO:Defining Hyperparameters
2024-09-08 11:54:12,406:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 11:54:12,406:INFO:Tuning with n_jobs=-1
2024-09-08 11:54:12,411:INFO:Initializing skopt.BayesSearchCV
2024-09-08 11:54:27,798:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.6), ('actual_estimator__gamma', 0), ('actual_estimator__learning_rate', 0.01), ('actual_estimator__max_depth', 7), ('actual_estimator__min_child_weight', 1), ('actual_estimator__n_estimators', 100), ('actual_estimator__subsample', 0.6)])
2024-09-08 11:54:27,798:INFO:Hyperparameter search completed
2024-09-08 11:54:27,798:INFO:SubProcess create_model() called ==================================
2024-09-08 11:54:27,798:INFO:Initializing create_model()
2024-09-08 11:54:27,798:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022D8CB56D40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.6})
2024-09-08 11:54:27,798:INFO:Checking exceptions
2024-09-08 11:54:27,798:INFO:Importing libraries
2024-09-08 11:54:27,798:INFO:Copying training dataset
2024-09-08 11:54:27,814:INFO:Defining folds
2024-09-08 11:54:27,814:INFO:Declaring metric variables
2024-09-08 11:54:27,814:INFO:Importing untrained model
2024-09-08 11:54:27,814:INFO:Declaring custom model
2024-09-08 11:54:27,818:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:54:27,818:INFO:Starting cross validation
2024-09-08 11:54:27,822:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:54:29,463:INFO:Calculating mean and std
2024-09-08 11:54:29,463:INFO:Creating metrics dataframe
2024-09-08 11:54:29,467:INFO:Finalizing model
2024-09-08 11:54:30,457:INFO:Uploading results into container
2024-09-08 11:54:30,457:INFO:Uploading model into container now
2024-09-08 11:54:30,457:INFO:_master_model_container: 19
2024-09-08 11:54:30,457:INFO:_display_container: 6
2024-09-08 11:54:30,464:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:54:30,464:INFO:create_model() successfully completed......................................
2024-09-08 11:54:30,617:INFO:SubProcess create_model() end ==================================
2024-09-08 11:54:30,617:INFO:choose_better activated
2024-09-08 11:54:30,617:INFO:SubProcess create_model() called ==================================
2024-09-08 11:54:30,621:INFO:Initializing create_model()
2024-09-08 11:54:30,621:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:54:30,621:INFO:Checking exceptions
2024-09-08 11:54:30,621:INFO:Importing libraries
2024-09-08 11:54:30,621:INFO:Copying training dataset
2024-09-08 11:54:30,638:INFO:Defining folds
2024-09-08 11:54:30,638:INFO:Declaring metric variables
2024-09-08 11:54:30,638:INFO:Importing untrained model
2024-09-08 11:54:30,638:INFO:Declaring custom model
2024-09-08 11:54:30,638:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:54:30,638:INFO:Starting cross validation
2024-09-08 11:54:30,647:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 11:54:32,147:INFO:Calculating mean and std
2024-09-08 11:54:32,147:INFO:Creating metrics dataframe
2024-09-08 11:54:32,147:INFO:Finalizing model
2024-09-08 11:54:32,938:INFO:Uploading results into container
2024-09-08 11:54:32,946:INFO:Uploading model into container now
2024-09-08 11:54:32,948:INFO:_master_model_container: 20
2024-09-08 11:54:32,948:INFO:_display_container: 7
2024-09-08 11:54:32,948:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:54:32,948:INFO:create_model() successfully completed......................................
2024-09-08 11:54:33,101:INFO:SubProcess create_model() end ==================================
2024-09-08 11:54:33,101:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8186
2024-09-08 11:54:33,106:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8386
2024-09-08 11:54:33,106:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 11:54:33,106:INFO:choose_better completed
2024-09-08 11:54:33,122:INFO:_master_model_container: 20
2024-09-08 11:54:33,122:INFO:_display_container: 6
2024-09-08 11:54:33,122:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:54:33,122:INFO:tune_model() successfully completed......................................
2024-09-08 11:54:33,267:INFO:Initializing predict_model()
2024-09-08 11:54:33,267:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022D88061480>)
2024-09-08 11:54:33,267:INFO:Checking exceptions
2024-09-08 11:54:33,267:INFO:Preloading libraries
2024-09-08 11:54:33,788:INFO:Initializing get_config()
2024-09-08 11:54:33,788:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, variable=X_train)
2024-09-08 11:54:33,788:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 11:54:33,848:INFO:Variable:  returned as       id Marital status Application mode Application order  ... Curricular units 2nd sem (without evaluations) Unemployment rate Inflation rate   GDP
765  765              1                1                 6  ...                                              0         10.800000            1.4  1.74
323  323              1               17                 1  ...                                              0          7.600000            2.6  0.32
909  909              1                1                 1  ...                                              0         12.400000            0.5  1.79
774  774              1               17                 1  ...                                              0          8.900000            1.4  3.51
931  931              1               39                 1  ...                                              0         16.200001            0.3 -0.92
..   ...            ...              ...               ...  ...                                            ...               ...            ...   ...
806  806              4               39                 1  ...                                              0         10.800000            1.4  1.74
492  492              1                1                 3  ...                                              0          7.600000            2.6  0.32
698  698              1               39                 1  ...                                              1         10.800000            1.4  1.74
44    44              1                1                 1  ...                                              0          9.400000           -0.8 -3.12
604  604              1                1                 1  ...                                              0         10.800000            1.4  1.74

[700 rows x 37 columns]
2024-09-08 11:54:33,848:INFO:get_config() successfully completed......................................
2024-09-08 11:54:33,848:INFO:Initializing predict_model()
2024-09-08 11:54:33,848:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022D8CC1DC60>)
2024-09-08 11:54:33,848:INFO:Checking exceptions
2024-09-08 11:54:33,848:INFO:Preloading libraries
2024-09-08 11:54:33,848:INFO:Set up data.
2024-09-08 11:54:33,858:INFO:Set up index.
2024-09-08 11:54:34,217:INFO:Initializing get_config()
2024-09-08 11:54:34,217:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, variable=y_train)
2024-09-08 11:54:34,228:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 11:54:34,228:INFO:Variable:  returned as 765    0
323    1
909    0
774    0
931    1
      ..
806    1
492    0
698    1
44     0
604    1
Name: Target, Length: 700, dtype: int8
2024-09-08 11:54:34,228:INFO:get_config() successfully completed......................................
2024-09-08 11:54:34,228:INFO:Initializing get_config()
2024-09-08 11:54:34,228:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, variable=y_test)
2024-09-08 11:54:34,228:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 11:54:34,238:INFO:Variable:  returned as 47     1
233    1
132    2
34     0
162    0
      ..
772    0
632    0
365    2
136    0
299    1
Name: Target, Length: 300, dtype: int8
2024-09-08 11:54:34,238:INFO:get_config() successfully completed......................................
2024-09-08 11:54:34,238:INFO:Initializing finalize_model()
2024-09-08 11:54:34,238:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 11:54:34,238:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 11:54:34,247:INFO:Initializing create_model()
2024-09-08 11:54:34,247:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 11:54:34,247:INFO:Checking exceptions
2024-09-08 11:54:34,247:INFO:Importing libraries
2024-09-08 11:54:34,247:INFO:Copying training dataset
2024-09-08 11:54:34,253:INFO:Defining folds
2024-09-08 11:54:34,253:INFO:Declaring metric variables
2024-09-08 11:54:34,253:INFO:Importing untrained model
2024-09-08 11:54:34,253:INFO:Declaring custom model
2024-09-08 11:54:34,253:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 11:54:34,259:INFO:Cross validation set to False
2024-09-08 11:54:34,259:INFO:Fitting Model
2024-09-08 11:54:35,588:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 11:54:35,588:INFO:create_model() successfully completed......................................
2024-09-08 11:54:35,718:INFO:_master_model_container: 20
2024-09-08 11:54:35,718:INFO:_display_container: 7
2024-09-08 11:54:36,008:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 11:54:36,008:INFO:finalize_model() successfully completed......................................
2024-09-08 11:54:36,738:INFO:Initializing predict_model()
2024-09-08 11:54:36,738:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022D8AC77A90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022D8CF54E50>)
2024-09-08 11:54:36,738:INFO:Checking exceptions
2024-09-08 11:54:36,738:INFO:Preloading libraries
2024-09-08 11:54:36,738:INFO:Set up data.
2024-09-08 11:54:36,773:INFO:Set up index.
2024-09-08 12:03:19,874:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 12:03:19,874:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 12:03:19,874:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 12:03:19,874:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 12:03:47,199:INFO:PyCaret ClassificationExperiment
2024-09-08 12:03:47,199:INFO:Logging name: clf-default-name
2024-09-08 12:03:47,204:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 12:03:47,204:INFO:version 3.3.2
2024-09-08 12:03:47,204:INFO:Initializing setup()
2024-09-08 12:03:47,204:INFO:self.USI: 213f
2024-09-08 12:03:47,204:INFO:self._variable_keys: {'X_train', 'X_test', 'pipeline', 'data', 'idx', 'exp_name_log', 'fix_imbalance', '_available_plots', 'fold_shuffle_param', '_ml_usecase', 'y_train', 'html_param', 'y', 'fold_generator', 'X', 'gpu_param', 'y_test', 'exp_id', 'gpu_n_jobs_param', 'fold_groups_param', 'seed', 'logging_param', 'is_multiclass', 'USI', 'memory', 'log_plots_param', 'target_param', 'n_jobs_param'}
2024-09-08 12:03:47,204:INFO:Checking environment
2024-09-08 12:03:47,204:INFO:python_version: 3.10.11
2024-09-08 12:03:47,209:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 12:03:47,209:INFO:machine: AMD64
2024-09-08 12:03:47,220:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 12:03:47,220:INFO:Memory: svmem(total=16407719936, available=6505021440, percent=60.4, used=9902698496, free=6505021440)
2024-09-08 12:03:47,220:INFO:Physical Core: 4
2024-09-08 12:03:47,220:INFO:Logical Core: 8
2024-09-08 12:03:47,220:INFO:Checking libraries
2024-09-08 12:03:47,220:INFO:System:
2024-09-08 12:03:47,220:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 12:03:47,220:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 12:03:47,220:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 12:03:47,220:INFO:PyCaret required dependencies:
2024-09-08 12:03:47,338:INFO:                 pip: 24.2
2024-09-08 12:03:47,338:INFO:          setuptools: 72.1.0
2024-09-08 12:03:47,338:INFO:             pycaret: 3.3.2
2024-09-08 12:03:47,338:INFO:             IPython: 8.25.0
2024-09-08 12:03:47,338:INFO:          ipywidgets: 8.1.5
2024-09-08 12:03:47,338:INFO:                tqdm: 4.66.5
2024-09-08 12:03:47,338:INFO:               numpy: 1.26.4
2024-09-08 12:03:47,338:INFO:              pandas: 2.1.4
2024-09-08 12:03:47,338:INFO:              jinja2: 3.1.4
2024-09-08 12:03:47,338:INFO:               scipy: 1.11.4
2024-09-08 12:03:47,338:INFO:              joblib: 1.3.2
2024-09-08 12:03:47,338:INFO:             sklearn: 1.4.2
2024-09-08 12:03:47,338:INFO:                pyod: 2.0.1
2024-09-08 12:03:47,338:INFO:            imblearn: 0.12.3
2024-09-08 12:03:47,338:INFO:   category_encoders: 2.6.3
2024-09-08 12:03:47,338:INFO:            lightgbm: 4.5.0
2024-09-08 12:03:47,338:INFO:               numba: 0.60.0
2024-09-08 12:03:47,338:INFO:            requests: 2.32.3
2024-09-08 12:03:47,338:INFO:          matplotlib: 3.7.5
2024-09-08 12:03:47,338:INFO:          scikitplot: 0.3.7
2024-09-08 12:03:47,338:INFO:         yellowbrick: 1.5
2024-09-08 12:03:47,338:INFO:              plotly: 5.24.0
2024-09-08 12:03:47,338:INFO:    plotly-resampler: Not installed
2024-09-08 12:03:47,338:INFO:             kaleido: 0.2.1
2024-09-08 12:03:47,338:INFO:           schemdraw: 0.15
2024-09-08 12:03:47,338:INFO:         statsmodels: 0.14.2
2024-09-08 12:03:47,338:INFO:              sktime: 0.26.0
2024-09-08 12:03:47,338:INFO:               tbats: 1.1.3
2024-09-08 12:03:47,338:INFO:            pmdarima: 2.0.4
2024-09-08 12:03:47,338:INFO:              psutil: 5.9.0
2024-09-08 12:03:47,338:INFO:          markupsafe: 2.1.3
2024-09-08 12:03:47,338:INFO:             pickle5: Not installed
2024-09-08 12:03:47,338:INFO:         cloudpickle: 3.0.0
2024-09-08 12:03:47,338:INFO:         deprecation: 2.1.0
2024-09-08 12:03:47,338:INFO:              xxhash: 3.5.0
2024-09-08 12:03:47,338:INFO:           wurlitzer: Not installed
2024-09-08 12:03:47,338:INFO:PyCaret optional dependencies:
2024-09-08 12:03:47,360:INFO:                shap: Not installed
2024-09-08 12:03:47,360:INFO:           interpret: Not installed
2024-09-08 12:03:47,360:INFO:                umap: Not installed
2024-09-08 12:03:47,360:INFO:     ydata_profiling: Not installed
2024-09-08 12:03:47,360:INFO:  explainerdashboard: Not installed
2024-09-08 12:03:47,360:INFO:             autoviz: Not installed
2024-09-08 12:03:47,360:INFO:           fairlearn: Not installed
2024-09-08 12:03:47,360:INFO:          deepchecks: Not installed
2024-09-08 12:03:47,360:INFO:             xgboost: 2.1.1
2024-09-08 12:03:47,360:INFO:            catboost: Not installed
2024-09-08 12:03:47,360:INFO:              kmodes: Not installed
2024-09-08 12:03:47,360:INFO:             mlxtend: Not installed
2024-09-08 12:03:47,360:INFO:       statsforecast: Not installed
2024-09-08 12:03:47,360:INFO:        tune_sklearn: Not installed
2024-09-08 12:03:47,360:INFO:                 ray: Not installed
2024-09-08 12:03:47,360:INFO:            hyperopt: 0.2.7
2024-09-08 12:03:47,360:INFO:              optuna: 4.0.0
2024-09-08 12:03:47,360:INFO:               skopt: 0.10.2
2024-09-08 12:03:47,360:INFO:              mlflow: Not installed
2024-09-08 12:03:47,360:INFO:              gradio: Not installed
2024-09-08 12:03:47,360:INFO:             fastapi: Not installed
2024-09-08 12:03:47,360:INFO:             uvicorn: Not installed
2024-09-08 12:03:47,360:INFO:              m2cgen: Not installed
2024-09-08 12:03:47,360:INFO:           evidently: Not installed
2024-09-08 12:03:47,360:INFO:               fugue: Not installed
2024-09-08 12:03:47,360:INFO:           streamlit: 1.38.0
2024-09-08 12:03:47,360:INFO:             prophet: Not installed
2024-09-08 12:03:47,360:INFO:None
2024-09-08 12:03:47,360:INFO:Set up data.
2024-09-08 12:03:47,389:INFO:Set up folding strategy.
2024-09-08 12:03:47,389:INFO:Set up train/test split.
2024-09-08 12:03:47,409:INFO:Set up index.
2024-09-08 12:03:47,409:INFO:Assigning column types.
2024-09-08 12:03:47,419:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 12:03:47,508:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 12:03:47,518:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 12:03:47,719:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:03:47,730:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:03:48,062:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 12:03:48,062:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 12:03:48,228:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:03:48,248:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:03:48,252:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 12:03:48,571:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 12:03:48,793:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:03:48,801:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:03:49,121:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 12:03:49,279:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:03:49,293:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:03:49,299:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 12:03:49,789:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:03:49,809:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:03:50,289:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:03:50,303:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:03:50,309:INFO:Preparing preprocessing pipeline...
2024-09-08 12:03:50,312:INFO:Set up simple imputation.
2024-09-08 12:03:50,329:INFO:Set up encoding of ordinal features.
2024-09-08 12:03:50,361:INFO:Set up encoding of categorical features.
2024-09-08 12:03:50,369:INFO:Set up column name cleaning.
2024-09-08 12:03:50,740:INFO:Finished creating preprocessing pipeline.
2024-09-08 12:03:51,002:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                                                    'qualification',
                                                                    'Fathers '
                                                                    'qualification',
                                                                    'Mothers '
                                                                    'occupation',
                                                                    'Fathers '
                                                                    'occupation'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 12:03:51,002:INFO:Creating final display dataframe.
2024-09-08 12:03:51,443:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (1000, 38)
4        Transformed data shape       (1000, 129)
5   Transformed train set shape        (700, 129)
6    Transformed test set shape        (300, 129)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              213f
2024-09-08 12:03:51,601:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:03:51,609:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:03:51,749:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:03:51,749:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:03:51,756:INFO:setup() successfully completed in 4.59s...............
2024-09-08 12:03:51,756:INFO:Initializing compare_models()
2024-09-08 12:03:51,756:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 12:03:51,756:INFO:Checking exceptions
2024-09-08 12:03:51,764:INFO:Preparing display monitor
2024-09-08 12:03:51,769:INFO:Initializing Logistic Regression
2024-09-08 12:03:51,769:INFO:Total runtime is 0.0 minutes
2024-09-08 12:03:51,769:INFO:SubProcess create_model() called ==================================
2024-09-08 12:03:51,772:INFO:Initializing create_model()
2024-09-08 12:03:51,772:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:03:51,772:INFO:Checking exceptions
2024-09-08 12:03:51,772:INFO:Importing libraries
2024-09-08 12:03:51,772:INFO:Copying training dataset
2024-09-08 12:03:51,779:INFO:Defining folds
2024-09-08 12:03:51,779:INFO:Declaring metric variables
2024-09-08 12:03:51,779:INFO:Importing untrained model
2024-09-08 12:03:51,779:INFO:Logistic Regression Imported successfully
2024-09-08 12:03:51,779:INFO:Starting cross validation
2024-09-08 12:03:51,789:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:02,135:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:04:02,195:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:04:02,302:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:04:02,325:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:04:02,460:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:02,501:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:02,592:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:02,600:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:02,605:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:04:02,720:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:04:02,871:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:04:02,900:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:02,972:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:03,104:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:04,408:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:04:04,456:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:04:04,613:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:04,662:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:04,686:INFO:Calculating mean and std
2024-09-08 12:04:04,686:INFO:Creating metrics dataframe
2024-09-08 12:04:04,690:INFO:Uploading results into container
2024-09-08 12:04:04,690:INFO:Uploading model into container now
2024-09-08 12:04:04,695:INFO:_master_model_container: 1
2024-09-08 12:04:04,695:INFO:_display_container: 2
2024-09-08 12:04:04,695:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 12:04:04,695:INFO:create_model() successfully completed......................................
2024-09-08 12:04:04,861:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:04,868:INFO:Creating metrics dataframe
2024-09-08 12:04:04,870:INFO:Initializing K Neighbors Classifier
2024-09-08 12:04:04,870:INFO:Total runtime is 0.21834937731424967 minutes
2024-09-08 12:04:04,870:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:04,870:INFO:Initializing create_model()
2024-09-08 12:04:04,870:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:04,870:INFO:Checking exceptions
2024-09-08 12:04:04,870:INFO:Importing libraries
2024-09-08 12:04:04,870:INFO:Copying training dataset
2024-09-08 12:04:04,885:INFO:Defining folds
2024-09-08 12:04:04,885:INFO:Declaring metric variables
2024-09-08 12:04:04,885:INFO:Importing untrained model
2024-09-08 12:04:04,890:INFO:K Neighbors Classifier Imported successfully
2024-09-08 12:04:04,890:INFO:Starting cross validation
2024-09-08 12:04:04,893:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:07,036:INFO:Calculating mean and std
2024-09-08 12:04:07,036:INFO:Creating metrics dataframe
2024-09-08 12:04:07,040:INFO:Uploading results into container
2024-09-08 12:04:07,040:INFO:Uploading model into container now
2024-09-08 12:04:07,040:INFO:_master_model_container: 2
2024-09-08 12:04:07,040:INFO:_display_container: 2
2024-09-08 12:04:07,045:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 12:04:07,045:INFO:create_model() successfully completed......................................
2024-09-08 12:04:07,176:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:07,180:INFO:Creating metrics dataframe
2024-09-08 12:04:07,185:INFO:Initializing Naive Bayes
2024-09-08 12:04:07,185:INFO:Total runtime is 0.2569298386573791 minutes
2024-09-08 12:04:07,185:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:07,185:INFO:Initializing create_model()
2024-09-08 12:04:07,185:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:07,185:INFO:Checking exceptions
2024-09-08 12:04:07,185:INFO:Importing libraries
2024-09-08 12:04:07,185:INFO:Copying training dataset
2024-09-08 12:04:07,192:INFO:Defining folds
2024-09-08 12:04:07,200:INFO:Declaring metric variables
2024-09-08 12:04:07,200:INFO:Importing untrained model
2024-09-08 12:04:07,200:INFO:Naive Bayes Imported successfully
2024-09-08 12:04:07,200:INFO:Starting cross validation
2024-09-08 12:04:07,210:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:08,525:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:09,228:INFO:Calculating mean and std
2024-09-08 12:04:09,230:INFO:Creating metrics dataframe
2024-09-08 12:04:09,230:INFO:Uploading results into container
2024-09-08 12:04:09,230:INFO:Uploading model into container now
2024-09-08 12:04:09,230:INFO:_master_model_container: 3
2024-09-08 12:04:09,230:INFO:_display_container: 2
2024-09-08 12:04:09,230:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 12:04:09,230:INFO:create_model() successfully completed......................................
2024-09-08 12:04:09,370:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:09,370:INFO:Creating metrics dataframe
2024-09-08 12:04:09,370:INFO:Initializing Decision Tree Classifier
2024-09-08 12:04:09,370:INFO:Total runtime is 0.293359629313151 minutes
2024-09-08 12:04:09,370:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:09,376:INFO:Initializing create_model()
2024-09-08 12:04:09,376:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:09,376:INFO:Checking exceptions
2024-09-08 12:04:09,376:INFO:Importing libraries
2024-09-08 12:04:09,376:INFO:Copying training dataset
2024-09-08 12:04:09,390:INFO:Defining folds
2024-09-08 12:04:09,390:INFO:Declaring metric variables
2024-09-08 12:04:09,390:INFO:Importing untrained model
2024-09-08 12:04:09,390:INFO:Decision Tree Classifier Imported successfully
2024-09-08 12:04:09,390:INFO:Starting cross validation
2024-09-08 12:04:09,394:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:11,220:INFO:Calculating mean and std
2024-09-08 12:04:11,220:INFO:Creating metrics dataframe
2024-09-08 12:04:11,220:INFO:Uploading results into container
2024-09-08 12:04:11,220:INFO:Uploading model into container now
2024-09-08 12:04:11,220:INFO:_master_model_container: 4
2024-09-08 12:04:11,220:INFO:_display_container: 2
2024-09-08 12:04:11,220:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 12:04:11,220:INFO:create_model() successfully completed......................................
2024-09-08 12:04:11,370:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:11,370:INFO:Creating metrics dataframe
2024-09-08 12:04:11,370:INFO:Initializing SVM - Linear Kernel
2024-09-08 12:04:11,370:INFO:Total runtime is 0.3266939838727315 minutes
2024-09-08 12:04:11,376:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:11,376:INFO:Initializing create_model()
2024-09-08 12:04:11,376:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:11,376:INFO:Checking exceptions
2024-09-08 12:04:11,376:INFO:Importing libraries
2024-09-08 12:04:11,376:INFO:Copying training dataset
2024-09-08 12:04:11,390:INFO:Defining folds
2024-09-08 12:04:11,390:INFO:Declaring metric variables
2024-09-08 12:04:11,390:INFO:Importing untrained model
2024-09-08 12:04:11,393:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 12:04:11,393:INFO:Starting cross validation
2024-09-08 12:04:11,393:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:12,574:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:12,591:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:12,610:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:12,624:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:12,632:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:12,691:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:12,698:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:12,707:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:12,715:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:12,731:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:12,750:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:13,382:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:13,421:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:13,447:INFO:Calculating mean and std
2024-09-08 12:04:13,447:INFO:Creating metrics dataframe
2024-09-08 12:04:13,451:INFO:Uploading results into container
2024-09-08 12:04:13,451:INFO:Uploading model into container now
2024-09-08 12:04:13,451:INFO:_master_model_container: 5
2024-09-08 12:04:13,451:INFO:_display_container: 2
2024-09-08 12:04:13,455:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 12:04:13,455:INFO:create_model() successfully completed......................................
2024-09-08 12:04:13,600:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:13,600:INFO:Creating metrics dataframe
2024-09-08 12:04:13,606:INFO:Initializing Ridge Classifier
2024-09-08 12:04:13,606:INFO:Total runtime is 0.3639450311660767 minutes
2024-09-08 12:04:13,606:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:13,606:INFO:Initializing create_model()
2024-09-08 12:04:13,606:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:13,606:INFO:Checking exceptions
2024-09-08 12:04:13,606:INFO:Importing libraries
2024-09-08 12:04:13,606:INFO:Copying training dataset
2024-09-08 12:04:13,622:INFO:Defining folds
2024-09-08 12:04:13,622:INFO:Declaring metric variables
2024-09-08 12:04:13,622:INFO:Importing untrained model
2024-09-08 12:04:13,630:INFO:Ridge Classifier Imported successfully
2024-09-08 12:04:13,630:INFO:Starting cross validation
2024-09-08 12:04:13,641:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:14,780:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:14,801:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:14,831:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:14,894:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:15,002:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:15,052:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:15,068:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:15,093:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:15,494:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:15,502:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:15,536:INFO:Calculating mean and std
2024-09-08 12:04:15,536:INFO:Creating metrics dataframe
2024-09-08 12:04:15,541:INFO:Uploading results into container
2024-09-08 12:04:15,541:INFO:Uploading model into container now
2024-09-08 12:04:15,541:INFO:_master_model_container: 6
2024-09-08 12:04:15,541:INFO:_display_container: 2
2024-09-08 12:04:15,544:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 12:04:15,544:INFO:create_model() successfully completed......................................
2024-09-08 12:04:15,692:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:15,692:INFO:Creating metrics dataframe
2024-09-08 12:04:15,701:INFO:Initializing Random Forest Classifier
2024-09-08 12:04:15,701:INFO:Total runtime is 0.39886649449666345 minutes
2024-09-08 12:04:15,701:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:15,701:INFO:Initializing create_model()
2024-09-08 12:04:15,701:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:15,701:INFO:Checking exceptions
2024-09-08 12:04:15,701:INFO:Importing libraries
2024-09-08 12:04:15,701:INFO:Copying training dataset
2024-09-08 12:04:15,721:INFO:Defining folds
2024-09-08 12:04:15,721:INFO:Declaring metric variables
2024-09-08 12:04:15,721:INFO:Importing untrained model
2024-09-08 12:04:15,721:INFO:Random Forest Classifier Imported successfully
2024-09-08 12:04:15,721:INFO:Starting cross validation
2024-09-08 12:04:15,731:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:19,311:INFO:Calculating mean and std
2024-09-08 12:04:19,311:INFO:Creating metrics dataframe
2024-09-08 12:04:19,311:INFO:Uploading results into container
2024-09-08 12:04:19,318:INFO:Uploading model into container now
2024-09-08 12:04:19,318:INFO:_master_model_container: 7
2024-09-08 12:04:19,318:INFO:_display_container: 2
2024-09-08 12:04:19,321:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 12:04:19,321:INFO:create_model() successfully completed......................................
2024-09-08 12:04:19,505:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:19,505:INFO:Creating metrics dataframe
2024-09-08 12:04:19,511:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 12:04:19,513:INFO:Total runtime is 0.4624070286750794 minutes
2024-09-08 12:04:19,513:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:19,513:INFO:Initializing create_model()
2024-09-08 12:04:19,513:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:19,513:INFO:Checking exceptions
2024-09-08 12:04:19,513:INFO:Importing libraries
2024-09-08 12:04:19,513:INFO:Copying training dataset
2024-09-08 12:04:19,531:INFO:Defining folds
2024-09-08 12:04:19,537:INFO:Declaring metric variables
2024-09-08 12:04:19,537:INFO:Importing untrained model
2024-09-08 12:04:19,537:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 12:04:19,537:INFO:Starting cross validation
2024-09-08 12:04:19,546:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:20,558:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:04:20,558:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:04:20,572:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:04:20,611:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:04:20,654:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:04:20,654:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:04:20,676:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:04:20,731:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:04:20,858:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:20,886:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:20,896:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:20,924:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:21,022:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:21,041:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:21,055:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:21,096:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:21,438:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:04:21,451:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:04:21,601:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:21,611:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:21,651:INFO:Calculating mean and std
2024-09-08 12:04:21,651:INFO:Creating metrics dataframe
2024-09-08 12:04:21,651:INFO:Uploading results into container
2024-09-08 12:04:21,651:INFO:Uploading model into container now
2024-09-08 12:04:21,651:INFO:_master_model_container: 8
2024-09-08 12:04:21,651:INFO:_display_container: 2
2024-09-08 12:04:21,651:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 12:04:21,651:INFO:create_model() successfully completed......................................
2024-09-08 12:04:21,801:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:21,801:INFO:Creating metrics dataframe
2024-09-08 12:04:21,807:INFO:Initializing Ada Boost Classifier
2024-09-08 12:04:21,807:INFO:Total runtime is 0.5006290276845297 minutes
2024-09-08 12:04:21,807:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:21,807:INFO:Initializing create_model()
2024-09-08 12:04:21,807:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:21,811:INFO:Checking exceptions
2024-09-08 12:04:21,811:INFO:Importing libraries
2024-09-08 12:04:21,811:INFO:Copying training dataset
2024-09-08 12:04:21,823:INFO:Defining folds
2024-09-08 12:04:21,823:INFO:Declaring metric variables
2024-09-08 12:04:21,823:INFO:Importing untrained model
2024-09-08 12:04:21,823:INFO:Ada Boost Classifier Imported successfully
2024-09-08 12:04:21,823:INFO:Starting cross validation
2024-09-08 12:04:21,836:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:22,703:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:04:22,711:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:04:22,719:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:04:22,726:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:04:22,771:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:04:22,784:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:04:22,811:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:04:22,846:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:04:23,413:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:23,441:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:23,472:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:23,512:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:23,536:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:23,546:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:23,554:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:23,603:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:23,972:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:04:23,991:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:04:24,439:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:24,455:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:24,480:INFO:Calculating mean and std
2024-09-08 12:04:24,481:INFO:Creating metrics dataframe
2024-09-08 12:04:24,481:INFO:Uploading results into container
2024-09-08 12:04:24,486:INFO:Uploading model into container now
2024-09-08 12:04:24,488:INFO:_master_model_container: 9
2024-09-08 12:04:24,488:INFO:_display_container: 2
2024-09-08 12:04:24,488:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 12:04:24,488:INFO:create_model() successfully completed......................................
2024-09-08 12:04:24,641:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:24,641:INFO:Creating metrics dataframe
2024-09-08 12:04:24,644:INFO:Initializing Gradient Boosting Classifier
2024-09-08 12:04:24,644:INFO:Total runtime is 0.547924812634786 minutes
2024-09-08 12:04:24,644:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:24,644:INFO:Initializing create_model()
2024-09-08 12:04:24,644:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:24,644:INFO:Checking exceptions
2024-09-08 12:04:24,644:INFO:Importing libraries
2024-09-08 12:04:24,644:INFO:Copying training dataset
2024-09-08 12:04:24,662:INFO:Defining folds
2024-09-08 12:04:24,662:INFO:Declaring metric variables
2024-09-08 12:04:24,662:INFO:Importing untrained model
2024-09-08 12:04:24,662:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 12:04:24,662:INFO:Starting cross validation
2024-09-08 12:04:24,672:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:28,398:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:28,477:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:28,505:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:28,595:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:28,603:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:28,622:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:28,628:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:28,645:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:31,052:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:31,062:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:31,098:INFO:Calculating mean and std
2024-09-08 12:04:31,098:INFO:Creating metrics dataframe
2024-09-08 12:04:31,102:INFO:Uploading results into container
2024-09-08 12:04:31,102:INFO:Uploading model into container now
2024-09-08 12:04:31,106:INFO:_master_model_container: 10
2024-09-08 12:04:31,106:INFO:_display_container: 2
2024-09-08 12:04:31,106:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 12:04:31,106:INFO:create_model() successfully completed......................................
2024-09-08 12:04:31,252:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:31,252:INFO:Creating metrics dataframe
2024-09-08 12:04:31,260:INFO:Initializing Linear Discriminant Analysis
2024-09-08 12:04:31,260:INFO:Total runtime is 0.658193866411845 minutes
2024-09-08 12:04:31,260:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:31,260:INFO:Initializing create_model()
2024-09-08 12:04:31,260:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:31,260:INFO:Checking exceptions
2024-09-08 12:04:31,260:INFO:Importing libraries
2024-09-08 12:04:31,260:INFO:Copying training dataset
2024-09-08 12:04:31,282:INFO:Defining folds
2024-09-08 12:04:31,282:INFO:Declaring metric variables
2024-09-08 12:04:31,282:INFO:Importing untrained model
2024-09-08 12:04:31,282:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 12:04:31,282:INFO:Starting cross validation
2024-09-08 12:04:31,292:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:32,375:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:32,402:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:32,422:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:32,459:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:32,607:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:32,607:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:32,642:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:32,652:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:33,189:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:33,197:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:04:33,222:INFO:Calculating mean and std
2024-09-08 12:04:33,222:INFO:Creating metrics dataframe
2024-09-08 12:04:33,222:INFO:Uploading results into container
2024-09-08 12:04:33,227:INFO:Uploading model into container now
2024-09-08 12:04:33,227:INFO:_master_model_container: 11
2024-09-08 12:04:33,227:INFO:_display_container: 2
2024-09-08 12:04:33,227:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 12:04:33,227:INFO:create_model() successfully completed......................................
2024-09-08 12:04:33,378:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:33,378:INFO:Creating metrics dataframe
2024-09-08 12:04:33,386:INFO:Initializing Extra Trees Classifier
2024-09-08 12:04:33,387:INFO:Total runtime is 0.693635098139445 minutes
2024-09-08 12:04:33,387:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:33,387:INFO:Initializing create_model()
2024-09-08 12:04:33,387:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:33,387:INFO:Checking exceptions
2024-09-08 12:04:33,388:INFO:Importing libraries
2024-09-08 12:04:33,388:INFO:Copying training dataset
2024-09-08 12:04:33,402:INFO:Defining folds
2024-09-08 12:04:33,402:INFO:Declaring metric variables
2024-09-08 12:04:33,402:INFO:Importing untrained model
2024-09-08 12:04:33,402:INFO:Extra Trees Classifier Imported successfully
2024-09-08 12:04:33,402:INFO:Starting cross validation
2024-09-08 12:04:33,412:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:36,323:INFO:Calculating mean and std
2024-09-08 12:04:36,323:INFO:Creating metrics dataframe
2024-09-08 12:04:36,327:INFO:Uploading results into container
2024-09-08 12:04:36,327:INFO:Uploading model into container now
2024-09-08 12:04:36,327:INFO:_master_model_container: 12
2024-09-08 12:04:36,327:INFO:_display_container: 2
2024-09-08 12:04:36,327:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 12:04:36,327:INFO:create_model() successfully completed......................................
2024-09-08 12:04:36,465:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:36,465:INFO:Creating metrics dataframe
2024-09-08 12:04:36,465:INFO:Initializing Extreme Gradient Boosting
2024-09-08 12:04:36,465:INFO:Total runtime is 0.7449285825093588 minutes
2024-09-08 12:04:36,465:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:36,465:INFO:Initializing create_model()
2024-09-08 12:04:36,465:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:36,465:INFO:Checking exceptions
2024-09-08 12:04:36,465:INFO:Importing libraries
2024-09-08 12:04:36,465:INFO:Copying training dataset
2024-09-08 12:04:36,487:INFO:Defining folds
2024-09-08 12:04:36,487:INFO:Declaring metric variables
2024-09-08 12:04:36,487:INFO:Importing untrained model
2024-09-08 12:04:36,487:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:04:36,487:INFO:Starting cross validation
2024-09-08 12:04:36,493:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:39,758:INFO:Calculating mean and std
2024-09-08 12:04:39,758:INFO:Creating metrics dataframe
2024-09-08 12:04:39,762:INFO:Uploading results into container
2024-09-08 12:04:39,762:INFO:Uploading model into container now
2024-09-08 12:04:39,762:INFO:_master_model_container: 13
2024-09-08 12:04:39,762:INFO:_display_container: 2
2024-09-08 12:04:39,762:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 12:04:39,762:INFO:create_model() successfully completed......................................
2024-09-08 12:04:39,903:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:39,903:INFO:Creating metrics dataframe
2024-09-08 12:04:39,909:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 12:04:39,909:INFO:Total runtime is 0.8023317098617554 minutes
2024-09-08 12:04:39,909:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:39,912:INFO:Initializing create_model()
2024-09-08 12:04:39,912:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:39,912:INFO:Checking exceptions
2024-09-08 12:04:39,912:INFO:Importing libraries
2024-09-08 12:04:39,913:INFO:Copying training dataset
2024-09-08 12:04:39,926:INFO:Defining folds
2024-09-08 12:04:39,926:INFO:Declaring metric variables
2024-09-08 12:04:39,926:INFO:Importing untrained model
2024-09-08 12:04:39,927:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 12:04:39,927:INFO:Starting cross validation
2024-09-08 12:04:39,933:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:46,943:INFO:Calculating mean and std
2024-09-08 12:04:46,943:INFO:Creating metrics dataframe
2024-09-08 12:04:46,948:INFO:Uploading results into container
2024-09-08 12:04:46,948:INFO:Uploading model into container now
2024-09-08 12:04:46,948:INFO:_master_model_container: 14
2024-09-08 12:04:46,948:INFO:_display_container: 2
2024-09-08 12:04:46,948:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 12:04:46,953:INFO:create_model() successfully completed......................................
2024-09-08 12:04:47,103:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:47,103:INFO:Creating metrics dataframe
2024-09-08 12:04:47,113:INFO:Initializing Dummy Classifier
2024-09-08 12:04:47,113:INFO:Total runtime is 0.9224029978116354 minutes
2024-09-08 12:04:47,113:INFO:SubProcess create_model() called ==================================
2024-09-08 12:04:47,113:INFO:Initializing create_model()
2024-09-08 12:04:47,113:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B0A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:47,113:INFO:Checking exceptions
2024-09-08 12:04:47,113:INFO:Importing libraries
2024-09-08 12:04:47,113:INFO:Copying training dataset
2024-09-08 12:04:47,128:INFO:Defining folds
2024-09-08 12:04:47,128:INFO:Declaring metric variables
2024-09-08 12:04:47,128:INFO:Importing untrained model
2024-09-08 12:04:47,128:INFO:Dummy Classifier Imported successfully
2024-09-08 12:04:47,128:INFO:Starting cross validation
2024-09-08 12:04:47,135:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:48,148:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:48,188:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:48,193:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:48,198:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:48,263:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:48,278:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:48,313:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:48,883:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:48,893:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:04:48,907:INFO:Calculating mean and std
2024-09-08 12:04:48,907:INFO:Creating metrics dataframe
2024-09-08 12:04:48,907:INFO:Uploading results into container
2024-09-08 12:04:48,913:INFO:Uploading model into container now
2024-09-08 12:04:48,913:INFO:_master_model_container: 15
2024-09-08 12:04:48,913:INFO:_display_container: 2
2024-09-08 12:04:48,913:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 12:04:48,913:INFO:create_model() successfully completed......................................
2024-09-08 12:04:49,053:INFO:SubProcess create_model() end ==================================
2024-09-08 12:04:49,053:INFO:Creating metrics dataframe
2024-09-08 12:04:49,064:INFO:Initializing create_model()
2024-09-08 12:04:49,064:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:49,064:INFO:Checking exceptions
2024-09-08 12:04:49,064:INFO:Importing libraries
2024-09-08 12:04:49,064:INFO:Copying training dataset
2024-09-08 12:04:49,073:INFO:Defining folds
2024-09-08 12:04:49,073:INFO:Declaring metric variables
2024-09-08 12:04:49,073:INFO:Importing untrained model
2024-09-08 12:04:49,073:INFO:Declaring custom model
2024-09-08 12:04:49,073:INFO:Random Forest Classifier Imported successfully
2024-09-08 12:04:49,084:INFO:Cross validation set to False
2024-09-08 12:04:49,093:INFO:Fitting Model
2024-09-08 12:04:49,823:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 12:04:49,823:INFO:create_model() successfully completed......................................
2024-09-08 12:04:49,973:INFO:Initializing create_model()
2024-09-08 12:04:49,973:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:49,973:INFO:Checking exceptions
2024-09-08 12:04:49,973:INFO:Importing libraries
2024-09-08 12:04:49,973:INFO:Copying training dataset
2024-09-08 12:04:49,993:INFO:Defining folds
2024-09-08 12:04:50,003:INFO:Declaring metric variables
2024-09-08 12:04:50,004:INFO:Importing untrained model
2024-09-08 12:04:50,004:INFO:Declaring custom model
2024-09-08 12:04:50,006:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 12:04:50,015:INFO:Cross validation set to False
2024-09-08 12:04:50,015:INFO:Fitting Model
2024-09-08 12:04:50,433:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 12:04:50,433:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000545 seconds.
2024-09-08 12:04:50,433:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 12:04:50,433:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 12:04:50,433:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 12:04:50,438:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 12:04:50,438:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 12:04:50,438:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 12:04:50,438:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 12:04:50,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,533:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:50,777:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 12:04:50,777:INFO:create_model() successfully completed......................................
2024-09-08 12:04:50,929:INFO:Initializing create_model()
2024-09-08 12:04:50,929:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:50,929:INFO:Checking exceptions
2024-09-08 12:04:50,929:INFO:Importing libraries
2024-09-08 12:04:50,929:INFO:Copying training dataset
2024-09-08 12:04:50,945:INFO:Defining folds
2024-09-08 12:04:50,945:INFO:Declaring metric variables
2024-09-08 12:04:50,945:INFO:Importing untrained model
2024-09-08 12:04:50,945:INFO:Declaring custom model
2024-09-08 12:04:50,945:INFO:Extra Trees Classifier Imported successfully
2024-09-08 12:04:50,953:INFO:Cross validation set to False
2024-09-08 12:04:50,953:INFO:Fitting Model
2024-09-08 12:04:51,553:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 12:04:51,553:INFO:create_model() successfully completed......................................
2024-09-08 12:04:51,717:INFO:_master_model_container: 15
2024-09-08 12:04:51,717:INFO:_display_container: 2
2024-09-08 12:04:51,719:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)]
2024-09-08 12:04:51,719:INFO:compare_models() successfully completed......................................
2024-09-08 12:04:51,719:INFO:Initializing create_model()
2024-09-08 12:04:51,719:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:51,719:INFO:Checking exceptions
2024-09-08 12:04:51,723:INFO:Importing libraries
2024-09-08 12:04:51,723:INFO:Copying training dataset
2024-09-08 12:04:51,741:INFO:Defining folds
2024-09-08 12:04:51,741:INFO:Declaring metric variables
2024-09-08 12:04:51,743:INFO:Importing untrained model
2024-09-08 12:04:51,743:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 12:04:51,743:INFO:Starting cross validation
2024-09-08 12:04:51,753:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:04:58,524:INFO:Calculating mean and std
2024-09-08 12:04:58,524:INFO:Creating metrics dataframe
2024-09-08 12:04:58,529:INFO:Finalizing model
2024-09-08 12:04:58,934:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 12:04:58,934:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001044 seconds.
2024-09-08 12:04:58,934:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-09-08 12:04:58,934:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 12:04:58,934:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 12:04:58,939:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 12:04:58,939:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 12:04:58,939:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 12:04:58,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:58,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:04:59,279:INFO:Uploading results into container
2024-09-08 12:04:59,279:INFO:Uploading model into container now
2024-09-08 12:04:59,304:INFO:_master_model_container: 16
2024-09-08 12:04:59,304:INFO:_display_container: 3
2024-09-08 12:04:59,304:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 12:04:59,304:INFO:create_model() successfully completed......................................
2024-09-08 12:04:59,454:INFO:Initializing create_model()
2024-09-08 12:04:59,459:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:04:59,459:INFO:Checking exceptions
2024-09-08 12:04:59,460:INFO:Importing libraries
2024-09-08 12:04:59,460:INFO:Copying training dataset
2024-09-08 12:04:59,474:INFO:Defining folds
2024-09-08 12:04:59,474:INFO:Declaring metric variables
2024-09-08 12:04:59,474:INFO:Importing untrained model
2024-09-08 12:04:59,476:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:04:59,476:INFO:Starting cross validation
2024-09-08 12:04:59,484:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:05:03,018:INFO:Calculating mean and std
2024-09-08 12:05:03,018:INFO:Creating metrics dataframe
2024-09-08 12:05:03,018:INFO:Finalizing model
2024-09-08 12:05:03,994:INFO:Uploading results into container
2024-09-08 12:05:03,999:INFO:Uploading model into container now
2024-09-08 12:05:04,027:INFO:_master_model_container: 17
2024-09-08 12:05:04,027:INFO:_display_container: 4
2024-09-08 12:05:04,030:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:05:04,030:INFO:create_model() successfully completed......................................
2024-09-08 12:05:04,184:INFO:Initializing create_model()
2024-09-08 12:05:04,184:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:05:04,184:INFO:Checking exceptions
2024-09-08 12:05:04,189:INFO:Importing libraries
2024-09-08 12:05:04,189:INFO:Copying training dataset
2024-09-08 12:05:04,204:INFO:Defining folds
2024-09-08 12:05:04,209:INFO:Declaring metric variables
2024-09-08 12:05:04,209:INFO:Importing untrained model
2024-09-08 12:05:04,210:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 12:05:04,210:INFO:Starting cross validation
2024-09-08 12:05:04,219:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:05:07,320:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:05:07,347:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:05:07,407:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:05:07,576:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:05:07,658:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:05:07,680:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:05:07,718:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:05:07,810:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:05:09,592:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:05:09,635:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:05:09,658:INFO:Calculating mean and std
2024-09-08 12:05:09,658:INFO:Creating metrics dataframe
2024-09-08 12:05:09,660:INFO:Finalizing model
2024-09-08 12:05:11,610:INFO:Uploading results into container
2024-09-08 12:05:11,610:INFO:Uploading model into container now
2024-09-08 12:05:11,628:INFO:_master_model_container: 18
2024-09-08 12:05:11,628:INFO:_display_container: 5
2024-09-08 12:05:11,628:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 12:05:11,628:INFO:create_model() successfully completed......................................
2024-09-08 12:05:11,770:INFO:Initializing tune_model()
2024-09-08 12:05:11,770:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>)
2024-09-08 12:05:11,770:INFO:Checking exceptions
2024-09-08 12:05:11,770:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 12:05:11,852:INFO:Copying training dataset
2024-09-08 12:05:11,862:INFO:Checking base model
2024-09-08 12:05:11,862:INFO:Base model : Extreme Gradient Boosting
2024-09-08 12:05:11,862:INFO:Declaring metric variables
2024-09-08 12:05:11,862:INFO:Defining Hyperparameters
2024-09-08 12:05:12,005:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 12:05:12,005:INFO:Tuning with n_jobs=-1
2024-09-08 12:05:12,015:INFO:Initializing skopt.BayesSearchCV
2024-09-08 12:05:26,671:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.6), ('actual_estimator__gamma', 0), ('actual_estimator__learning_rate', 0.01), ('actual_estimator__max_depth', 7), ('actual_estimator__min_child_weight', 1), ('actual_estimator__n_estimators', 100), ('actual_estimator__subsample', 0.6)])
2024-09-08 12:05:26,671:INFO:Hyperparameter search completed
2024-09-08 12:05:26,671:INFO:SubProcess create_model() called ==================================
2024-09-08 12:05:26,679:INFO:Initializing create_model()
2024-09-08 12:05:26,679:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D6C011B7F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.6})
2024-09-08 12:05:26,679:INFO:Checking exceptions
2024-09-08 12:05:26,679:INFO:Importing libraries
2024-09-08 12:05:26,679:INFO:Copying training dataset
2024-09-08 12:05:26,691:INFO:Defining folds
2024-09-08 12:05:26,691:INFO:Declaring metric variables
2024-09-08 12:05:26,691:INFO:Importing untrained model
2024-09-08 12:05:26,691:INFO:Declaring custom model
2024-09-08 12:05:26,694:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:05:26,694:INFO:Starting cross validation
2024-09-08 12:05:26,701:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:05:28,392:INFO:Calculating mean and std
2024-09-08 12:05:28,393:INFO:Creating metrics dataframe
2024-09-08 12:05:28,396:INFO:Finalizing model
2024-09-08 12:05:29,350:INFO:Uploading results into container
2024-09-08 12:05:29,352:INFO:Uploading model into container now
2024-09-08 12:05:29,352:INFO:_master_model_container: 19
2024-09-08 12:05:29,352:INFO:_display_container: 6
2024-09-08 12:05:29,354:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:05:29,354:INFO:create_model() successfully completed......................................
2024-09-08 12:05:29,506:INFO:SubProcess create_model() end ==================================
2024-09-08 12:05:29,506:INFO:choose_better activated
2024-09-08 12:05:29,506:INFO:SubProcess create_model() called ==================================
2024-09-08 12:05:29,511:INFO:Initializing create_model()
2024-09-08 12:05:29,511:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:05:29,511:INFO:Checking exceptions
2024-09-08 12:05:29,511:INFO:Importing libraries
2024-09-08 12:05:29,511:INFO:Copying training dataset
2024-09-08 12:05:29,526:INFO:Defining folds
2024-09-08 12:05:29,526:INFO:Declaring metric variables
2024-09-08 12:05:29,526:INFO:Importing untrained model
2024-09-08 12:05:29,526:INFO:Declaring custom model
2024-09-08 12:05:29,526:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:05:29,526:INFO:Starting cross validation
2024-09-08 12:05:29,536:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:05:31,033:INFO:Calculating mean and std
2024-09-08 12:05:31,033:INFO:Creating metrics dataframe
2024-09-08 12:05:31,036:INFO:Finalizing model
2024-09-08 12:05:31,821:INFO:Uploading results into container
2024-09-08 12:05:31,821:INFO:Uploading model into container now
2024-09-08 12:05:31,821:INFO:_master_model_container: 20
2024-09-08 12:05:31,821:INFO:_display_container: 7
2024-09-08 12:05:31,826:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:05:31,826:INFO:create_model() successfully completed......................................
2024-09-08 12:05:31,974:INFO:SubProcess create_model() end ==================================
2024-09-08 12:05:31,974:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8186
2024-09-08 12:05:31,982:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8386
2024-09-08 12:05:31,982:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 12:05:31,982:INFO:choose_better completed
2024-09-08 12:05:31,998:INFO:_master_model_container: 20
2024-09-08 12:05:31,998:INFO:_display_container: 6
2024-09-08 12:05:32,001:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:05:32,001:INFO:tune_model() successfully completed......................................
2024-09-08 12:05:32,151:INFO:Initializing predict_model()
2024-09-08 12:05:32,151:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D6A95B1480>)
2024-09-08 12:05:32,151:INFO:Checking exceptions
2024-09-08 12:05:32,151:INFO:Preloading libraries
2024-09-08 12:05:32,684:INFO:Initializing get_config()
2024-09-08 12:05:32,684:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, variable=X_train)
2024-09-08 12:05:32,684:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 12:05:32,747:INFO:Variable:  returned as       id Marital status Application mode Application order  ... Curricular units 2nd sem (without evaluations) Unemployment rate Inflation rate   GDP
765  765              1                1                 6  ...                                              0         10.800000            1.4  1.74
323  323              1               17                 1  ...                                              0          7.600000            2.6  0.32
909  909              1                1                 1  ...                                              0         12.400000            0.5  1.79
774  774              1               17                 1  ...                                              0          8.900000            1.4  3.51
931  931              1               39                 1  ...                                              0         16.200001            0.3 -0.92
..   ...            ...              ...               ...  ...                                            ...               ...            ...   ...
806  806              4               39                 1  ...                                              0         10.800000            1.4  1.74
492  492              1                1                 3  ...                                              0          7.600000            2.6  0.32
698  698              1               39                 1  ...                                              1         10.800000            1.4  1.74
44    44              1                1                 1  ...                                              0          9.400000           -0.8 -3.12
604  604              1                1                 1  ...                                              0         10.800000            1.4  1.74

[700 rows x 37 columns]
2024-09-08 12:05:32,747:INFO:get_config() successfully completed......................................
2024-09-08 12:05:32,747:INFO:Initializing predict_model()
2024-09-08 12:05:32,747:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D6C00EDC60>)
2024-09-08 12:05:32,747:INFO:Checking exceptions
2024-09-08 12:05:32,747:INFO:Preloading libraries
2024-09-08 12:05:32,747:INFO:Set up data.
2024-09-08 12:05:32,762:INFO:Set up index.
2024-09-08 12:05:33,111:INFO:Initializing get_config()
2024-09-08 12:05:33,111:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, variable=y_train)
2024-09-08 12:05:33,121:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 12:05:33,123:INFO:Variable:  returned as 765    0
323    1
909    0
774    0
931    1
      ..
806    1
492    0
698    1
44     0
604    1
Name: Target, Length: 700, dtype: int8
2024-09-08 12:05:33,123:INFO:get_config() successfully completed......................................
2024-09-08 12:05:33,123:INFO:Initializing get_config()
2024-09-08 12:05:33,123:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, variable=y_test)
2024-09-08 12:05:33,123:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 12:05:33,131:INFO:Variable:  returned as 47     1
233    1
132    2
34     0
162    0
      ..
772    0
632    0
365    2
136    0
299    1
Name: Target, Length: 300, dtype: int8
2024-09-08 12:05:33,131:INFO:get_config() successfully completed......................................
2024-09-08 12:05:33,131:INFO:Initializing finalize_model()
2024-09-08 12:05:33,131:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 12:05:33,131:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:05:33,141:INFO:Initializing create_model()
2024-09-08 12:05:33,141:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:05:33,141:INFO:Checking exceptions
2024-09-08 12:05:33,141:INFO:Importing libraries
2024-09-08 12:05:33,141:INFO:Copying training dataset
2024-09-08 12:05:33,141:INFO:Defining folds
2024-09-08 12:05:33,146:INFO:Declaring metric variables
2024-09-08 12:05:33,146:INFO:Importing untrained model
2024-09-08 12:05:33,146:INFO:Declaring custom model
2024-09-08 12:05:33,146:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:05:33,155:INFO:Cross validation set to False
2024-09-08 12:05:33,155:INFO:Fitting Model
2024-09-08 12:05:34,641:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 12:05:34,641:INFO:create_model() successfully completed......................................
2024-09-08 12:05:34,791:INFO:_master_model_container: 20
2024-09-08 12:05:34,791:INFO:_display_container: 7
2024-09-08 12:05:35,117:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 12:05:35,117:INFO:finalize_model() successfully completed......................................
2024-09-08 12:05:35,665:INFO:Initializing predict_model()
2024-09-08 12:05:35,669:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D6BE107400>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D6C0424B80>)
2024-09-08 12:05:35,670:INFO:Checking exceptions
2024-09-08 12:05:35,670:INFO:Preloading libraries
2024-09-08 12:05:35,670:INFO:Set up data.
2024-09-08 12:05:35,703:INFO:Set up index.
2024-09-08 12:26:33,925:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 12:26:33,925:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 12:26:33,925:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 12:26:33,925:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 12:27:05,497:INFO:PyCaret ClassificationExperiment
2024-09-08 12:27:05,497:INFO:Logging name: clf-default-name
2024-09-08 12:27:05,497:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 12:27:05,504:INFO:version 3.3.2
2024-09-08 12:27:05,504:INFO:Initializing setup()
2024-09-08 12:27:05,504:INFO:self.USI: 234c
2024-09-08 12:27:05,504:INFO:self._variable_keys: {'fold_generator', 'USI', 'logging_param', 'gpu_n_jobs_param', 'fix_imbalance', 'X_train', 'X_test', 'seed', 'data', 'y_test', 'is_multiclass', 'n_jobs_param', 'exp_id', 'fold_shuffle_param', 'idx', 'y', 'log_plots_param', 'target_param', '_available_plots', 'gpu_param', 'X', 'pipeline', 'memory', '_ml_usecase', 'exp_name_log', 'fold_groups_param', 'y_train', 'html_param'}
2024-09-08 12:27:05,507:INFO:Checking environment
2024-09-08 12:27:05,507:INFO:python_version: 3.10.11
2024-09-08 12:27:05,507:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 12:27:05,507:INFO:machine: AMD64
2024-09-08 12:27:05,528:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 12:27:05,528:INFO:Memory: svmem(total=16407719936, available=5794365440, percent=64.7, used=10613354496, free=5794365440)
2024-09-08 12:27:05,528:INFO:Physical Core: 4
2024-09-08 12:27:05,528:INFO:Logical Core: 8
2024-09-08 12:27:05,528:INFO:Checking libraries
2024-09-08 12:27:05,528:INFO:System:
2024-09-08 12:27:05,528:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 12:27:05,528:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 12:27:05,528:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 12:27:05,528:INFO:PyCaret required dependencies:
2024-09-08 12:27:05,643:INFO:                 pip: 24.2
2024-09-08 12:27:05,643:INFO:          setuptools: 72.1.0
2024-09-08 12:27:05,643:INFO:             pycaret: 3.3.2
2024-09-08 12:27:05,643:INFO:             IPython: 8.25.0
2024-09-08 12:27:05,643:INFO:          ipywidgets: 8.1.5
2024-09-08 12:27:05,643:INFO:                tqdm: 4.66.5
2024-09-08 12:27:05,643:INFO:               numpy: 1.26.4
2024-09-08 12:27:05,643:INFO:              pandas: 2.1.4
2024-09-08 12:27:05,643:INFO:              jinja2: 3.1.4
2024-09-08 12:27:05,647:INFO:               scipy: 1.11.4
2024-09-08 12:27:05,659:INFO:              joblib: 1.3.2
2024-09-08 12:27:05,659:INFO:             sklearn: 1.4.2
2024-09-08 12:27:05,659:INFO:                pyod: 2.0.1
2024-09-08 12:27:05,659:INFO:            imblearn: 0.12.3
2024-09-08 12:27:05,659:INFO:   category_encoders: 2.6.3
2024-09-08 12:27:05,659:INFO:            lightgbm: 4.5.0
2024-09-08 12:27:05,659:INFO:               numba: 0.60.0
2024-09-08 12:27:05,659:INFO:            requests: 2.32.3
2024-09-08 12:27:05,659:INFO:          matplotlib: 3.7.5
2024-09-08 12:27:05,659:INFO:          scikitplot: 0.3.7
2024-09-08 12:27:05,659:INFO:         yellowbrick: 1.5
2024-09-08 12:27:05,659:INFO:              plotly: 5.24.0
2024-09-08 12:27:05,659:INFO:    plotly-resampler: Not installed
2024-09-08 12:27:05,659:INFO:             kaleido: 0.2.1
2024-09-08 12:27:05,659:INFO:           schemdraw: 0.15
2024-09-08 12:27:05,659:INFO:         statsmodels: 0.14.2
2024-09-08 12:27:05,659:INFO:              sktime: 0.26.0
2024-09-08 12:27:05,659:INFO:               tbats: 1.1.3
2024-09-08 12:27:05,659:INFO:            pmdarima: 2.0.4
2024-09-08 12:27:05,659:INFO:              psutil: 5.9.0
2024-09-08 12:27:05,659:INFO:          markupsafe: 2.1.3
2024-09-08 12:27:05,659:INFO:             pickle5: Not installed
2024-09-08 12:27:05,659:INFO:         cloudpickle: 3.0.0
2024-09-08 12:27:05,659:INFO:         deprecation: 2.1.0
2024-09-08 12:27:05,659:INFO:              xxhash: 3.5.0
2024-09-08 12:27:05,659:INFO:           wurlitzer: Not installed
2024-09-08 12:27:05,659:INFO:PyCaret optional dependencies:
2024-09-08 12:27:05,687:INFO:                shap: Not installed
2024-09-08 12:27:05,687:INFO:           interpret: Not installed
2024-09-08 12:27:05,687:INFO:                umap: Not installed
2024-09-08 12:27:05,687:INFO:     ydata_profiling: Not installed
2024-09-08 12:27:05,687:INFO:  explainerdashboard: Not installed
2024-09-08 12:27:05,687:INFO:             autoviz: Not installed
2024-09-08 12:27:05,687:INFO:           fairlearn: Not installed
2024-09-08 12:27:05,687:INFO:          deepchecks: Not installed
2024-09-08 12:27:05,687:INFO:             xgboost: 2.1.1
2024-09-08 12:27:05,687:INFO:            catboost: Not installed
2024-09-08 12:27:05,687:INFO:              kmodes: Not installed
2024-09-08 12:27:05,687:INFO:             mlxtend: Not installed
2024-09-08 12:27:05,687:INFO:       statsforecast: Not installed
2024-09-08 12:27:05,691:INFO:        tune_sklearn: Not installed
2024-09-08 12:27:05,691:INFO:                 ray: Not installed
2024-09-08 12:27:05,691:INFO:            hyperopt: 0.2.7
2024-09-08 12:27:05,691:INFO:              optuna: 4.0.0
2024-09-08 12:27:05,692:INFO:               skopt: 0.10.2
2024-09-08 12:27:05,692:INFO:              mlflow: Not installed
2024-09-08 12:27:05,692:INFO:              gradio: Not installed
2024-09-08 12:27:05,692:INFO:             fastapi: Not installed
2024-09-08 12:27:05,692:INFO:             uvicorn: Not installed
2024-09-08 12:27:05,692:INFO:              m2cgen: Not installed
2024-09-08 12:27:05,692:INFO:           evidently: Not installed
2024-09-08 12:27:05,692:INFO:               fugue: Not installed
2024-09-08 12:27:05,692:INFO:           streamlit: 1.38.0
2024-09-08 12:27:05,692:INFO:             prophet: Not installed
2024-09-08 12:27:05,692:INFO:None
2024-09-08 12:27:05,692:INFO:Set up data.
2024-09-08 12:27:05,717:INFO:Set up folding strategy.
2024-09-08 12:27:05,717:INFO:Set up train/test split.
2024-09-08 12:27:05,749:INFO:Set up index.
2024-09-08 12:27:05,749:INFO:Assigning column types.
2024-09-08 12:27:05,757:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 12:27:05,857:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 12:27:05,864:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 12:27:05,938:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:27:05,938:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:27:06,037:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 12:27:06,037:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 12:27:06,097:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:27:06,102:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:27:06,102:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 12:27:06,201:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 12:27:06,258:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:27:06,258:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:27:06,348:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 12:27:06,398:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:27:06,407:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:27:06,407:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 12:27:06,548:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:27:06,557:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:27:06,779:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:27:06,787:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:27:06,787:INFO:Preparing preprocessing pipeline...
2024-09-08 12:27:06,795:INFO:Set up simple imputation.
2024-09-08 12:27:06,811:INFO:Set up encoding of ordinal features.
2024-09-08 12:27:06,860:INFO:Set up encoding of categorical features.
2024-09-08 12:27:06,862:INFO:Set up column name cleaning.
2024-09-08 12:27:07,325:INFO:Finished creating preprocessing pipeline.
2024-09-08 12:27:07,707:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                                                    'qualification',
                                                                    'Fathers '
                                                                    'qualification',
                                                                    'Mothers '
                                                                    'occupation',
                                                                    'Fathers '
                                                                    'occupation'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 12:27:07,707:INFO:Creating final display dataframe.
2024-09-08 12:27:08,199:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (1000, 38)
4        Transformed data shape       (1000, 129)
5   Transformed train set shape        (700, 129)
6    Transformed test set shape        (300, 129)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              234c
2024-09-08 12:27:08,384:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:27:08,387:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:27:08,567:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:27:08,568:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:27:08,568:INFO:setup() successfully completed in 3.1s...............
2024-09-08 12:27:08,568:INFO:Initializing compare_models()
2024-09-08 12:27:08,568:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 12:27:08,568:INFO:Checking exceptions
2024-09-08 12:27:08,587:INFO:Preparing display monitor
2024-09-08 12:27:08,587:INFO:Initializing Logistic Regression
2024-09-08 12:27:08,587:INFO:Total runtime is 0.0 minutes
2024-09-08 12:27:08,587:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:08,587:INFO:Initializing create_model()
2024-09-08 12:27:08,587:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:08,597:INFO:Checking exceptions
2024-09-08 12:27:08,597:INFO:Importing libraries
2024-09-08 12:27:08,597:INFO:Copying training dataset
2024-09-08 12:27:08,617:INFO:Defining folds
2024-09-08 12:27:08,617:INFO:Declaring metric variables
2024-09-08 12:27:08,617:INFO:Importing untrained model
2024-09-08 12:27:08,617:INFO:Logistic Regression Imported successfully
2024-09-08 12:27:08,617:INFO:Starting cross validation
2024-09-08 12:27:08,627:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:27:22,793:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:27:22,849:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:27:22,958:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:27:23,088:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:23,132:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:23,138:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:27:23,271:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:27:23,279:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:23,288:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:27:23,293:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:27:23,454:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:27:23,483:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:23,538:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:23,549:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:23,563:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:23,744:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:25,078:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:27:25,118:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:27:25,234:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:25,268:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:25,298:INFO:Calculating mean and std
2024-09-08 12:27:25,298:INFO:Creating metrics dataframe
2024-09-08 12:27:25,298:INFO:Uploading results into container
2024-09-08 12:27:25,298:INFO:Uploading model into container now
2024-09-08 12:27:25,298:INFO:_master_model_container: 1
2024-09-08 12:27:25,298:INFO:_display_container: 2
2024-09-08 12:27:25,298:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 12:27:25,298:INFO:create_model() successfully completed......................................
2024-09-08 12:27:25,458:INFO:SubProcess create_model() end ==================================
2024-09-08 12:27:25,458:INFO:Creating metrics dataframe
2024-09-08 12:27:25,458:INFO:Initializing K Neighbors Classifier
2024-09-08 12:27:25,458:INFO:Total runtime is 0.2811842123667399 minutes
2024-09-08 12:27:25,458:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:25,458:INFO:Initializing create_model()
2024-09-08 12:27:25,458:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:25,458:INFO:Checking exceptions
2024-09-08 12:27:25,458:INFO:Importing libraries
2024-09-08 12:27:25,458:INFO:Copying training dataset
2024-09-08 12:27:25,488:INFO:Defining folds
2024-09-08 12:27:25,488:INFO:Declaring metric variables
2024-09-08 12:27:25,488:INFO:Importing untrained model
2024-09-08 12:27:25,488:INFO:K Neighbors Classifier Imported successfully
2024-09-08 12:27:25,488:INFO:Starting cross validation
2024-09-08 12:27:25,504:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:27:27,822:INFO:Calculating mean and std
2024-09-08 12:27:27,822:INFO:Creating metrics dataframe
2024-09-08 12:27:27,822:INFO:Uploading results into container
2024-09-08 12:27:27,829:INFO:Uploading model into container now
2024-09-08 12:27:27,829:INFO:_master_model_container: 2
2024-09-08 12:27:27,829:INFO:_display_container: 2
2024-09-08 12:27:27,829:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 12:27:27,829:INFO:create_model() successfully completed......................................
2024-09-08 12:27:27,999:INFO:SubProcess create_model() end ==================================
2024-09-08 12:27:27,999:INFO:Creating metrics dataframe
2024-09-08 12:27:28,008:INFO:Initializing Naive Bayes
2024-09-08 12:27:28,008:INFO:Total runtime is 0.32368764479955037 minutes
2024-09-08 12:27:28,012:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:28,012:INFO:Initializing create_model()
2024-09-08 12:27:28,012:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:28,012:INFO:Checking exceptions
2024-09-08 12:27:28,012:INFO:Importing libraries
2024-09-08 12:27:28,012:INFO:Copying training dataset
2024-09-08 12:27:28,029:INFO:Defining folds
2024-09-08 12:27:28,029:INFO:Declaring metric variables
2024-09-08 12:27:28,039:INFO:Importing untrained model
2024-09-08 12:27:28,039:INFO:Naive Bayes Imported successfully
2024-09-08 12:27:28,039:INFO:Starting cross validation
2024-09-08 12:27:28,049:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:27:29,564:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:27:30,299:INFO:Calculating mean and std
2024-09-08 12:27:30,299:INFO:Creating metrics dataframe
2024-09-08 12:27:30,299:INFO:Uploading results into container
2024-09-08 12:27:30,299:INFO:Uploading model into container now
2024-09-08 12:27:30,299:INFO:_master_model_container: 3
2024-09-08 12:27:30,299:INFO:_display_container: 2
2024-09-08 12:27:30,299:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 12:27:30,299:INFO:create_model() successfully completed......................................
2024-09-08 12:27:30,454:INFO:SubProcess create_model() end ==================================
2024-09-08 12:27:30,454:INFO:Creating metrics dataframe
2024-09-08 12:27:30,459:INFO:Initializing Decision Tree Classifier
2024-09-08 12:27:30,459:INFO:Total runtime is 0.364531139532725 minutes
2024-09-08 12:27:30,459:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:30,459:INFO:Initializing create_model()
2024-09-08 12:27:30,459:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:30,459:INFO:Checking exceptions
2024-09-08 12:27:30,459:INFO:Importing libraries
2024-09-08 12:27:30,459:INFO:Copying training dataset
2024-09-08 12:27:30,479:INFO:Defining folds
2024-09-08 12:27:30,479:INFO:Declaring metric variables
2024-09-08 12:27:30,479:INFO:Importing untrained model
2024-09-08 12:27:30,484:INFO:Decision Tree Classifier Imported successfully
2024-09-08 12:27:30,484:INFO:Starting cross validation
2024-09-08 12:27:30,489:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:27:32,498:INFO:Calculating mean and std
2024-09-08 12:27:32,499:INFO:Creating metrics dataframe
2024-09-08 12:27:32,499:INFO:Uploading results into container
2024-09-08 12:27:32,499:INFO:Uploading model into container now
2024-09-08 12:27:32,499:INFO:_master_model_container: 4
2024-09-08 12:27:32,499:INFO:_display_container: 2
2024-09-08 12:27:32,499:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 12:27:32,499:INFO:create_model() successfully completed......................................
2024-09-08 12:27:32,639:INFO:SubProcess create_model() end ==================================
2024-09-08 12:27:32,639:INFO:Creating metrics dataframe
2024-09-08 12:27:32,639:INFO:Initializing SVM - Linear Kernel
2024-09-08 12:27:32,639:INFO:Total runtime is 0.40086394151051835 minutes
2024-09-08 12:27:32,639:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:32,639:INFO:Initializing create_model()
2024-09-08 12:27:32,639:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:32,639:INFO:Checking exceptions
2024-09-08 12:27:32,639:INFO:Importing libraries
2024-09-08 12:27:32,639:INFO:Copying training dataset
2024-09-08 12:27:32,659:INFO:Defining folds
2024-09-08 12:27:32,659:INFO:Declaring metric variables
2024-09-08 12:27:32,659:INFO:Importing untrained model
2024-09-08 12:27:32,659:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 12:27:32,659:INFO:Starting cross validation
2024-09-08 12:27:32,670:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:27:33,878:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:33,891:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:27:33,901:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:33,901:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:33,917:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:33,929:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:33,944:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:27:33,949:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:33,959:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:33,984:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:27:34,699:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:34,719:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:34,756:INFO:Calculating mean and std
2024-09-08 12:27:34,756:INFO:Creating metrics dataframe
2024-09-08 12:27:34,759:INFO:Uploading results into container
2024-09-08 12:27:34,759:INFO:Uploading model into container now
2024-09-08 12:27:34,759:INFO:_master_model_container: 5
2024-09-08 12:27:34,759:INFO:_display_container: 2
2024-09-08 12:27:34,764:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 12:27:34,764:INFO:create_model() successfully completed......................................
2024-09-08 12:27:34,899:INFO:SubProcess create_model() end ==================================
2024-09-08 12:27:34,899:INFO:Creating metrics dataframe
2024-09-08 12:27:34,899:INFO:Initializing Ridge Classifier
2024-09-08 12:27:34,899:INFO:Total runtime is 0.43853001197179153 minutes
2024-09-08 12:27:34,899:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:34,899:INFO:Initializing create_model()
2024-09-08 12:27:34,899:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:34,899:INFO:Checking exceptions
2024-09-08 12:27:34,899:INFO:Importing libraries
2024-09-08 12:27:34,899:INFO:Copying training dataset
2024-09-08 12:27:34,919:INFO:Defining folds
2024-09-08 12:27:34,919:INFO:Declaring metric variables
2024-09-08 12:27:34,919:INFO:Importing untrained model
2024-09-08 12:27:34,919:INFO:Ridge Classifier Imported successfully
2024-09-08 12:27:34,919:INFO:Starting cross validation
2024-09-08 12:27:34,929:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:27:35,943:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:35,960:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:35,984:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:35,994:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:35,999:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:36,009:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:36,009:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:36,039:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:36,639:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:36,659:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:36,700:INFO:Calculating mean and std
2024-09-08 12:27:36,700:INFO:Creating metrics dataframe
2024-09-08 12:27:36,700:INFO:Uploading results into container
2024-09-08 12:27:36,700:INFO:Uploading model into container now
2024-09-08 12:27:36,700:INFO:_master_model_container: 6
2024-09-08 12:27:36,700:INFO:_display_container: 2
2024-09-08 12:27:36,709:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 12:27:36,709:INFO:create_model() successfully completed......................................
2024-09-08 12:27:36,869:INFO:SubProcess create_model() end ==================================
2024-09-08 12:27:36,871:INFO:Creating metrics dataframe
2024-09-08 12:27:36,872:INFO:Initializing Random Forest Classifier
2024-09-08 12:27:36,872:INFO:Total runtime is 0.47140564123789463 minutes
2024-09-08 12:27:36,872:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:36,872:INFO:Initializing create_model()
2024-09-08 12:27:36,872:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:36,872:INFO:Checking exceptions
2024-09-08 12:27:36,872:INFO:Importing libraries
2024-09-08 12:27:36,872:INFO:Copying training dataset
2024-09-08 12:27:36,889:INFO:Defining folds
2024-09-08 12:27:36,889:INFO:Declaring metric variables
2024-09-08 12:27:36,889:INFO:Importing untrained model
2024-09-08 12:27:36,889:INFO:Random Forest Classifier Imported successfully
2024-09-08 12:27:36,889:INFO:Starting cross validation
2024-09-08 12:27:36,903:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:27:41,110:INFO:Calculating mean and std
2024-09-08 12:27:41,110:INFO:Creating metrics dataframe
2024-09-08 12:27:41,110:INFO:Uploading results into container
2024-09-08 12:27:41,110:INFO:Uploading model into container now
2024-09-08 12:27:41,110:INFO:_master_model_container: 7
2024-09-08 12:27:41,110:INFO:_display_container: 2
2024-09-08 12:27:41,110:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 12:27:41,110:INFO:create_model() successfully completed......................................
2024-09-08 12:27:41,290:INFO:SubProcess create_model() end ==================================
2024-09-08 12:27:41,290:INFO:Creating metrics dataframe
2024-09-08 12:27:41,299:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 12:27:41,299:INFO:Total runtime is 0.5452040751775105 minutes
2024-09-08 12:27:41,299:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:41,299:INFO:Initializing create_model()
2024-09-08 12:27:41,299:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:41,299:INFO:Checking exceptions
2024-09-08 12:27:41,299:INFO:Importing libraries
2024-09-08 12:27:41,299:INFO:Copying training dataset
2024-09-08 12:27:41,320:INFO:Defining folds
2024-09-08 12:27:41,320:INFO:Declaring metric variables
2024-09-08 12:27:41,320:INFO:Importing untrained model
2024-09-08 12:27:41,320:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 12:27:41,320:INFO:Starting cross validation
2024-09-08 12:27:41,330:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:27:42,220:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:27:42,261:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:27:42,270:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:27:42,380:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:27:42,380:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:27:42,387:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:27:42,395:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:27:42,415:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:27:42,645:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:42,690:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:42,720:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:42,766:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:42,885:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:42,885:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:42,960:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:42,990:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:43,430:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:27:43,432:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:27:43,591:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:43,600:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:43,638:INFO:Calculating mean and std
2024-09-08 12:27:43,640:INFO:Creating metrics dataframe
2024-09-08 12:27:43,640:INFO:Uploading results into container
2024-09-08 12:27:43,640:INFO:Uploading model into container now
2024-09-08 12:27:43,640:INFO:_master_model_container: 8
2024-09-08 12:27:43,640:INFO:_display_container: 2
2024-09-08 12:27:43,640:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 12:27:43,640:INFO:create_model() successfully completed......................................
2024-09-08 12:27:43,800:INFO:SubProcess create_model() end ==================================
2024-09-08 12:27:43,800:INFO:Creating metrics dataframe
2024-09-08 12:27:43,800:INFO:Initializing Ada Boost Classifier
2024-09-08 12:27:43,800:INFO:Total runtime is 0.5868772109349567 minutes
2024-09-08 12:27:43,800:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:43,800:INFO:Initializing create_model()
2024-09-08 12:27:43,800:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:43,800:INFO:Checking exceptions
2024-09-08 12:27:43,810:INFO:Importing libraries
2024-09-08 12:27:43,810:INFO:Copying training dataset
2024-09-08 12:27:43,820:INFO:Defining folds
2024-09-08 12:27:43,820:INFO:Declaring metric variables
2024-09-08 12:27:43,820:INFO:Importing untrained model
2024-09-08 12:27:43,820:INFO:Ada Boost Classifier Imported successfully
2024-09-08 12:27:43,820:INFO:Starting cross validation
2024-09-08 12:27:43,830:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:27:44,735:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:27:44,750:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:27:44,760:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:27:44,765:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:27:44,773:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:27:44,790:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:27:44,810:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:27:44,820:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:27:45,455:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:45,474:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:45,500:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:45,515:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:45,596:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:45,610:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:45,621:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:45,670:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:46,052:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:27:46,067:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:27:46,560:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:46,583:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:46,625:INFO:Calculating mean and std
2024-09-08 12:27:46,625:INFO:Creating metrics dataframe
2024-09-08 12:27:46,630:INFO:Uploading results into container
2024-09-08 12:27:46,630:INFO:Uploading model into container now
2024-09-08 12:27:46,630:INFO:_master_model_container: 9
2024-09-08 12:27:46,630:INFO:_display_container: 2
2024-09-08 12:27:46,630:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 12:27:46,630:INFO:create_model() successfully completed......................................
2024-09-08 12:27:46,784:INFO:SubProcess create_model() end ==================================
2024-09-08 12:27:46,784:INFO:Creating metrics dataframe
2024-09-08 12:27:46,791:INFO:Initializing Gradient Boosting Classifier
2024-09-08 12:27:46,791:INFO:Total runtime is 0.6367238044738768 minutes
2024-09-08 12:27:46,791:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:46,791:INFO:Initializing create_model()
2024-09-08 12:27:46,791:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:46,791:INFO:Checking exceptions
2024-09-08 12:27:46,791:INFO:Importing libraries
2024-09-08 12:27:46,791:INFO:Copying training dataset
2024-09-08 12:27:46,810:INFO:Defining folds
2024-09-08 12:27:46,810:INFO:Declaring metric variables
2024-09-08 12:27:46,810:INFO:Importing untrained model
2024-09-08 12:27:46,815:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 12:27:46,816:INFO:Starting cross validation
2024-09-08 12:27:46,823:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:27:50,290:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:50,357:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:50,376:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:50,480:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:50,712:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:50,761:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:50,785:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:50,793:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:52,721:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:52,724:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:52,766:INFO:Calculating mean and std
2024-09-08 12:27:52,766:INFO:Creating metrics dataframe
2024-09-08 12:27:52,771:INFO:Uploading results into container
2024-09-08 12:27:52,771:INFO:Uploading model into container now
2024-09-08 12:27:52,771:INFO:_master_model_container: 10
2024-09-08 12:27:52,771:INFO:_display_container: 2
2024-09-08 12:27:52,774:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 12:27:52,774:INFO:create_model() successfully completed......................................
2024-09-08 12:27:52,908:INFO:SubProcess create_model() end ==================================
2024-09-08 12:27:52,908:INFO:Creating metrics dataframe
2024-09-08 12:27:52,910:INFO:Initializing Linear Discriminant Analysis
2024-09-08 12:27:52,910:INFO:Total runtime is 0.7387197613716123 minutes
2024-09-08 12:27:52,910:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:52,910:INFO:Initializing create_model()
2024-09-08 12:27:52,910:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:52,910:INFO:Checking exceptions
2024-09-08 12:27:52,910:INFO:Importing libraries
2024-09-08 12:27:52,910:INFO:Copying training dataset
2024-09-08 12:27:52,924:INFO:Defining folds
2024-09-08 12:27:52,924:INFO:Declaring metric variables
2024-09-08 12:27:52,924:INFO:Importing untrained model
2024-09-08 12:27:52,924:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 12:27:52,924:INFO:Starting cross validation
2024-09-08 12:27:52,940:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:27:53,965:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:53,991:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:53,998:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:53,998:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:54,051:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:54,056:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:54,071:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:54,096:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:54,671:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:54,693:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:27:54,728:INFO:Calculating mean and std
2024-09-08 12:27:54,731:INFO:Creating metrics dataframe
2024-09-08 12:27:54,731:INFO:Uploading results into container
2024-09-08 12:27:54,735:INFO:Uploading model into container now
2024-09-08 12:27:54,736:INFO:_master_model_container: 11
2024-09-08 12:27:54,736:INFO:_display_container: 2
2024-09-08 12:27:54,736:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 12:27:54,736:INFO:create_model() successfully completed......................................
2024-09-08 12:27:54,871:INFO:SubProcess create_model() end ==================================
2024-09-08 12:27:54,875:INFO:Creating metrics dataframe
2024-09-08 12:27:54,875:INFO:Initializing Extra Trees Classifier
2024-09-08 12:27:54,875:INFO:Total runtime is 0.7714595079421995 minutes
2024-09-08 12:27:54,875:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:54,875:INFO:Initializing create_model()
2024-09-08 12:27:54,875:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:54,875:INFO:Checking exceptions
2024-09-08 12:27:54,875:INFO:Importing libraries
2024-09-08 12:27:54,875:INFO:Copying training dataset
2024-09-08 12:27:54,891:INFO:Defining folds
2024-09-08 12:27:54,891:INFO:Declaring metric variables
2024-09-08 12:27:54,891:INFO:Importing untrained model
2024-09-08 12:27:54,891:INFO:Extra Trees Classifier Imported successfully
2024-09-08 12:27:54,891:INFO:Starting cross validation
2024-09-08 12:27:54,901:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:27:57,572:INFO:Calculating mean and std
2024-09-08 12:27:57,572:INFO:Creating metrics dataframe
2024-09-08 12:27:57,576:INFO:Uploading results into container
2024-09-08 12:27:57,576:INFO:Uploading model into container now
2024-09-08 12:27:57,576:INFO:_master_model_container: 12
2024-09-08 12:27:57,576:INFO:_display_container: 2
2024-09-08 12:27:57,581:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 12:27:57,581:INFO:create_model() successfully completed......................................
2024-09-08 12:27:57,731:INFO:SubProcess create_model() end ==================================
2024-09-08 12:27:57,731:INFO:Creating metrics dataframe
2024-09-08 12:27:57,741:INFO:Initializing Extreme Gradient Boosting
2024-09-08 12:27:57,741:INFO:Total runtime is 0.8192230145136513 minutes
2024-09-08 12:27:57,741:INFO:SubProcess create_model() called ==================================
2024-09-08 12:27:57,741:INFO:Initializing create_model()
2024-09-08 12:27:57,741:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:27:57,741:INFO:Checking exceptions
2024-09-08 12:27:57,741:INFO:Importing libraries
2024-09-08 12:27:57,741:INFO:Copying training dataset
2024-09-08 12:27:57,757:INFO:Defining folds
2024-09-08 12:27:57,757:INFO:Declaring metric variables
2024-09-08 12:27:57,757:INFO:Importing untrained model
2024-09-08 12:27:57,757:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:27:57,757:INFO:Starting cross validation
2024-09-08 12:27:57,762:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:28:01,322:INFO:Calculating mean and std
2024-09-08 12:28:01,325:INFO:Creating metrics dataframe
2024-09-08 12:28:01,325:INFO:Uploading results into container
2024-09-08 12:28:01,325:INFO:Uploading model into container now
2024-09-08 12:28:01,325:INFO:_master_model_container: 13
2024-09-08 12:28:01,325:INFO:_display_container: 2
2024-09-08 12:28:01,331:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 12:28:01,331:INFO:create_model() successfully completed......................................
2024-09-08 12:28:01,482:INFO:SubProcess create_model() end ==================================
2024-09-08 12:28:01,482:INFO:Creating metrics dataframe
2024-09-08 12:28:01,486:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 12:28:01,486:INFO:Total runtime is 0.8816494385401406 minutes
2024-09-08 12:28:01,486:INFO:SubProcess create_model() called ==================================
2024-09-08 12:28:01,489:INFO:Initializing create_model()
2024-09-08 12:28:01,489:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:28:01,489:INFO:Checking exceptions
2024-09-08 12:28:01,489:INFO:Importing libraries
2024-09-08 12:28:01,489:INFO:Copying training dataset
2024-09-08 12:28:01,502:INFO:Defining folds
2024-09-08 12:28:01,502:INFO:Declaring metric variables
2024-09-08 12:28:01,502:INFO:Importing untrained model
2024-09-08 12:28:01,502:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 12:28:01,505:INFO:Starting cross validation
2024-09-08 12:28:01,511:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:28:08,643:INFO:Calculating mean and std
2024-09-08 12:28:08,643:INFO:Creating metrics dataframe
2024-09-08 12:28:08,643:INFO:Uploading results into container
2024-09-08 12:28:08,643:INFO:Uploading model into container now
2024-09-08 12:28:08,651:INFO:_master_model_container: 14
2024-09-08 12:28:08,651:INFO:_display_container: 2
2024-09-08 12:28:08,651:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 12:28:08,651:INFO:create_model() successfully completed......................................
2024-09-08 12:28:08,797:INFO:SubProcess create_model() end ==================================
2024-09-08 12:28:08,797:INFO:Creating metrics dataframe
2024-09-08 12:28:08,805:INFO:Initializing Dummy Classifier
2024-09-08 12:28:08,805:INFO:Total runtime is 1.0036257783571876 minutes
2024-09-08 12:28:08,805:INFO:SubProcess create_model() called ==================================
2024-09-08 12:28:08,805:INFO:Initializing create_model()
2024-09-08 12:28:08,805:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7AF19BD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:28:08,805:INFO:Checking exceptions
2024-09-08 12:28:08,805:INFO:Importing libraries
2024-09-08 12:28:08,805:INFO:Copying training dataset
2024-09-08 12:28:08,822:INFO:Defining folds
2024-09-08 12:28:08,822:INFO:Declaring metric variables
2024-09-08 12:28:08,822:INFO:Importing untrained model
2024-09-08 12:28:08,822:INFO:Dummy Classifier Imported successfully
2024-09-08 12:28:08,822:INFO:Starting cross validation
2024-09-08 12:28:08,822:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:28:09,842:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:28:09,872:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:28:09,882:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:28:09,904:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:28:09,917:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:28:09,942:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:28:09,952:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:28:10,014:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:28:10,572:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:28:10,612:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:28:10,632:INFO:Calculating mean and std
2024-09-08 12:28:10,632:INFO:Creating metrics dataframe
2024-09-08 12:28:10,635:INFO:Uploading results into container
2024-09-08 12:28:10,635:INFO:Uploading model into container now
2024-09-08 12:28:10,635:INFO:_master_model_container: 15
2024-09-08 12:28:10,635:INFO:_display_container: 2
2024-09-08 12:28:10,635:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 12:28:10,635:INFO:create_model() successfully completed......................................
2024-09-08 12:28:10,772:INFO:SubProcess create_model() end ==================================
2024-09-08 12:28:10,772:INFO:Creating metrics dataframe
2024-09-08 12:28:10,782:INFO:Initializing create_model()
2024-09-08 12:28:10,782:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:28:10,782:INFO:Checking exceptions
2024-09-08 12:28:10,782:INFO:Importing libraries
2024-09-08 12:28:10,782:INFO:Copying training dataset
2024-09-08 12:28:10,797:INFO:Defining folds
2024-09-08 12:28:10,797:INFO:Declaring metric variables
2024-09-08 12:28:10,797:INFO:Importing untrained model
2024-09-08 12:28:10,797:INFO:Declaring custom model
2024-09-08 12:28:10,797:INFO:Random Forest Classifier Imported successfully
2024-09-08 12:28:10,803:INFO:Cross validation set to False
2024-09-08 12:28:10,803:INFO:Fitting Model
2024-09-08 12:28:11,454:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 12:28:11,454:INFO:create_model() successfully completed......................................
2024-09-08 12:28:11,587:INFO:Initializing create_model()
2024-09-08 12:28:11,587:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:28:11,587:INFO:Checking exceptions
2024-09-08 12:28:11,587:INFO:Importing libraries
2024-09-08 12:28:11,587:INFO:Copying training dataset
2024-09-08 12:28:11,602:INFO:Defining folds
2024-09-08 12:28:11,602:INFO:Declaring metric variables
2024-09-08 12:28:11,602:INFO:Importing untrained model
2024-09-08 12:28:11,602:INFO:Declaring custom model
2024-09-08 12:28:11,607:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 12:28:11,614:INFO:Cross validation set to False
2024-09-08 12:28:11,614:INFO:Fitting Model
2024-09-08 12:28:11,953:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 12:28:11,953:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.
2024-09-08 12:28:11,953:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 12:28:11,957:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 12:28:11,957:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 12:28:11,957:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 12:28:11,957:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 12:28:11,957:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 12:28:11,957:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 12:28:11,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:11,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:12,277:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 12:28:12,277:INFO:create_model() successfully completed......................................
2024-09-08 12:28:12,430:INFO:Initializing create_model()
2024-09-08 12:28:12,430:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:28:12,430:INFO:Checking exceptions
2024-09-08 12:28:12,432:INFO:Importing libraries
2024-09-08 12:28:12,432:INFO:Copying training dataset
2024-09-08 12:28:12,442:INFO:Defining folds
2024-09-08 12:28:12,442:INFO:Declaring metric variables
2024-09-08 12:28:12,442:INFO:Importing untrained model
2024-09-08 12:28:12,442:INFO:Declaring custom model
2024-09-08 12:28:12,446:INFO:Extra Trees Classifier Imported successfully
2024-09-08 12:28:12,452:INFO:Cross validation set to False
2024-09-08 12:28:12,452:INFO:Fitting Model
2024-09-08 12:28:13,037:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 12:28:13,037:INFO:create_model() successfully completed......................................
2024-09-08 12:28:13,205:INFO:_master_model_container: 15
2024-09-08 12:28:13,205:INFO:_display_container: 2
2024-09-08 12:28:13,207:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)]
2024-09-08 12:28:13,207:INFO:compare_models() successfully completed......................................
2024-09-08 12:28:13,207:INFO:Initializing create_model()
2024-09-08 12:28:13,207:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:28:13,207:INFO:Checking exceptions
2024-09-08 12:28:13,210:INFO:Importing libraries
2024-09-08 12:28:13,210:INFO:Copying training dataset
2024-09-08 12:28:13,222:INFO:Defining folds
2024-09-08 12:28:13,222:INFO:Declaring metric variables
2024-09-08 12:28:13,222:INFO:Importing untrained model
2024-09-08 12:28:13,227:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 12:28:13,228:INFO:Starting cross validation
2024-09-08 12:28:13,236:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:28:21,510:INFO:Calculating mean and std
2024-09-08 12:28:21,513:INFO:Creating metrics dataframe
2024-09-08 12:28:21,513:INFO:Finalizing model
2024-09-08 12:28:21,873:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 12:28:21,880:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001437 seconds.
2024-09-08 12:28:21,880:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-09-08 12:28:21,880:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 12:28:21,880:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 12:28:21,883:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 12:28:21,883:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 12:28:21,883:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 12:28:21,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:21,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:28:22,168:INFO:Uploading results into container
2024-09-08 12:28:22,168:INFO:Uploading model into container now
2024-09-08 12:28:22,194:INFO:_master_model_container: 16
2024-09-08 12:28:22,194:INFO:_display_container: 3
2024-09-08 12:28:22,194:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 12:28:22,194:INFO:create_model() successfully completed......................................
2024-09-08 12:28:22,333:INFO:Initializing create_model()
2024-09-08 12:28:22,333:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:28:22,333:INFO:Checking exceptions
2024-09-08 12:28:22,333:INFO:Importing libraries
2024-09-08 12:28:22,333:INFO:Copying training dataset
2024-09-08 12:28:22,352:INFO:Defining folds
2024-09-08 12:28:22,352:INFO:Declaring metric variables
2024-09-08 12:28:22,352:INFO:Importing untrained model
2024-09-08 12:28:22,357:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:28:22,357:INFO:Starting cross validation
2024-09-08 12:28:22,365:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:28:25,603:INFO:Calculating mean and std
2024-09-08 12:28:25,603:INFO:Creating metrics dataframe
2024-09-08 12:28:25,603:INFO:Finalizing model
2024-09-08 12:28:26,390:INFO:Uploading results into container
2024-09-08 12:28:26,393:INFO:Uploading model into container now
2024-09-08 12:28:26,416:INFO:_master_model_container: 17
2024-09-08 12:28:26,416:INFO:_display_container: 4
2024-09-08 12:28:26,416:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:28:26,419:INFO:create_model() successfully completed......................................
2024-09-08 12:28:26,563:INFO:Initializing create_model()
2024-09-08 12:28:26,563:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:28:26,563:INFO:Checking exceptions
2024-09-08 12:28:26,568:INFO:Importing libraries
2024-09-08 12:28:26,568:INFO:Copying training dataset
2024-09-08 12:28:26,583:INFO:Defining folds
2024-09-08 12:28:26,583:INFO:Declaring metric variables
2024-09-08 12:28:26,583:INFO:Importing untrained model
2024-09-08 12:28:26,583:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 12:28:26,583:INFO:Starting cross validation
2024-09-08 12:28:26,596:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:28:29,750:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:28:29,773:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:28:29,794:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:28:29,928:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:28:29,988:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:28:30,013:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:28:30,051:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:28:30,073:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:28:32,167:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:28:32,203:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:28:32,234:INFO:Calculating mean and std
2024-09-08 12:28:32,234:INFO:Creating metrics dataframe
2024-09-08 12:28:32,234:INFO:Finalizing model
2024-09-08 12:28:34,854:INFO:Uploading results into container
2024-09-08 12:28:34,855:INFO:Uploading model into container now
2024-09-08 12:28:34,880:INFO:_master_model_container: 18
2024-09-08 12:28:34,880:INFO:_display_container: 5
2024-09-08 12:28:34,884:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 12:28:34,884:INFO:create_model() successfully completed......................................
2024-09-08 12:28:35,074:INFO:Initializing tune_model()
2024-09-08 12:28:35,074:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>)
2024-09-08 12:28:35,074:INFO:Checking exceptions
2024-09-08 12:28:35,074:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 12:28:35,174:INFO:Copying training dataset
2024-09-08 12:28:35,188:INFO:Checking base model
2024-09-08 12:28:35,188:INFO:Base model : Extreme Gradient Boosting
2024-09-08 12:28:35,188:INFO:Declaring metric variables
2024-09-08 12:28:35,188:INFO:Defining Hyperparameters
2024-09-08 12:28:35,335:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 12:28:35,339:INFO:Tuning with n_jobs=-1
2024-09-08 12:28:35,343:INFO:Initializing skopt.BayesSearchCV
2024-09-08 12:28:50,616:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.6), ('actual_estimator__gamma', 0), ('actual_estimator__learning_rate', 0.01), ('actual_estimator__max_depth', 7), ('actual_estimator__min_child_weight', 1), ('actual_estimator__n_estimators', 100), ('actual_estimator__subsample', 0.6)])
2024-09-08 12:28:50,616:INFO:Hyperparameter search completed
2024-09-08 12:28:50,616:INFO:SubProcess create_model() called ==================================
2024-09-08 12:28:50,616:INFO:Initializing create_model()
2024-09-08 12:28:50,616:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C7ADB87FA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.6})
2024-09-08 12:28:50,616:INFO:Checking exceptions
2024-09-08 12:28:50,616:INFO:Importing libraries
2024-09-08 12:28:50,616:INFO:Copying training dataset
2024-09-08 12:28:50,633:INFO:Defining folds
2024-09-08 12:28:50,633:INFO:Declaring metric variables
2024-09-08 12:28:50,633:INFO:Importing untrained model
2024-09-08 12:28:50,633:INFO:Declaring custom model
2024-09-08 12:28:50,635:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:28:50,635:INFO:Starting cross validation
2024-09-08 12:28:50,640:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:28:52,309:INFO:Calculating mean and std
2024-09-08 12:28:52,309:INFO:Creating metrics dataframe
2024-09-08 12:28:52,309:INFO:Finalizing model
2024-09-08 12:28:53,195:INFO:Uploading results into container
2024-09-08 12:28:53,195:INFO:Uploading model into container now
2024-09-08 12:28:53,195:INFO:_master_model_container: 19
2024-09-08 12:28:53,195:INFO:_display_container: 6
2024-09-08 12:28:53,200:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:28:53,200:INFO:create_model() successfully completed......................................
2024-09-08 12:28:53,350:INFO:SubProcess create_model() end ==================================
2024-09-08 12:28:53,350:INFO:choose_better activated
2024-09-08 12:28:53,350:INFO:SubProcess create_model() called ==================================
2024-09-08 12:28:53,355:INFO:Initializing create_model()
2024-09-08 12:28:53,355:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:28:53,355:INFO:Checking exceptions
2024-09-08 12:28:53,355:INFO:Importing libraries
2024-09-08 12:28:53,355:INFO:Copying training dataset
2024-09-08 12:28:53,365:INFO:Defining folds
2024-09-08 12:28:53,365:INFO:Declaring metric variables
2024-09-08 12:28:53,365:INFO:Importing untrained model
2024-09-08 12:28:53,365:INFO:Declaring custom model
2024-09-08 12:28:53,365:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:28:53,365:INFO:Starting cross validation
2024-09-08 12:28:53,377:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:28:54,885:INFO:Calculating mean and std
2024-09-08 12:28:54,885:INFO:Creating metrics dataframe
2024-09-08 12:28:54,885:INFO:Finalizing model
2024-09-08 12:28:55,625:INFO:Uploading results into container
2024-09-08 12:28:55,625:INFO:Uploading model into container now
2024-09-08 12:28:55,625:INFO:_master_model_container: 20
2024-09-08 12:28:55,625:INFO:_display_container: 7
2024-09-08 12:28:55,625:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:28:55,630:INFO:create_model() successfully completed......................................
2024-09-08 12:28:55,775:INFO:SubProcess create_model() end ==================================
2024-09-08 12:28:55,775:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8186
2024-09-08 12:28:55,782:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8386
2024-09-08 12:28:55,782:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 12:28:55,782:INFO:choose_better completed
2024-09-08 12:28:55,798:INFO:_master_model_container: 20
2024-09-08 12:28:55,798:INFO:_display_container: 6
2024-09-08 12:28:55,798:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:28:55,798:INFO:tune_model() successfully completed......................................
2024-09-08 12:28:55,939:INFO:Initializing predict_model()
2024-09-08 12:28:55,945:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001C798771480>)
2024-09-08 12:28:55,945:INFO:Checking exceptions
2024-09-08 12:28:55,945:INFO:Preloading libraries
2024-09-08 12:28:56,469:INFO:Initializing get_config()
2024-09-08 12:28:56,475:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, variable=X_train)
2024-09-08 12:28:56,475:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 12:28:56,530:INFO:Variable:  returned as       id Marital status Application mode Application order  ... Curricular units 2nd sem (without evaluations) Unemployment rate Inflation rate   GDP
765  765              1                1                 6  ...                                              0         10.800000            1.4  1.74
323  323              1               17                 1  ...                                              0          7.600000            2.6  0.32
909  909              1                1                 1  ...                                              0         12.400000            0.5  1.79
774  774              1               17                 1  ...                                              0          8.900000            1.4  3.51
931  931              1               39                 1  ...                                              0         16.200001            0.3 -0.92
..   ...            ...              ...               ...  ...                                            ...               ...            ...   ...
806  806              4               39                 1  ...                                              0         10.800000            1.4  1.74
492  492              1                1                 3  ...                                              0          7.600000            2.6  0.32
698  698              1               39                 1  ...                                              1         10.800000            1.4  1.74
44    44              1                1                 1  ...                                              0          9.400000           -0.8 -3.12
604  604              1                1                 1  ...                                              0         10.800000            1.4  1.74

[700 rows x 37 columns]
2024-09-08 12:28:56,530:INFO:get_config() successfully completed......................................
2024-09-08 12:28:56,530:INFO:Initializing predict_model()
2024-09-08 12:28:56,530:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001C7AF26DC60>)
2024-09-08 12:28:56,530:INFO:Checking exceptions
2024-09-08 12:28:56,530:INFO:Preloading libraries
2024-09-08 12:28:56,535:INFO:Set up data.
2024-09-08 12:28:56,545:INFO:Set up index.
2024-09-08 12:28:56,865:INFO:Initializing get_config()
2024-09-08 12:28:56,865:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, variable=y_train)
2024-09-08 12:28:56,865:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 12:28:56,875:INFO:Variable:  returned as 765    0
323    1
909    0
774    0
931    1
      ..
806    1
492    0
698    1
44     0
604    1
Name: Target, Length: 700, dtype: int8
2024-09-08 12:28:56,875:INFO:get_config() successfully completed......................................
2024-09-08 12:28:56,875:INFO:Initializing get_config()
2024-09-08 12:28:56,875:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, variable=y_test)
2024-09-08 12:28:56,875:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 12:28:56,880:INFO:Variable:  returned as 47     1
233    1
132    2
34     0
162    0
      ..
772    0
632    0
365    2
136    0
299    1
Name: Target, Length: 300, dtype: int8
2024-09-08 12:28:56,880:INFO:get_config() successfully completed......................................
2024-09-08 12:28:56,880:INFO:Initializing finalize_model()
2024-09-08 12:28:56,880:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 12:28:56,880:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:28:56,890:INFO:Initializing create_model()
2024-09-08 12:28:56,890:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:28:56,890:INFO:Checking exceptions
2024-09-08 12:28:56,890:INFO:Importing libraries
2024-09-08 12:28:56,895:INFO:Copying training dataset
2024-09-08 12:28:56,895:INFO:Defining folds
2024-09-08 12:28:56,895:INFO:Declaring metric variables
2024-09-08 12:28:56,895:INFO:Importing untrained model
2024-09-08 12:28:56,895:INFO:Declaring custom model
2024-09-08 12:28:56,895:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:28:56,906:INFO:Cross validation set to False
2024-09-08 12:28:56,906:INFO:Fitting Model
2024-09-08 12:28:58,125:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 12:28:58,125:INFO:create_model() successfully completed......................................
2024-09-08 12:28:58,260:INFO:_master_model_container: 20
2024-09-08 12:28:58,260:INFO:_display_container: 7
2024-09-08 12:28:58,521:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 12:28:58,521:INFO:finalize_model() successfully completed......................................
2024-09-08 12:28:58,965:INFO:Initializing predict_model()
2024-09-08 12:28:58,965:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C7AD2C33D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001C7AF5A4E50>)
2024-09-08 12:28:58,965:INFO:Checking exceptions
2024-09-08 12:28:58,965:INFO:Preloading libraries
2024-09-08 12:28:58,965:INFO:Set up data.
2024-09-08 12:28:58,997:INFO:Set up index.
2024-09-08 12:35:28,722:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 12:35:28,722:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 12:35:28,722:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 12:35:28,722:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-09-08 12:35:56,188:INFO:PyCaret ClassificationExperiment
2024-09-08 12:35:56,191:INFO:Logging name: clf-default-name
2024-09-08 12:35:56,191:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-09-08 12:35:56,191:INFO:version 3.3.2
2024-09-08 12:35:56,191:INFO:Initializing setup()
2024-09-08 12:35:56,191:INFO:self.USI: 30a5
2024-09-08 12:35:56,191:INFO:self._variable_keys: {'y_train', 'X_test', '_ml_usecase', 'USI', 'gpu_n_jobs_param', 'exp_name_log', 'logging_param', 'y', 'memory', 'data', 'X_train', 'html_param', 'fold_shuffle_param', 'X', 'fix_imbalance', 'y_test', 'target_param', 'fold_generator', 'idx', 'log_plots_param', 'exp_id', 'n_jobs_param', 'is_multiclass', 'pipeline', 'fold_groups_param', '_available_plots', 'gpu_param', 'seed'}
2024-09-08 12:35:56,191:INFO:Checking environment
2024-09-08 12:35:56,191:INFO:python_version: 3.10.11
2024-09-08 12:35:56,191:INFO:python_build: ('main', 'May 16 2023 00:55:32')
2024-09-08 12:35:56,191:INFO:machine: AMD64
2024-09-08 12:35:56,211:INFO:platform: Windows-10-10.0.22631-SP0
2024-09-08 12:35:56,213:INFO:Memory: svmem(total=16407719936, available=5987074048, percent=63.5, used=10420645888, free=5987074048)
2024-09-08 12:35:56,213:INFO:Physical Core: 4
2024-09-08 12:35:56,213:INFO:Logical Core: 8
2024-09-08 12:35:56,213:INFO:Checking libraries
2024-09-08 12:35:56,213:INFO:System:
2024-09-08 12:35:56,213:INFO:    python: 3.10.11 | packaged by Anaconda, Inc. | (main, May 16 2023, 00:55:32) [MSC v.1916 64 bit (AMD64)]
2024-09-08 12:35:56,213:INFO:executable: C:\Users\mbr19\anaconda3\envs\my_env\python.exe
2024-09-08 12:35:56,213:INFO:   machine: Windows-10-10.0.22631-SP0
2024-09-08 12:35:56,213:INFO:PyCaret required dependencies:
2024-09-08 12:35:56,319:INFO:                 pip: 24.2
2024-09-08 12:35:56,321:INFO:          setuptools: 72.1.0
2024-09-08 12:35:56,321:INFO:             pycaret: 3.3.2
2024-09-08 12:35:56,321:INFO:             IPython: 8.25.0
2024-09-08 12:35:56,321:INFO:          ipywidgets: 8.1.5
2024-09-08 12:35:56,321:INFO:                tqdm: 4.66.5
2024-09-08 12:35:56,321:INFO:               numpy: 1.26.4
2024-09-08 12:35:56,321:INFO:              pandas: 2.1.4
2024-09-08 12:35:56,321:INFO:              jinja2: 3.1.4
2024-09-08 12:35:56,321:INFO:               scipy: 1.11.4
2024-09-08 12:35:56,321:INFO:              joblib: 1.3.2
2024-09-08 12:35:56,321:INFO:             sklearn: 1.4.2
2024-09-08 12:35:56,321:INFO:                pyod: 2.0.1
2024-09-08 12:35:56,321:INFO:            imblearn: 0.12.3
2024-09-08 12:35:56,321:INFO:   category_encoders: 2.6.3
2024-09-08 12:35:56,321:INFO:            lightgbm: 4.5.0
2024-09-08 12:35:56,321:INFO:               numba: 0.60.0
2024-09-08 12:35:56,321:INFO:            requests: 2.32.3
2024-09-08 12:35:56,328:INFO:          matplotlib: 3.7.5
2024-09-08 12:35:56,328:INFO:          scikitplot: 0.3.7
2024-09-08 12:35:56,328:INFO:         yellowbrick: 1.5
2024-09-08 12:35:56,328:INFO:              plotly: 5.24.0
2024-09-08 12:35:56,328:INFO:    plotly-resampler: Not installed
2024-09-08 12:35:56,328:INFO:             kaleido: 0.2.1
2024-09-08 12:35:56,328:INFO:           schemdraw: 0.15
2024-09-08 12:35:56,328:INFO:         statsmodels: 0.14.2
2024-09-08 12:35:56,328:INFO:              sktime: 0.26.0
2024-09-08 12:35:56,328:INFO:               tbats: 1.1.3
2024-09-08 12:35:56,328:INFO:            pmdarima: 2.0.4
2024-09-08 12:35:56,328:INFO:              psutil: 5.9.0
2024-09-08 12:35:56,328:INFO:          markupsafe: 2.1.3
2024-09-08 12:35:56,328:INFO:             pickle5: Not installed
2024-09-08 12:35:56,331:INFO:         cloudpickle: 3.0.0
2024-09-08 12:35:56,331:INFO:         deprecation: 2.1.0
2024-09-08 12:35:56,331:INFO:              xxhash: 3.5.0
2024-09-08 12:35:56,331:INFO:           wurlitzer: Not installed
2024-09-08 12:35:56,331:INFO:PyCaret optional dependencies:
2024-09-08 12:35:56,361:INFO:                shap: Not installed
2024-09-08 12:35:56,361:INFO:           interpret: Not installed
2024-09-08 12:35:56,361:INFO:                umap: Not installed
2024-09-08 12:35:56,361:INFO:     ydata_profiling: Not installed
2024-09-08 12:35:56,361:INFO:  explainerdashboard: Not installed
2024-09-08 12:35:56,361:INFO:             autoviz: Not installed
2024-09-08 12:35:56,361:INFO:           fairlearn: Not installed
2024-09-08 12:35:56,361:INFO:          deepchecks: Not installed
2024-09-08 12:35:56,361:INFO:             xgboost: 2.1.1
2024-09-08 12:35:56,361:INFO:            catboost: Not installed
2024-09-08 12:35:56,361:INFO:              kmodes: Not installed
2024-09-08 12:35:56,361:INFO:             mlxtend: Not installed
2024-09-08 12:35:56,361:INFO:       statsforecast: Not installed
2024-09-08 12:35:56,361:INFO:        tune_sklearn: Not installed
2024-09-08 12:35:56,361:INFO:                 ray: Not installed
2024-09-08 12:35:56,361:INFO:            hyperopt: 0.2.7
2024-09-08 12:35:56,361:INFO:              optuna: 4.0.0
2024-09-08 12:35:56,361:INFO:               skopt: 0.10.2
2024-09-08 12:35:56,361:INFO:              mlflow: Not installed
2024-09-08 12:35:56,361:INFO:              gradio: Not installed
2024-09-08 12:35:56,361:INFO:             fastapi: Not installed
2024-09-08 12:35:56,361:INFO:             uvicorn: Not installed
2024-09-08 12:35:56,361:INFO:              m2cgen: Not installed
2024-09-08 12:35:56,361:INFO:           evidently: Not installed
2024-09-08 12:35:56,361:INFO:               fugue: Not installed
2024-09-08 12:35:56,361:INFO:           streamlit: 1.38.0
2024-09-08 12:35:56,361:INFO:             prophet: Not installed
2024-09-08 12:35:56,361:INFO:None
2024-09-08 12:35:56,361:INFO:Set up data.
2024-09-08 12:35:56,385:INFO:Set up folding strategy.
2024-09-08 12:35:56,385:INFO:Set up train/test split.
2024-09-08 12:35:56,401:INFO:Set up index.
2024-09-08 12:35:56,401:INFO:Assigning column types.
2024-09-08 12:35:56,416:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-09-08 12:35:56,498:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 12:35:56,501:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 12:35:56,561:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:35:56,566:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:35:56,651:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-09-08 12:35:56,651:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 12:35:56,706:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:35:56,711:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:35:56,711:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-09-08 12:35:56,796:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 12:35:56,856:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:35:56,861:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:35:56,946:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-09-08 12:35:57,003:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:35:57,004:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:35:57,004:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-09-08 12:35:57,143:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:35:57,151:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:35:57,292:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:35:57,296:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:35:57,301:INFO:Preparing preprocessing pipeline...
2024-09-08 12:35:57,301:INFO:Set up simple imputation.
2024-09-08 12:35:57,316:INFO:Set up encoding of ordinal features.
2024-09-08 12:35:57,343:INFO:Set up encoding of categorical features.
2024-09-08 12:35:57,343:INFO:Set up column name cleaning.
2024-09-08 12:35:57,787:INFO:Finished creating preprocessing pipeline.
2024-09-08 12:35:58,148:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\mbr19\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st se...
                                                                    'qualification',
                                                                    'Fathers '
                                                                    'qualification',
                                                                    'Mothers '
                                                                    'occupation',
                                                                    'Fathers '
                                                                    'occupation'],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-09-08 12:35:58,148:INFO:Creating final display dataframe.
2024-09-08 12:35:58,639:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            Target
2                   Target type        Multiclass
3           Original data shape        (1000, 38)
4        Transformed data shape       (1000, 129)
5   Transformed train set shape        (700, 129)
6    Transformed test set shape        (300, 129)
7              Numeric features                19
8          Categorical features                18
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13     Maximum one-hot encoding                25
14              Encoding method              None
15               Fold Generator   StratifiedKFold
16                  Fold Number                10
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              30a5
2024-09-08 12:35:58,815:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:35:58,822:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:35:58,982:INFO:Soft dependency imported: xgboost: 2.1.1
2024-09-08 12:35:58,991:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-09-08 12:35:58,991:INFO:setup() successfully completed in 2.83s...............
2024-09-08 12:35:58,995:INFO:Initializing compare_models()
2024-09-08 12:35:58,995:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, include=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-09-08 12:35:58,995:INFO:Checking exceptions
2024-09-08 12:35:59,003:INFO:Preparing display monitor
2024-09-08 12:35:59,012:INFO:Initializing Logistic Regression
2024-09-08 12:35:59,012:INFO:Total runtime is 0.0 minutes
2024-09-08 12:35:59,012:INFO:SubProcess create_model() called ==================================
2024-09-08 12:35:59,012:INFO:Initializing create_model()
2024-09-08 12:35:59,012:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:35:59,012:INFO:Checking exceptions
2024-09-08 12:35:59,012:INFO:Importing libraries
2024-09-08 12:35:59,012:INFO:Copying training dataset
2024-09-08 12:35:59,032:INFO:Defining folds
2024-09-08 12:35:59,036:INFO:Declaring metric variables
2024-09-08 12:35:59,036:INFO:Importing untrained model
2024-09-08 12:35:59,038:INFO:Logistic Regression Imported successfully
2024-09-08 12:35:59,038:INFO:Starting cross validation
2024-09-08 12:35:59,056:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:10,576:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:36:10,662:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:36:10,668:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:36:10,752:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:36:10,774:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:36:10,791:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:36:10,873:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:10,882:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:10,906:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:10,914:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:10,928:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:36:10,996:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:11,017:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:11,045:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:11,168:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:12,184:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:36:12,244:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-09-08 12:36:12,357:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:12,397:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:12,430:INFO:Calculating mean and std
2024-09-08 12:36:12,430:INFO:Creating metrics dataframe
2024-09-08 12:36:12,433:INFO:Uploading results into container
2024-09-08 12:36:12,433:INFO:Uploading model into container now
2024-09-08 12:36:12,433:INFO:_master_model_container: 1
2024-09-08 12:36:12,433:INFO:_display_container: 2
2024-09-08 12:36:12,433:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-09-08 12:36:12,438:INFO:create_model() successfully completed......................................
2024-09-08 12:36:12,562:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:12,571:INFO:Creating metrics dataframe
2024-09-08 12:36:12,573:INFO:Initializing K Neighbors Classifier
2024-09-08 12:36:12,573:INFO:Total runtime is 0.22601423263549805 minutes
2024-09-08 12:36:12,573:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:12,573:INFO:Initializing create_model()
2024-09-08 12:36:12,573:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:12,573:INFO:Checking exceptions
2024-09-08 12:36:12,573:INFO:Importing libraries
2024-09-08 12:36:12,573:INFO:Copying training dataset
2024-09-08 12:36:12,588:INFO:Defining folds
2024-09-08 12:36:12,588:INFO:Declaring metric variables
2024-09-08 12:36:12,588:INFO:Importing untrained model
2024-09-08 12:36:12,588:INFO:K Neighbors Classifier Imported successfully
2024-09-08 12:36:12,588:INFO:Starting cross validation
2024-09-08 12:36:12,596:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:14,720:INFO:Calculating mean and std
2024-09-08 12:36:14,723:INFO:Creating metrics dataframe
2024-09-08 12:36:14,723:INFO:Uploading results into container
2024-09-08 12:36:14,728:INFO:Uploading model into container now
2024-09-08 12:36:14,728:INFO:_master_model_container: 2
2024-09-08 12:36:14,728:INFO:_display_container: 2
2024-09-08 12:36:14,728:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-09-08 12:36:14,728:INFO:create_model() successfully completed......................................
2024-09-08 12:36:14,870:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:14,873:INFO:Creating metrics dataframe
2024-09-08 12:36:14,878:INFO:Initializing Naive Bayes
2024-09-08 12:36:14,878:INFO:Total runtime is 0.264434031645457 minutes
2024-09-08 12:36:14,878:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:14,878:INFO:Initializing create_model()
2024-09-08 12:36:14,878:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:14,878:INFO:Checking exceptions
2024-09-08 12:36:14,878:INFO:Importing libraries
2024-09-08 12:36:14,878:INFO:Copying training dataset
2024-09-08 12:36:14,894:INFO:Defining folds
2024-09-08 12:36:14,894:INFO:Declaring metric variables
2024-09-08 12:36:14,894:INFO:Importing untrained model
2024-09-08 12:36:14,894:INFO:Naive Bayes Imported successfully
2024-09-08 12:36:14,894:INFO:Starting cross validation
2024-09-08 12:36:14,903:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:15,943:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:16,783:INFO:Calculating mean and std
2024-09-08 12:36:16,784:INFO:Creating metrics dataframe
2024-09-08 12:36:16,784:INFO:Uploading results into container
2024-09-08 12:36:16,784:INFO:Uploading model into container now
2024-09-08 12:36:16,784:INFO:_master_model_container: 3
2024-09-08 12:36:16,784:INFO:_display_container: 2
2024-09-08 12:36:16,784:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-09-08 12:36:16,784:INFO:create_model() successfully completed......................................
2024-09-08 12:36:16,955:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:16,955:INFO:Creating metrics dataframe
2024-09-08 12:36:16,963:INFO:Initializing Decision Tree Classifier
2024-09-08 12:36:16,963:INFO:Total runtime is 0.2991894602775574 minutes
2024-09-08 12:36:16,963:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:16,966:INFO:Initializing create_model()
2024-09-08 12:36:16,966:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:16,966:INFO:Checking exceptions
2024-09-08 12:36:16,966:INFO:Importing libraries
2024-09-08 12:36:16,966:INFO:Copying training dataset
2024-09-08 12:36:16,988:INFO:Defining folds
2024-09-08 12:36:16,988:INFO:Declaring metric variables
2024-09-08 12:36:16,993:INFO:Importing untrained model
2024-09-08 12:36:16,993:INFO:Decision Tree Classifier Imported successfully
2024-09-08 12:36:16,993:INFO:Starting cross validation
2024-09-08 12:36:17,005:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:18,849:INFO:Calculating mean and std
2024-09-08 12:36:18,849:INFO:Creating metrics dataframe
2024-09-08 12:36:18,853:INFO:Uploading results into container
2024-09-08 12:36:18,853:INFO:Uploading model into container now
2024-09-08 12:36:18,853:INFO:_master_model_container: 4
2024-09-08 12:36:18,853:INFO:_display_container: 2
2024-09-08 12:36:18,857:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-09-08 12:36:18,857:INFO:create_model() successfully completed......................................
2024-09-08 12:36:19,006:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:19,006:INFO:Creating metrics dataframe
2024-09-08 12:36:19,014:INFO:Initializing SVM - Linear Kernel
2024-09-08 12:36:19,014:INFO:Total runtime is 0.3333672086397807 minutes
2024-09-08 12:36:19,014:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:19,014:INFO:Initializing create_model()
2024-09-08 12:36:19,014:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:19,018:INFO:Checking exceptions
2024-09-08 12:36:19,018:INFO:Importing libraries
2024-09-08 12:36:19,018:INFO:Copying training dataset
2024-09-08 12:36:19,033:INFO:Defining folds
2024-09-08 12:36:19,033:INFO:Declaring metric variables
2024-09-08 12:36:19,033:INFO:Importing untrained model
2024-09-08 12:36:19,033:INFO:SVM - Linear Kernel Imported successfully
2024-09-08 12:36:19,033:INFO:Starting cross validation
2024-09-08 12:36:19,043:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:20,133:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:20,140:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:20,153:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:20,157:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:20,165:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:20,173:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:20,173:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:20,190:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:20,190:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:20,215:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:20,215:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:20,712:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:20,713:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:20,737:INFO:Calculating mean and std
2024-09-08 12:36:20,737:INFO:Creating metrics dataframe
2024-09-08 12:36:20,737:INFO:Uploading results into container
2024-09-08 12:36:20,743:INFO:Uploading model into container now
2024-09-08 12:36:20,743:INFO:_master_model_container: 5
2024-09-08 12:36:20,745:INFO:_display_container: 2
2024-09-08 12:36:20,745:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-09-08 12:36:20,745:INFO:create_model() successfully completed......................................
2024-09-08 12:36:20,878:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:20,878:INFO:Creating metrics dataframe
2024-09-08 12:36:20,883:INFO:Initializing Ridge Classifier
2024-09-08 12:36:20,886:INFO:Total runtime is 0.36457394758860273 minutes
2024-09-08 12:36:20,886:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:20,886:INFO:Initializing create_model()
2024-09-08 12:36:20,886:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:20,886:INFO:Checking exceptions
2024-09-08 12:36:20,888:INFO:Importing libraries
2024-09-08 12:36:20,888:INFO:Copying training dataset
2024-09-08 12:36:20,895:INFO:Defining folds
2024-09-08 12:36:20,903:INFO:Declaring metric variables
2024-09-08 12:36:20,904:INFO:Importing untrained model
2024-09-08 12:36:20,904:INFO:Ridge Classifier Imported successfully
2024-09-08 12:36:20,904:INFO:Starting cross validation
2024-09-08 12:36:20,912:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:21,917:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:21,953:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:21,981:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:21,983:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:21,983:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:21,989:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:22,018:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:22,033:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:22,518:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:22,543:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:22,574:INFO:Calculating mean and std
2024-09-08 12:36:22,576:INFO:Creating metrics dataframe
2024-09-08 12:36:22,576:INFO:Uploading results into container
2024-09-08 12:36:22,576:INFO:Uploading model into container now
2024-09-08 12:36:22,576:INFO:_master_model_container: 6
2024-09-08 12:36:22,576:INFO:_display_container: 2
2024-09-08 12:36:22,576:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-09-08 12:36:22,576:INFO:create_model() successfully completed......................................
2024-09-08 12:36:22,714:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:22,714:INFO:Creating metrics dataframe
2024-09-08 12:36:22,714:INFO:Initializing Random Forest Classifier
2024-09-08 12:36:22,714:INFO:Total runtime is 0.39504309495290124 minutes
2024-09-08 12:36:22,714:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:22,714:INFO:Initializing create_model()
2024-09-08 12:36:22,723:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:22,723:INFO:Checking exceptions
2024-09-08 12:36:22,723:INFO:Importing libraries
2024-09-08 12:36:22,723:INFO:Copying training dataset
2024-09-08 12:36:22,733:INFO:Defining folds
2024-09-08 12:36:22,733:INFO:Declaring metric variables
2024-09-08 12:36:22,733:INFO:Importing untrained model
2024-09-08 12:36:22,738:INFO:Random Forest Classifier Imported successfully
2024-09-08 12:36:22,738:INFO:Starting cross validation
2024-09-08 12:36:22,743:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:25,802:INFO:Calculating mean and std
2024-09-08 12:36:25,803:INFO:Creating metrics dataframe
2024-09-08 12:36:25,803:INFO:Uploading results into container
2024-09-08 12:36:25,803:INFO:Uploading model into container now
2024-09-08 12:36:25,810:INFO:_master_model_container: 7
2024-09-08 12:36:25,810:INFO:_display_container: 2
2024-09-08 12:36:25,810:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 12:36:25,810:INFO:create_model() successfully completed......................................
2024-09-08 12:36:25,947:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:25,947:INFO:Creating metrics dataframe
2024-09-08 12:36:25,954:INFO:Initializing Quadratic Discriminant Analysis
2024-09-08 12:36:25,954:INFO:Total runtime is 0.4490289251009624 minutes
2024-09-08 12:36:25,954:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:25,956:INFO:Initializing create_model()
2024-09-08 12:36:25,956:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:25,956:INFO:Checking exceptions
2024-09-08 12:36:25,956:INFO:Importing libraries
2024-09-08 12:36:25,956:INFO:Copying training dataset
2024-09-08 12:36:25,965:INFO:Defining folds
2024-09-08 12:36:25,965:INFO:Declaring metric variables
2024-09-08 12:36:25,965:INFO:Importing untrained model
2024-09-08 12:36:25,965:INFO:Quadratic Discriminant Analysis Imported successfully
2024-09-08 12:36:25,965:INFO:Starting cross validation
2024-09-08 12:36:25,978:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:26,745:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:36:26,783:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:36:26,804:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:36:26,811:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:36:26,814:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:36:26,819:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:36:26,835:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:36:26,863:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:36:27,020:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:27,054:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:27,111:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:27,124:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:27,127:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:27,127:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:27,136:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:27,160:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:27,574:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:36:27,595:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-09-08 12:36:27,738:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:27,767:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:27,794:INFO:Calculating mean and std
2024-09-08 12:36:27,794:INFO:Creating metrics dataframe
2024-09-08 12:36:27,802:INFO:Uploading results into container
2024-09-08 12:36:27,804:INFO:Uploading model into container now
2024-09-08 12:36:27,804:INFO:_master_model_container: 8
2024-09-08 12:36:27,804:INFO:_display_container: 2
2024-09-08 12:36:27,804:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-09-08 12:36:27,804:INFO:create_model() successfully completed......................................
2024-09-08 12:36:27,944:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:27,944:INFO:Creating metrics dataframe
2024-09-08 12:36:27,949:INFO:Initializing Ada Boost Classifier
2024-09-08 12:36:27,949:INFO:Total runtime is 0.4822778701782227 minutes
2024-09-08 12:36:27,949:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:27,949:INFO:Initializing create_model()
2024-09-08 12:36:27,949:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:27,949:INFO:Checking exceptions
2024-09-08 12:36:27,949:INFO:Importing libraries
2024-09-08 12:36:27,949:INFO:Copying training dataset
2024-09-08 12:36:27,967:INFO:Defining folds
2024-09-08 12:36:27,967:INFO:Declaring metric variables
2024-09-08 12:36:27,967:INFO:Importing untrained model
2024-09-08 12:36:27,967:INFO:Ada Boost Classifier Imported successfully
2024-09-08 12:36:27,967:INFO:Starting cross validation
2024-09-08 12:36:27,974:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:28,709:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:36:28,744:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:36:28,744:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:36:28,754:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:36:28,774:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:36:28,789:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:36:28,794:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:36:28,804:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:36:29,399:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:29,455:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:29,494:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:29,496:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:29,520:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:29,534:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:29,554:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:29,569:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:30,023:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:36:30,044:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-09-08 12:36:30,484:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:30,512:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:30,554:INFO:Calculating mean and std
2024-09-08 12:36:30,554:INFO:Creating metrics dataframe
2024-09-08 12:36:30,563:INFO:Uploading results into container
2024-09-08 12:36:30,563:INFO:Uploading model into container now
2024-09-08 12:36:30,563:INFO:_master_model_container: 9
2024-09-08 12:36:30,563:INFO:_display_container: 2
2024-09-08 12:36:30,563:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-09-08 12:36:30,563:INFO:create_model() successfully completed......................................
2024-09-08 12:36:30,725:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:30,725:INFO:Creating metrics dataframe
2024-09-08 12:36:30,725:INFO:Initializing Gradient Boosting Classifier
2024-09-08 12:36:30,725:INFO:Total runtime is 0.5285481651624044 minutes
2024-09-08 12:36:30,725:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:30,733:INFO:Initializing create_model()
2024-09-08 12:36:30,733:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:30,733:INFO:Checking exceptions
2024-09-08 12:36:30,734:INFO:Importing libraries
2024-09-08 12:36:30,734:INFO:Copying training dataset
2024-09-08 12:36:30,749:INFO:Defining folds
2024-09-08 12:36:30,749:INFO:Declaring metric variables
2024-09-08 12:36:30,749:INFO:Importing untrained model
2024-09-08 12:36:30,749:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 12:36:30,749:INFO:Starting cross validation
2024-09-08 12:36:30,757:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:34,734:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:34,879:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:34,895:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:34,994:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:36,051:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:36,174:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:36,220:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:36,264:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:38,090:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:38,214:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:38,239:INFO:Calculating mean and std
2024-09-08 12:36:38,239:INFO:Creating metrics dataframe
2024-09-08 12:36:38,244:INFO:Uploading results into container
2024-09-08 12:36:38,244:INFO:Uploading model into container now
2024-09-08 12:36:38,248:INFO:_master_model_container: 10
2024-09-08 12:36:38,248:INFO:_display_container: 2
2024-09-08 12:36:38,248:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 12:36:38,248:INFO:create_model() successfully completed......................................
2024-09-08 12:36:38,387:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:38,387:INFO:Creating metrics dataframe
2024-09-08 12:36:38,387:INFO:Initializing Linear Discriminant Analysis
2024-09-08 12:36:38,387:INFO:Total runtime is 0.656245756149292 minutes
2024-09-08 12:36:38,387:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:38,387:INFO:Initializing create_model()
2024-09-08 12:36:38,387:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:38,387:INFO:Checking exceptions
2024-09-08 12:36:38,387:INFO:Importing libraries
2024-09-08 12:36:38,387:INFO:Copying training dataset
2024-09-08 12:36:38,414:INFO:Defining folds
2024-09-08 12:36:38,414:INFO:Declaring metric variables
2024-09-08 12:36:38,414:INFO:Importing untrained model
2024-09-08 12:36:38,419:INFO:Linear Discriminant Analysis Imported successfully
2024-09-08 12:36:38,419:INFO:Starting cross validation
2024-09-08 12:36:38,427:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:39,444:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:39,455:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:39,485:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:39,496:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:39,538:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:39,546:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:39,571:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:39,595:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:40,203:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:40,203:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:36:40,245:INFO:Calculating mean and std
2024-09-08 12:36:40,245:INFO:Creating metrics dataframe
2024-09-08 12:36:40,245:INFO:Uploading results into container
2024-09-08 12:36:40,251:INFO:Uploading model into container now
2024-09-08 12:36:40,251:INFO:_master_model_container: 11
2024-09-08 12:36:40,251:INFO:_display_container: 2
2024-09-08 12:36:40,251:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-09-08 12:36:40,251:INFO:create_model() successfully completed......................................
2024-09-08 12:36:40,424:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:40,424:INFO:Creating metrics dataframe
2024-09-08 12:36:40,434:INFO:Initializing Extra Trees Classifier
2024-09-08 12:36:40,434:INFO:Total runtime is 0.6903743187586467 minutes
2024-09-08 12:36:40,434:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:40,434:INFO:Initializing create_model()
2024-09-08 12:36:40,434:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:40,434:INFO:Checking exceptions
2024-09-08 12:36:40,434:INFO:Importing libraries
2024-09-08 12:36:40,434:INFO:Copying training dataset
2024-09-08 12:36:40,455:INFO:Defining folds
2024-09-08 12:36:40,455:INFO:Declaring metric variables
2024-09-08 12:36:40,455:INFO:Importing untrained model
2024-09-08 12:36:40,465:INFO:Extra Trees Classifier Imported successfully
2024-09-08 12:36:40,465:INFO:Starting cross validation
2024-09-08 12:36:40,481:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:43,721:INFO:Calculating mean and std
2024-09-08 12:36:43,721:INFO:Creating metrics dataframe
2024-09-08 12:36:43,725:INFO:Uploading results into container
2024-09-08 12:36:43,725:INFO:Uploading model into container now
2024-09-08 12:36:43,730:INFO:_master_model_container: 12
2024-09-08 12:36:43,730:INFO:_display_container: 2
2024-09-08 12:36:43,730:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 12:36:43,730:INFO:create_model() successfully completed......................................
2024-09-08 12:36:43,875:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:43,875:INFO:Creating metrics dataframe
2024-09-08 12:36:43,876:INFO:Initializing Extreme Gradient Boosting
2024-09-08 12:36:43,880:INFO:Total runtime is 0.7478004574775696 minutes
2024-09-08 12:36:43,880:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:43,880:INFO:Initializing create_model()
2024-09-08 12:36:43,880:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:43,880:INFO:Checking exceptions
2024-09-08 12:36:43,880:INFO:Importing libraries
2024-09-08 12:36:43,880:INFO:Copying training dataset
2024-09-08 12:36:43,895:INFO:Defining folds
2024-09-08 12:36:43,895:INFO:Declaring metric variables
2024-09-08 12:36:43,895:INFO:Importing untrained model
2024-09-08 12:36:43,895:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:36:43,895:INFO:Starting cross validation
2024-09-08 12:36:43,905:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:47,733:INFO:Calculating mean and std
2024-09-08 12:36:47,735:INFO:Creating metrics dataframe
2024-09-08 12:36:47,735:INFO:Uploading results into container
2024-09-08 12:36:47,735:INFO:Uploading model into container now
2024-09-08 12:36:47,740:INFO:_master_model_container: 13
2024-09-08 12:36:47,740:INFO:_display_container: 2
2024-09-08 12:36:47,742:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2024-09-08 12:36:47,742:INFO:create_model() successfully completed......................................
2024-09-08 12:36:47,885:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:47,885:INFO:Creating metrics dataframe
2024-09-08 12:36:47,892:INFO:Initializing Light Gradient Boosting Machine
2024-09-08 12:36:47,892:INFO:Total runtime is 0.8146644552548727 minutes
2024-09-08 12:36:47,892:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:47,892:INFO:Initializing create_model()
2024-09-08 12:36:47,892:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:47,892:INFO:Checking exceptions
2024-09-08 12:36:47,892:INFO:Importing libraries
2024-09-08 12:36:47,892:INFO:Copying training dataset
2024-09-08 12:36:47,908:INFO:Defining folds
2024-09-08 12:36:47,908:INFO:Declaring metric variables
2024-09-08 12:36:47,908:INFO:Importing untrained model
2024-09-08 12:36:47,908:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 12:36:47,908:INFO:Starting cross validation
2024-09-08 12:36:47,916:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:56,888:INFO:Calculating mean and std
2024-09-08 12:36:56,888:INFO:Creating metrics dataframe
2024-09-08 12:36:56,888:INFO:Uploading results into container
2024-09-08 12:36:56,888:INFO:Uploading model into container now
2024-09-08 12:36:56,896:INFO:_master_model_container: 14
2024-09-08 12:36:56,896:INFO:_display_container: 2
2024-09-08 12:36:56,896:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 12:36:56,896:INFO:create_model() successfully completed......................................
2024-09-08 12:36:57,051:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:57,051:INFO:Creating metrics dataframe
2024-09-08 12:36:57,056:INFO:Initializing Dummy Classifier
2024-09-08 12:36:57,056:INFO:Total runtime is 0.9674077471097311 minutes
2024-09-08 12:36:57,056:INFO:SubProcess create_model() called ==================================
2024-09-08 12:36:57,056:INFO:Initializing create_model()
2024-09-08 12:36:57,056:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EEBD00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:57,056:INFO:Checking exceptions
2024-09-08 12:36:57,056:INFO:Importing libraries
2024-09-08 12:36:57,056:INFO:Copying training dataset
2024-09-08 12:36:57,067:INFO:Defining folds
2024-09-08 12:36:57,067:INFO:Declaring metric variables
2024-09-08 12:36:57,067:INFO:Importing untrained model
2024-09-08 12:36:57,067:INFO:Dummy Classifier Imported successfully
2024-09-08 12:36:57,067:INFO:Starting cross validation
2024-09-08 12:36:57,076:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:36:58,166:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:58,171:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:58,171:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:58,201:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:58,232:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:58,233:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:58,316:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:58,370:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:58,839:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:58,856:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-09-08 12:36:58,877:INFO:Calculating mean and std
2024-09-08 12:36:58,877:INFO:Creating metrics dataframe
2024-09-08 12:36:58,877:INFO:Uploading results into container
2024-09-08 12:36:58,877:INFO:Uploading model into container now
2024-09-08 12:36:58,877:INFO:_master_model_container: 15
2024-09-08 12:36:58,877:INFO:_display_container: 2
2024-09-08 12:36:58,877:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-09-08 12:36:58,877:INFO:create_model() successfully completed......................................
2024-09-08 12:36:59,019:INFO:SubProcess create_model() end ==================================
2024-09-08 12:36:59,019:INFO:Creating metrics dataframe
2024-09-08 12:36:59,028:INFO:Initializing create_model()
2024-09-08 12:36:59,028:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:59,028:INFO:Checking exceptions
2024-09-08 12:36:59,028:INFO:Importing libraries
2024-09-08 12:36:59,028:INFO:Copying training dataset
2024-09-08 12:36:59,041:INFO:Defining folds
2024-09-08 12:36:59,046:INFO:Declaring metric variables
2024-09-08 12:36:59,046:INFO:Importing untrained model
2024-09-08 12:36:59,046:INFO:Declaring custom model
2024-09-08 12:36:59,046:INFO:Random Forest Classifier Imported successfully
2024-09-08 12:36:59,056:INFO:Cross validation set to False
2024-09-08 12:36:59,056:INFO:Fitting Model
2024-09-08 12:36:59,756:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-09-08 12:36:59,756:INFO:create_model() successfully completed......................................
2024-09-08 12:36:59,906:INFO:Initializing create_model()
2024-09-08 12:36:59,906:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:36:59,906:INFO:Checking exceptions
2024-09-08 12:36:59,911:INFO:Importing libraries
2024-09-08 12:36:59,911:INFO:Copying training dataset
2024-09-08 12:36:59,916:INFO:Defining folds
2024-09-08 12:36:59,926:INFO:Declaring metric variables
2024-09-08 12:36:59,926:INFO:Importing untrained model
2024-09-08 12:36:59,926:INFO:Declaring custom model
2024-09-08 12:36:59,926:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 12:36:59,934:INFO:Cross validation set to False
2024-09-08 12:36:59,934:INFO:Fitting Model
2024-09-08 12:37:00,296:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 12:37:00,306:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000728 seconds.
2024-09-08 12:37:00,306:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-09-08 12:37:00,306:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-09-08 12:37:00,306:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 12:37:00,306:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 12:37:00,306:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 12:37:00,306:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 12:37:00,306:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 12:37:00,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:00,636:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 12:37:00,636:INFO:create_model() successfully completed......................................
2024-09-08 12:37:00,789:INFO:Initializing create_model()
2024-09-08 12:37:00,789:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:37:00,789:INFO:Checking exceptions
2024-09-08 12:37:00,789:INFO:Importing libraries
2024-09-08 12:37:00,789:INFO:Copying training dataset
2024-09-08 12:37:00,805:INFO:Defining folds
2024-09-08 12:37:00,805:INFO:Declaring metric variables
2024-09-08 12:37:00,806:INFO:Importing untrained model
2024-09-08 12:37:00,806:INFO:Declaring custom model
2024-09-08 12:37:00,806:INFO:Extra Trees Classifier Imported successfully
2024-09-08 12:37:00,806:INFO:Cross validation set to False
2024-09-08 12:37:00,806:INFO:Fitting Model
2024-09-08 12:37:01,396:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-09-08 12:37:01,396:INFO:create_model() successfully completed......................................
2024-09-08 12:37:01,563:INFO:_master_model_container: 15
2024-09-08 12:37:01,563:INFO:_display_container: 2
2024-09-08 12:37:01,566:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)]
2024-09-08 12:37:01,566:INFO:compare_models() successfully completed......................................
2024-09-08 12:37:01,566:INFO:Initializing create_model()
2024-09-08 12:37:01,566:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:37:01,566:INFO:Checking exceptions
2024-09-08 12:37:01,573:INFO:Importing libraries
2024-09-08 12:37:01,573:INFO:Copying training dataset
2024-09-08 12:37:01,598:INFO:Defining folds
2024-09-08 12:37:01,598:INFO:Declaring metric variables
2024-09-08 12:37:01,598:INFO:Importing untrained model
2024-09-08 12:37:01,600:INFO:Light Gradient Boosting Machine Imported successfully
2024-09-08 12:37:01,600:INFO:Starting cross validation
2024-09-08 12:37:01,606:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:37:09,457:INFO:Calculating mean and std
2024-09-08 12:37:09,457:INFO:Creating metrics dataframe
2024-09-08 12:37:09,462:INFO:Finalizing model
2024-09-08 12:37:09,876:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-09-08 12:37:09,882:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000857 seconds.
2024-09-08 12:37:09,882:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-09-08 12:37:09,882:INFO:[LightGBM] [Info] Total Bins 973
2024-09-08 12:37:09,885:INFO:[LightGBM] [Info] Number of data points in the train set: 700, number of used features: 81
2024-09-08 12:37:09,885:INFO:[LightGBM] [Info] Start training from score -0.713350
2024-09-08 12:37:09,885:INFO:[LightGBM] [Info] Start training from score -1.108663
2024-09-08 12:37:09,885:INFO:[LightGBM] [Info] Start training from score -1.714798
2024-09-08 12:37:09,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:09,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-09-08 12:37:10,202:INFO:Uploading results into container
2024-09-08 12:37:10,202:INFO:Uploading model into container now
2024-09-08 12:37:10,227:INFO:_master_model_container: 16
2024-09-08 12:37:10,227:INFO:_display_container: 3
2024-09-08 12:37:10,227:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-09-08 12:37:10,227:INFO:create_model() successfully completed......................................
2024-09-08 12:37:10,377:INFO:Initializing create_model()
2024-09-08 12:37:10,377:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:37:10,377:INFO:Checking exceptions
2024-09-08 12:37:10,377:INFO:Importing libraries
2024-09-08 12:37:10,377:INFO:Copying training dataset
2024-09-08 12:37:10,387:INFO:Defining folds
2024-09-08 12:37:10,387:INFO:Declaring metric variables
2024-09-08 12:37:10,387:INFO:Importing untrained model
2024-09-08 12:37:10,387:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:37:10,397:INFO:Starting cross validation
2024-09-08 12:37:10,401:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:37:13,717:INFO:Calculating mean and std
2024-09-08 12:37:13,717:INFO:Creating metrics dataframe
2024-09-08 12:37:13,717:INFO:Finalizing model
2024-09-08 12:37:14,564:INFO:Uploading results into container
2024-09-08 12:37:14,567:INFO:Uploading model into container now
2024-09-08 12:37:14,590:INFO:_master_model_container: 17
2024-09-08 12:37:14,590:INFO:_display_container: 4
2024-09-08 12:37:14,592:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:37:14,592:INFO:create_model() successfully completed......................................
2024-09-08 12:37:14,737:INFO:Initializing create_model()
2024-09-08 12:37:14,737:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:37:14,737:INFO:Checking exceptions
2024-09-08 12:37:14,737:INFO:Importing libraries
2024-09-08 12:37:14,737:INFO:Copying training dataset
2024-09-08 12:37:14,750:INFO:Defining folds
2024-09-08 12:37:14,750:INFO:Declaring metric variables
2024-09-08 12:37:14,757:INFO:Importing untrained model
2024-09-08 12:37:14,757:INFO:Gradient Boosting Classifier Imported successfully
2024-09-08 12:37:14,757:INFO:Starting cross validation
2024-09-08 12:37:14,762:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:37:18,102:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:37:18,117:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:37:18,251:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:37:18,323:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:37:18,589:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:37:18,612:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:37:18,684:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:37:18,742:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:37:21,372:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:37:21,539:WARNING:C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\mbr19\anaconda3\envs\my_env\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-09-08 12:37:21,579:INFO:Calculating mean and std
2024-09-08 12:37:21,579:INFO:Creating metrics dataframe
2024-09-08 12:37:21,579:INFO:Finalizing model
2024-09-08 12:37:24,038:INFO:Uploading results into container
2024-09-08 12:37:24,038:INFO:Uploading model into container now
2024-09-08 12:37:24,061:INFO:_master_model_container: 18
2024-09-08 12:37:24,061:INFO:_display_container: 5
2024-09-08 12:37:24,061:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-09-08 12:37:24,068:INFO:create_model() successfully completed......................................
2024-09-08 12:37:24,242:INFO:Initializing tune_model()
2024-09-08 12:37:24,242:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=5, round=4, n_iter=10, custom_grid={'n_estimators': [50, 100, 200], 'max_depth': [3, 7], 'learning_rate': [0.01, 0.2], 'subsample': [0.6, 1.0], 'colsample_bytree': [0.6, 1.0], 'gamma': [0, 0.3], 'min_child_weight': [1, 5]}, optimize=Accuracy, custom_scorer=None, search_library=scikit-optimize, search_algorithm=bayesian, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>)
2024-09-08 12:37:24,242:INFO:Checking exceptions
2024-09-08 12:37:24,242:INFO:Soft dependency imported: skopt: 0.10.2
2024-09-08 12:37:24,378:INFO:Copying training dataset
2024-09-08 12:37:24,389:INFO:Checking base model
2024-09-08 12:37:24,397:INFO:Base model : Extreme Gradient Boosting
2024-09-08 12:37:24,398:INFO:Declaring metric variables
2024-09-08 12:37:24,398:INFO:Defining Hyperparameters
2024-09-08 12:37:24,598:INFO:custom_grid: {'actual_estimator__n_estimators': CategoricalDistribution(values=[50, 100, 200]), 'actual_estimator__max_depth': CategoricalDistribution(values=[3, 7]), 'actual_estimator__learning_rate': CategoricalDistribution(values=[0.01, 0.2]), 'actual_estimator__subsample': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__colsample_bytree': CategoricalDistribution(values=[0.6, 1.0]), 'actual_estimator__gamma': CategoricalDistribution(values=[0, 0.3]), 'actual_estimator__min_child_weight': CategoricalDistribution(values=[1, 5])}
2024-09-08 12:37:24,598:INFO:Tuning with n_jobs=-1
2024-09-08 12:37:24,611:INFO:Initializing skopt.BayesSearchCV
2024-09-08 12:37:41,611:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.6), ('actual_estimator__gamma', 0), ('actual_estimator__learning_rate', 0.01), ('actual_estimator__max_depth', 7), ('actual_estimator__min_child_weight', 1), ('actual_estimator__n_estimators', 100), ('actual_estimator__subsample', 0.6)])
2024-09-08 12:37:41,619:INFO:Hyperparameter search completed
2024-09-08 12:37:41,619:INFO:SubProcess create_model() called ==================================
2024-09-08 12:37:41,619:INFO:Initializing create_model()
2024-09-08 12:37:41,619:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017E23EF7430>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.6})
2024-09-08 12:37:41,619:INFO:Checking exceptions
2024-09-08 12:37:41,619:INFO:Importing libraries
2024-09-08 12:37:41,619:INFO:Copying training dataset
2024-09-08 12:37:41,629:INFO:Defining folds
2024-09-08 12:37:41,636:INFO:Declaring metric variables
2024-09-08 12:37:41,636:INFO:Importing untrained model
2024-09-08 12:37:41,636:INFO:Declaring custom model
2024-09-08 12:37:41,639:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:37:41,639:INFO:Starting cross validation
2024-09-08 12:37:41,644:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:37:43,459:INFO:Calculating mean and std
2024-09-08 12:37:43,459:INFO:Creating metrics dataframe
2024-09-08 12:37:43,459:INFO:Finalizing model
2024-09-08 12:37:44,551:INFO:Uploading results into container
2024-09-08 12:37:44,551:INFO:Uploading model into container now
2024-09-08 12:37:44,559:INFO:_master_model_container: 19
2024-09-08 12:37:44,559:INFO:_display_container: 6
2024-09-08 12:37:44,561:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:37:44,561:INFO:create_model() successfully completed......................................
2024-09-08 12:37:44,719:INFO:SubProcess create_model() end ==================================
2024-09-08 12:37:44,719:INFO:choose_better activated
2024-09-08 12:37:44,719:INFO:SubProcess create_model() called ==================================
2024-09-08 12:37:44,719:INFO:Initializing create_model()
2024-09-08 12:37:44,719:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:37:44,719:INFO:Checking exceptions
2024-09-08 12:37:44,719:INFO:Importing libraries
2024-09-08 12:37:44,719:INFO:Copying training dataset
2024-09-08 12:37:44,739:INFO:Defining folds
2024-09-08 12:37:44,739:INFO:Declaring metric variables
2024-09-08 12:37:44,739:INFO:Importing untrained model
2024-09-08 12:37:44,739:INFO:Declaring custom model
2024-09-08 12:37:44,742:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:37:44,742:INFO:Starting cross validation
2024-09-08 12:37:44,750:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-09-08 12:37:46,398:INFO:Calculating mean and std
2024-09-08 12:37:46,400:INFO:Creating metrics dataframe
2024-09-08 12:37:46,400:INFO:Finalizing model
2024-09-08 12:37:47,349:INFO:Uploading results into container
2024-09-08 12:37:47,349:INFO:Uploading model into container now
2024-09-08 12:37:47,349:INFO:_master_model_container: 20
2024-09-08 12:37:47,349:INFO:_display_container: 7
2024-09-08 12:37:47,355:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:37:47,355:INFO:create_model() successfully completed......................................
2024-09-08 12:37:47,509:INFO:SubProcess create_model() end ==================================
2024-09-08 12:37:47,509:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8186
2024-09-08 12:37:47,514:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8386
2024-09-08 12:37:47,514:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2024-09-08 12:37:47,514:INFO:choose_better completed
2024-09-08 12:37:47,532:INFO:_master_model_container: 20
2024-09-08 12:37:47,533:INFO:_display_container: 6
2024-09-08 12:37:47,534:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:37:47,536:INFO:tune_model() successfully completed......................................
2024-09-08 12:37:47,682:INFO:Initializing predict_model()
2024-09-08 12:37:47,682:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000017E0D49D480>)
2024-09-08 12:37:47,682:INFO:Checking exceptions
2024-09-08 12:37:47,682:INFO:Preloading libraries
2024-09-08 12:37:48,291:INFO:Initializing get_config()
2024-09-08 12:37:48,291:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, variable=X_train)
2024-09-08 12:37:48,291:INFO:Variable: 'X_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'X_train_transformed' instead.
2024-09-08 12:37:48,361:INFO:Variable:  returned as       id Marital status Application mode Application order  ... Curricular units 2nd sem (without evaluations) Unemployment rate Inflation rate   GDP
765  765              1                1                 6  ...                                              0         10.800000            1.4  1.74
323  323              1               17                 1  ...                                              0          7.600000            2.6  0.32
909  909              1                1                 1  ...                                              0         12.400000            0.5  1.79
774  774              1               17                 1  ...                                              0          8.900000            1.4  3.51
931  931              1               39                 1  ...                                              0         16.200001            0.3 -0.92
..   ...            ...              ...               ...  ...                                            ...               ...            ...   ...
806  806              4               39                 1  ...                                              0         10.800000            1.4  1.74
492  492              1                1                 3  ...                                              0          7.600000            2.6  0.32
698  698              1               39                 1  ...                                              1         10.800000            1.4  1.74
44    44              1                1                 1  ...                                              0          9.400000           -0.8 -3.12
604  604              1                1                 1  ...                                              0         10.800000            1.4  1.74

[700 rows x 37 columns]
2024-09-08 12:37:48,361:INFO:get_config() successfully completed......................................
2024-09-08 12:37:48,361:INFO:Initializing predict_model()
2024-09-08 12:37:48,361:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000017E23FBDC60>)
2024-09-08 12:37:48,361:INFO:Checking exceptions
2024-09-08 12:37:48,361:INFO:Preloading libraries
2024-09-08 12:37:48,361:INFO:Set up data.
2024-09-08 12:37:48,374:INFO:Set up index.
2024-09-08 12:37:48,762:INFO:Initializing get_config()
2024-09-08 12:37:48,762:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, variable=y_train)
2024-09-08 12:37:48,762:INFO:Variable: 'y_train' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_train_transformed' instead.
2024-09-08 12:37:48,762:INFO:Variable:  returned as 765    0
323    1
909    0
774    0
931    1
      ..
806    1
492    0
698    1
44     0
604    1
Name: Target, Length: 700, dtype: int8
2024-09-08 12:37:48,762:INFO:get_config() successfully completed......................................
2024-09-08 12:37:48,770:INFO:Initializing get_config()
2024-09-08 12:37:48,770:INFO:get_config(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, variable=y_test)
2024-09-08 12:37:48,770:INFO:Variable: 'y_test' used to return the transformed values in PyCaret 2.x. From PyCaret 3.x, this will return the raw values. If you need the transformed values, call get_config with 'y_test_transformed' instead.
2024-09-08 12:37:48,770:INFO:Variable:  returned as 47     1
233    1
132    2
34     0
162    0
      ..
772    0
632    0
365    2
136    0
299    1
Name: Target, Length: 300, dtype: int8
2024-09-08 12:37:48,770:INFO:get_config() successfully completed......................................
2024-09-08 12:37:48,770:INFO:Initializing finalize_model()
2024-09-08 12:37:48,770:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-09-08 12:37:48,779:INFO:Finalizing XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2024-09-08 12:37:48,790:INFO:Initializing create_model()
2024-09-08 12:37:48,790:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.6, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.01, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=7, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=100, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-09-08 12:37:48,790:INFO:Checking exceptions
2024-09-08 12:37:48,790:INFO:Importing libraries
2024-09-08 12:37:48,790:INFO:Copying training dataset
2024-09-08 12:37:48,790:INFO:Defining folds
2024-09-08 12:37:48,790:INFO:Declaring metric variables
2024-09-08 12:37:48,790:INFO:Importing untrained model
2024-09-08 12:37:48,790:INFO:Declaring custom model
2024-09-08 12:37:48,795:INFO:Extreme Gradient Boosting Imported successfully
2024-09-08 12:37:48,803:INFO:Cross validation set to False
2024-09-08 12:37:48,803:INFO:Fitting Model
2024-09-08 12:37:50,590:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 12:37:50,595:INFO:create_model() successfully completed......................................
2024-09-08 12:37:50,745:INFO:_master_model_container: 20
2024-09-08 12:37:50,745:INFO:_display_container: 7
2024-09-08 12:37:51,130:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False)
2024-09-08 12:37:51,130:INFO:finalize_model() successfully completed......................................
2024-09-08 12:37:51,757:INFO:Initializing predict_model()
2024-09-08 12:37:51,757:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000017E21FE73D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['id',
                                             'Previous qualification (grade)',
                                             'Admission grade',
                                             'Age at enrollment',
                                             'Curricular units 1st sem '
                                             '(credited)',
                                             'Curricular units 1st sem '
                                             '(enrolled)',
                                             'Curricular units 1st sem '
                                             '(evaluations)',
                                             'Curricular units 1st sem '
                                             '(approved)',
                                             'Curricular units 1st sem (gr...
                               importance_type=None,
                               interaction_constraints=None, learning_rate=0.01,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=7, max_leaves=None, min_child_weight=1,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None,
                               objective='multi:softprob', ...))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000017E242F4E50>)
2024-09-08 12:37:51,757:INFO:Checking exceptions
2024-09-08 12:37:51,757:INFO:Preloading libraries
2024-09-08 12:37:51,757:INFO:Set up data.
2024-09-08 12:37:51,813:INFO:Set up index.
